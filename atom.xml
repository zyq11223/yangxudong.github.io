<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小毛驴</title>
  <icon>https://www.gravatar.com/avatar/f846aadee6c8f6f3945510781c210bbf</icon>
  <subtitle>Adventure may hurt you, but monotony will kill you.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://xudongyang.coding.me/"/>
  <updated>2019-04-02T02:43:36.877Z</updated>
  <id>http://xudongyang.coding.me/</id>
  
  <author>
    <name>yangxudong</name>
    <email>yangxudongsuda@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深度CTR预估模型中的特征自动组合机制演化简史</title>
    <link href="http://xudongyang.coding.me/xdeepfm/"/>
    <id>http://xudongyang.coding.me/xdeepfm/</id>
    <published>2018-12-20T08:46:41.000Z</published>
    <updated>2019-04-02T02:43:36.877Z</updated>
    
    <content type="html"><![CDATA[<p>众所周知，深度学习在计算机视觉、语音识别、自然语言处理等领域最先取得突破并成为主流方法。但是，深度学习为什么是在这些领域而不是其他领域最先成功呢？我想一个原因就是图像、语音、文本数据在空间和时间上具有一定的内在关联性。比如，图像中会有大量的像素与周围的像素比较类似；文本数据中语言会受到语法规则的限制。CNN对于空间特征有很好的学习能力，正如RNN对于时序特征有强大的表示能力一样，因此CNN和RNN在上述领域各领风骚好多年。</p><p>在Web-scale的搜索、推荐和广告系统中，特征数据具有高维、稀疏、多类别的特点，一般情况下缺少类图像、语音、文本领域的时空关联性。因此，如何构建合适的网络结构以便在信息检索、推荐系统和计算广告领域取得良好的特征表示能力，进一步提升最终的业务效果成了学术界和工业界共同关注的问题。</p><p>本文在跟踪了最近主流的互联网业务中大量使用的排序模型的基础上，总结出了深度CTR、CVR预估模型发展演化的三条主线，跟大家分享。</p><ol><li>第一条主脉络是以FM家族为代表的深度模型，它们的共同特点是自动学习从原始特征交叉组合新的高阶特征。</li><li>第二条主脉络是一类使用attention机制处理时序特征的深度模型，以DIN、DIEN等模型为代表。</li><li>第三条主脉络是以迁移学习、多任务学习为基础的联合训练模型或pre-train机制，以<a href="https://zhuanlan.zhihu.com/p/37562283" target="_blank" rel="noopener">ESMM</a>、DUPN等模型为代表。</li></ol><p>其中前两条主脉络虽然出发点不同，但个人认为也有一些共通之处，比如attention机制是不是可以在某种程度上理解为一种特殊形式的组合特征。第三条主脉络属于流程或框架层面的创建。本文的主要目标是理清楚第一条主线中各个经典的深度模型的发展演化脉络，包括它们的优缺点和共通之处。<br><a id="more"></a></p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>构建好的特征对于机器学习任务来说至关重要，它关系到模型的学习难易程度及泛化性能。好的特征是相互独立的有区分性且易于理解的特征，具体地可以参考《<a href="https://yangxudong.github.io/good-feature/" target="_blank" rel="noopener">何为优秀的机器学习特征</a>》。</p><p>交叉组合原始特征构成新的特征是一种常用且有效的特征构建方法。哪些特征需要被交叉组合以便生成新的有效特征？需要多少阶的交叉组合？这些问题在深度学习流行之前需要算法工程师依靠经验来解决。人工构建组合特征特别耗时耗力，在样本数据生成的速度和数量巨大的互联网时代，依靠人的经验和技能识别出所有潜在有效的特征组合模式几乎是不可能的。一些有效的组合特征甚至没有在样本数据中出现过。</p><p>那么，能否自动构建有效的交叉组合特征？答案是肯定的。在深度学习之前，一些有益的尝试是把特征组合的任务交给子模型来学习，最经典的方法就是Facebook在2014年的论文中介绍的通过GBDT（Gradient Boost Decision Tree）模型解决LR模型的特征组合问题。该方法思路很简单，特征工程分为两部分，一部分特征用于训练一个GBDT模型，把GBDT模型每颗树的叶子节点编号作为新的特征，加入到原始特征集中，再训练最终的LR模型。详细介绍可以查看我之前的一篇博文：《<a href="https://zhuanlan.zhihu.com/p/35465875" target="_blank" rel="noopener">主流CTR预估模型的演化及对比</a>》。此类解决方案在特征工程阶段就引入了机器学习模型，虽然可以部分解决问题，但还是过于麻烦，不够灵活。</p><p>要避免上述麻烦，自然而然就是要引入端到端学习的思路，即用一个统一的模型同时完成特征组合和目标拟合的任务。因子分解机(Factorization Machines, FM)模型是第一个从原始特征出发，端到端学习的例子。然而，FM毕竟还是一个浅层模型，经典的FM模型只能做二阶的特征交叉，模型学习复杂组合特征的能力偏弱。尽管如此，FM提出了一种很好的自动学习交叉组合特征的思路，随后融入FM模型思路的深度学习模型便如雨后春笋般应运而生，典型的代表有FNN、PNN、DeepFM、DCN、xDeepFM等。关于这些模型的介绍和对比，在我之前的两篇博文中也有详细介绍，感兴趣的读者可以查阅《<a href="https://zhuanlan.zhihu.com/p/35465875" target="_blank" rel="noopener">主流CTR预估模型的演化及对比</a>》、《<a href="https://zhuanlan.zhihu.com/p/43364598" target="_blank" rel="noopener">玩转企业级Deep&amp;Cross Network模型你只差一步</a>》。</p><p>本文的其余内容将会对这些模型做一个详细的复盘，同时对该主线的集大成者xDeepFM模型做一个详细的介绍，其中包括一些自己对模型的理解，实际的使用心得，以及某些模型实现时的一些trick。文章的最后还会提供某些模型的源代码链接。</p><h2 id="特征组合的演化路线"><a href="#特征组合的演化路线" class="headerlink" title="特征组合的演化路线"></a>特征组合的演化路线</h2><p>从<strong>FM</strong>模型说起，FM通过特征对之间的隐变量内积来提取特征组合，其函数形式如下：</p><script type="math/tex; mode=display">y=w_0 + \sum_{i=1}^{n}w_i x_i + \sum_{i=1}^{n}\sum_{j=i+1}^n \langle v_i,v_j \rangle x_i x_j</script><p>对于每个原始特征，FM都会学习一个隐向量。模型通过穷举所有的特征对（pair）并逐一检测特征对的效用值的方法来自动识别出有效的特征组合。特征对的效用值通过该特征对涉及的两个原始特征的隐向量的内积来计算。</p><p>可以看出FM最多只能识别出二阶的特征组合，模型有一定的局限性。<strong>FNN</strong>模型最先提出了一种增强FM模型的思路，就是用FM模型学习到的隐向量初始化深度神经网络模型（MLP），再由MLP完成最终学习。</p><p>MLP（plain-DNN）因其特殊的结构天然就具有学习高阶特征组合的能力，它可以在一定的条件下以任意精度逼近任意函数。然而，plain-DNN以一种隐式的方式建模特征之间的交互关系，我们无法确定它学习到了多少阶的交叉关系。高维稀疏的原始特征在输入给DNN之前一般都会经过embedding处理，每一个域（类别）的原始特征都会被映射到一个低维稠密的实数向量，称之为embedding向量。FM模型中的隐向量也可以理解为embedding向量。Embedding向量中的元素用术语bit表示，可以看出plain-DNN的高阶特征交互建模是元素级的（bit-wise），也就是说同一个域对应的embedding向量中的元素也会相互影响。这与FM显式构建特征交叉关系的方式是不一样的，FM类方法是以向量级（vector-wise）的方式来构建高阶交叉关系。经验上，vector-wise的方式构建的特征交叉关系比bit-wise的方式更容易学习。</p><p>虽然两种建模交叉特征的方式有一些区别，但两者并不是相互排斥的，如果能把两者集合起来，便会相得益彰。<strong>PNN</strong>模型最先提出了一种融合bit-wise和vector-wise交叉特征的方法，其通过在网络的embedding层与全连接层之间加了一层Product Layer来完成特征组合。PNN与FM相比，舍弃了低阶特征，也就是线性的部分，这在一定程度上使得模型不太容易记住一些数据中的规律。<strong>WDL（Wide &amp; Deep Learning）</strong>模型混合了宽度模型与深度模型，其宽度部分保留了低价特征，偏重记忆；深度部分引入了bit-wise的特征交叉能力。WDL模型的一大缺点是宽度部分的输入依旧依赖于大量的人工特征工程。</p><p>能不能在融合bit-wise和vector-wise交叉特征的基础上，同时还能保留低阶特征(linear part)呢？当然是可以的。<strong>DeepFM</strong>模型融合了FM和WDL模型，其FM部分实现了低阶特征和vector-wise的二阶交叉特征建模，其Deep部分使模型具有了bit-wise的高阶交叉特征建模的能力。具体地，DeepFM包含两部分：神经网络部分与因子分解机部分，这两部分共享同样的输入。对于给定特征$i$，向量$w_i$用于表征一阶特征的重要性，隐变量$V_i$用于表示该特征与其他特征的相互影响。在FM部分，$V_i$用于表征二阶特征，同时在神经网络部分用于构建高阶特征。所有的参数共同参与训练。DeepFM的预测结果可以写为</p><script type="math/tex; mode=display">\hat{y}=sigmoid(y_{FM}+y_{DNN})</script><p>其中$\hat{y}∈(0,1)$是预测的点击率，$y_{FM}$与$y_{DNN}$分是FM部分与DNN部分。</p><script type="math/tex; mode=display">y_{FM}=\langle w,x \rangle + \sum_{i=1}^d \sum_{j=i+1}^d \langle V_i,V_j \rangle x_i x_j</script><p>其中$w∈R^d,V_i∈R^k$ 。加法部分反映了一阶特征的重要性，而内积部分反应了二阶特征的影响。</p><script type="math/tex; mode=display">y_{DNN}=\sigma(W^{H+1} \cdot a^H + b^{H+1})</script><p>其中H是隐层的层数。</p><p><img src="https://yangxudong.github.io/ctr-models/dnn-models.png" alt></p><p>FM、DeepFM和Inner-PNN都是通过原始特征隐向量的内积来构建vector-wise的二阶交叉特征，这种方式有两个主要的缺点：</p><ol><li>必须要穷举出所有的特征对，即任意两个field之间都会形成特征组合关系，而过多的组合关系可能会引入无效的交叉特征，给模型引入过多的噪音，从而导致性能下降。</li><li>二阶交叉特征有时候是不够的，好的特征可能需要更高阶的组合。虽然DNN部分可以部分弥补这个不足，但bit-wise的交叉关系是晦涩难懂、不确定并且不容易学习的。</li></ol><p>那么，<strong>有没有可能引入更高阶的vector-wise的交叉特征，同时又能控制模型的复杂度，避免产生过多的无效交叉特征呢？</strong>让我们先来思考一个问题。二阶交叉特征通过穷举所有的原始特征对得到，那么通过穷举的方法得到更高阶的交叉特征，必然会产生组合爆炸的维数灾难，导致网络参数过于庞大而无法学习，同时也会产生很多的无效交叉特征。让我们把这个问题称之为<strong>维数灾难挑战</strong>。</p><p>解决维数灾难挑战不可避免的就是要引入某种“压缩”机制，就是要把高阶的组合特征向量的维数降到一个合理的范围，同时在这个过程中尽量多的保留有效的交叉特征，去除无效的交叉特征。让我们谨记，所有构建高阶交叉特征的模型必然要引入特定的“压缩”机制，在学习建模高阶交叉特征的模型时我们脑中要始终绷紧一根弦，那就是这种压缩机制是如何实现的？这种压缩机制的效率和效果如何？</p><p>解决维数灾难挑战，<a href="https://zhuanlan.zhihu.com/p/43364598" target="_blank" rel="noopener">Deep &amp; Cross Network(DCN)</a>模型交出一份让人比较满意的答卷，让我们来看看它是如何做到的。</p><p><strong>DCN</strong>模型以一个嵌入和堆叠层(embedding and stacking layer)开始，接着并列连一个cross network和一个deep network，接着通过一个combination layer将两个network的输出进行组合。交叉网络（cross network）的核心思想是以有效的方式应用显式特征交叉。交叉网络由交叉层组成，每个层具有以下公式：</p><script type="math/tex; mode=display">x_{l+1} = x_0 x_l^T w_l + b_l + x_l = f(x_l, w_l, b_l) + x_l</script><p>其中:</p><ul><li>$x_l,x_{l+1}$是列向量（column vectors），分别表示来自第$l$层和第($l+1$)层cross layers的输出；</li><li>$w_l, b_l \in R^d$是第$l$层layer的weight和bias参数。</li></ul><p>在完成一个特征交叉f后，每个cross layer会将它的输入加回去，对应的mapping function $f：R^d \rightarrow R^d$，刚好等于残差$x_{l+1} - x_l$，这里借鉴了残差网络的思想。</p><p><img src="https://yangxudong.github.io/dcn/dcn.png" alt></p><p><strong>特征的高阶交叉（high-degree interaction）</strong>：cross network的独特结构使得交叉特征的阶（the degress of cross features）随着layer的深度而增长。对于第$l$层layer，它的最高多项式阶（在输入$x_0$上）是$l+1$。 实际上，cross network由这些交叉项$x_1^{\alpha_1} x_2^{\alpha_2} … x_d^{\alpha_d}$组成，对应的阶从$1$到$l+1$。</p><p><strong>复杂度分析</strong>：假设$L_c$表示cross layers的数目，$d$表示输入$x_0$的维度。那么，在该cross network中涉及的参数数目为：$d \times L_c \times 2$。</p><p>一个cross network的时间和空间复杂度对于输入维度是线性关系。因而，比起它的deep部分，一个cross network引入的复杂度微不足道，DCN的整体复杂度与传统的DNN在同一水平线上。如此高效（efficiency）是受益于$x_0 x_l^T$的rank-one特性(两个向量的叉积)，它可以使我们生成所有的交叉项，无需计算或存储整个matrix。</p><p>关于DCN模型的实现，有一个重要的技巧可以节省大量的内存空间和训练时间，就是在计算cross layer的时候需要利用矩阵乘法的结合律，优先计算$x_l^T w$，而不是$x_0 x_l^T$，这是因为$x_l^T w$的计算结果是一个标量，几乎不占用存储空间，具体请参考《<a href="https://zhuanlan.zhihu.com/p/43364598" target="_blank" rel="noopener">玩转企业级Deep&amp;Cross Network模型你只差一步</a>》。</p><p>亲爱的读者们，你们脑中的那根弦还在吗？DCN是如何有效压缩高维特征空间的呢？其实，对于cross layer可以换一种理解方式：假设$\tilde{x} \in R^d$是一个cross layer的输入，cross layer首先构建$d^2$个关于$x_i \tilde{x}_j$的pairwise交叉，接着以一种内存高效的方式将它们投影到维度$d$上。如果采用全连接Layer那样直接投影的方式会带来3次方的开销。Cross layer提供了一种有效的解决方式，将开销减小到维度$d$的量级上：考虑到$x_p = x_0 \tilde{x}^T w$等价于：</p><script type="math/tex; mode=display">x_p^T = [x_1\tilde{x}_1 ... x_1\tilde{x}_d ... x_d\tilde{x}_1 ... x_d\tilde{x}_d] \left[\begin{array}{ccc}  w&0&...&0\\  0&w&...&0\\  \vdots&\vdots&\ddots&\vdots\\  0&0&...&w\end{array}\right]</script><p>其中，行向量包含了所有$d^2$个关于$x_i \tilde{x}_j$的pairwise交叉，投影矩阵具有一个块对角化结构，其中$w \in R^d$是一个列向量。现在我们了解了DCN模型的“压缩”机制，即每个cross layer都把$d^2$维度的特征空间投影到$d$维的空间上。</p><p>DCN模型中使用的这种“压缩”机制是完美的吗，有没有什么局限性？实际上这种“压缩”方式把特征交互关系限定在一种特殊的形式上。我们再来看看cross layer的计算公式，为了简化，以便说明问题，下面去掉偏置项。</p><script type="math/tex; mode=display">x_k = x_0x_{k-1}^Tw_k+x_{k-1}</script><p>对于$x_1$，有如下公式:</p><script type="math/tex; mode=display">x_1 = x_0x_0^Tw_k+x_0</script><p>合并可得到：</p><script type="math/tex; mode=display">x_1 = x_0（x_0^Tw_k+1）= \alpha^1x_0</script><p>其中$\alpha^1=x_0^Tw_k+1$ 是一个关于$x_0$的线性回归函数，也就是一个标量。</p><p>根据数学归纳法，当$k=i$ 时，上式成立；当$k=i+1$ 时，我们可以得到</p><script type="math/tex; mode=display">x_{i+1}=x_0x_i^Tw_{i+1}=x_0((\alpha^ix_0)w_{i+1})+\alpha^ix_0=\alpha^{i+1}x_0</script><p>实际上，$\alpha^{i+1}$又是一个标量。因此Cross Network的输出就相当于$x_0$不断乘以一个数，当然这个数是和$x_0$高度相关的。</p><p>因此，我们可以总结出DCN模型的两个主要的不足：</p><ol><li>CrossNet的输出被限定在一种特殊的形式上</li><li>特征交叉还是以bit-wise的方式构建的</li></ol><p>让我们回到最初的那个问题，<strong>有没有可能引入更高阶的vector-wise的交叉特征，同时又能控制模型的复杂度，避免产生过多的无效交叉特征呢？</strong></p><h2 id="极深因子分解机模型（xDeepFM）"><a href="#极深因子分解机模型（xDeepFM）" class="headerlink" title="极深因子分解机模型（xDeepFM）"></a>极深因子分解机模型（xDeepFM）</h2><p><a href="https://arxiv.org/abs/1803.05170" target="_blank" rel="noopener">xDeepFM</a>模型是自动构建交叉特征且能够端到端学习的集大成者，它很好的回答了上一小节末提出的问题。让我们来看看它是如何做到的。</p><p>为了实现自动学习显式的高阶特征交互，同时使得交互发生在向量级上，xDeepFM首先提出了一种新的名为<strong>压缩交互网络</strong>（Compressed Interaction Network，简称CIN）的模型。</p><p>CIN的输入是所有field的embedding向量构成的矩阵$X^0 \in R^{m \times  D}$，该矩阵的第$i$行对应第$i$个field的embedding向量，假设共有$m$个field，每个field的embedding向量的维度为$D$。CIN网络也是一个多层的网络，它的第$k$层的输出也是一个矩阵，记为$X^k \in R^{H_k \times  D}$，该矩阵的行数为 $H_k$，表示第$k$层共有$H_k$个特征（embedding）向量，其中$H_0=m$。</p><p>CIN中第$k$层的输出$X^k$由第$k-1$层的输出$X^{k-1}$和输入$X^0$经过一个比较复杂的运算得到，具体地，矩阵$X^k$中的第$h$行的计算公式如下：</p><script type="math/tex; mode=display">X_{h,*}^k = \sum_{i=1}^{H_{k-1}}\sum_{j=1}^m{W_{ij}^{k,h}(X_{i,*}^{k-1} \circ X_{j,*}^0)}</script><p>其中，$\circ$ 表示哈达玛积，即两个矩阵或向量对应元素相乘得到相同大小的矩阵或向量，示例如：$\langle a_1,a_2,a_3 \rangle \circ \langle b_1,b_2,b_3\rangle =\langle a_1b_1,a_2b_2,a_3b_3 \rangle$。</p><p>上述计算公式可能不是很好理解，论文作者给出了另一种更加方便理解的视角。在计算$X^{k+1}$时，定义一个中间变量$Z^{k+1} \in R^{H_k \times m \times D}$，$Z^{k+1}$是一个数据立方体，由D个数据矩阵堆叠而成，其中每个数据矩阵是由$X^k$的一个列向量与$X^0$的一个列向量的外积运算（Outer product）而得，如下图所示。$Z^{k+1}$的生成过程实际上是由$X^k$与$X^0$沿着各自embedding向量的方向计算外积的过程。</p><p><img src="https://www.msra.cn/wp-content/uploads/2018/08/kdd-2018-xdeepfm-2.png" alt></p><p>$Z^{k+1}$可以被看作是一个宽度为$m$、高度为$H_k$、通道数为D的图像，在这个虚拟的图像上施加一些卷积操作即得到$X^{k+1}$。$W^{k,h}$是其中一个卷积核，总共有$H_{k+1}$个不同的卷积核，因而借用CNN网络中的概念，$X^{k+1}$可以看作是由$H_{k+1}$个feature map堆叠而成，如下图所示。</p><p><img src="https://www.msra.cn/wp-content/uploads/2018/08/kdd-2018-xdeepfm-3.png" alt></p><p>正是通过卷积操作，CIN把第$k+1$层由$H_k \times m$个向量压缩到了$H_{k+1}$个向量，起到了防止维数灾难的效果。</p><p>CIN的宏观框架如下图所示，它的特点是，最终学习出的特征交互的阶数是由网络的层数决定的，每一层隐层都通过一个池化操作连接到输出层，从而保证了输出单元可以见到不同阶数的特征交互模式。同时不难看出，CIN的结构与循环神经网络RNN是很类似的，即每一层的状态是由前一层隐层的值与一个额外的输入数据计算所得。不同的是，CIN中不同层的参数是不一样的，而在RNN中是相同的；RNN中每次额外的输入数据是不一样的，而CIN中额外的输入数据是固定的，始终是$X^0$。</p><p><img src="/xdeepfm/CIN.png" alt></p><p>有了基础结构CIN之后，借鉴Wide&amp;Deep和DeepFM等模型的设计，将CIN与线性回归单元、全连接神经网络单元组合在一起，得到最终的模型并命名为极深因子分解机xDeepFM，其结构如下图所示。同时包含多种不同的结构成分可以提升模型的表达能力。</p><p><img src="https://www.msra.cn/wp-content/uploads/2018/08/kdd-2018-xdeepfm-5.png" alt></p><p>集成的CIN和DNN两个模块能够帮助模型同时以显式和隐式的方式学习高阶的特征交互，而集成的线性模块和深度神经模块也让模型兼具记忆与泛化的学习能力。值得一提的是，为了提高模型的通用性，xDeepFM中不同的模块共享相同的输入数据。而在具体的应用场景下，不同的模块也可以接入各自不同的输入数据，例如，线性模块中依旧可以接入很多根据先验知识提取的交叉特征来提高记忆能力，而在CIN或者DNN中，为了减少模型的计算复杂度，可以只导入一部分稀疏的特征子集。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>特征交叉组合作为一种常用的特征工程方法，可以有效地提升模型的效果。特征交叉组合从人工方式开始，经历了模型辅助的阶段，最后发展到各种端到端模型的阶段。端到端模型从建模二阶交叉关系向构建高阶交叉关系的方向发展，同时建模方式也从bit-wise向vector-wise发展。</p><p><img src="/xdeepfm/feature_interaction.png" alt="图"></p><p>本文总结了FM家族的一系列深度学习模型，这些模型有一个共同的强制要求：所有field的embedding向量的维数是相同的。这个要求是合理的吗？我们知道不同的field对应的值空间大小是不一样的，比如淘宝商品ID的量级在十亿级，类目的量级在万级，用户年龄段的量级在十级，在如此巨大的差异的情况下，embedding向量的维数只能取得尽可能的大，这大大增加了模型的参数量级和网络的收敛时间。所以我认为本文提及的FM家族模型有两个主要缺点：</p><ol><li>强制要求所有field的embedding向量的维数，增加了网络复杂度；</li><li>对连续值特征不友好。</li></ol><p>大家对此有什么看法呢？欢迎在评论区留言。</p><p>部分模型的tensorflow源代码<a href="https://github.com/yangxudong/deeplearning" target="_blank" rel="noopener">可以在此找到</a>，实现不一定完全正确，欢迎批评指正。</p><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p><a href="https://zhuanlan.zhihu.com/p/35465875" target="_blank" rel="noopener">主流CTR预估模型的演化及对比</a><br><a href="https://zhuanlan.zhihu.com/p/43364598" target="_blank" rel="noopener">玩转企业级Deep&amp;Cross Network模型你只差一步</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;众所周知，深度学习在计算机视觉、语音识别、自然语言处理等领域最先取得突破并成为主流方法。但是，深度学习为什么是在这些领域而不是其他领域最先成功呢？我想一个原因就是图像、语音、文本数据在空间和时间上具有一定的内在关联性。比如，图像中会有大量的像素与周围的像素比较类似；文本数据中语言会受到语法规则的限制。CNN对于空间特征有很好的学习能力，正如RNN对于时序特征有强大的表示能力一样，因此CNN和RNN在上述领域各领风骚好多年。&lt;/p&gt;
&lt;p&gt;在Web-scale的搜索、推荐和广告系统中，特征数据具有高维、稀疏、多类别的特点，一般情况下缺少类图像、语音、文本领域的时空关联性。因此，如何构建合适的网络结构以便在信息检索、推荐系统和计算广告领域取得良好的特征表示能力，进一步提升最终的业务效果成了学术界和工业界共同关注的问题。&lt;/p&gt;
&lt;p&gt;本文在跟踪了最近主流的互联网业务中大量使用的排序模型的基础上，总结出了深度CTR、CVR预估模型发展演化的三条主线，跟大家分享。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;第一条主脉络是以FM家族为代表的深度模型，它们的共同特点是自动学习从原始特征交叉组合新的高阶特征。&lt;/li&gt;
&lt;li&gt;第二条主脉络是一类使用attention机制处理时序特征的深度模型，以DIN、DIEN等模型为代表。&lt;/li&gt;
&lt;li&gt;第三条主脉络是以迁移学习、多任务学习为基础的联合训练模型或pre-train机制，以&lt;a href=&quot;https://zhuanlan.zhihu.com/p/37562283&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ESMM&lt;/a&gt;、DUPN等模型为代表。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中前两条主脉络虽然出发点不同，但个人认为也有一些共通之处，比如attention机制是不是可以在某种程度上理解为一种特殊形式的组合特征。第三条主脉络属于流程或框架层面的创建。本文的主要目标是理清楚第一条主线中各个经典的深度模型的发展演化脉络，包括它们的优缺点和共通之处。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="CTR预估" scheme="http://xudongyang.coding.me/tags/CTR%E9%A2%84%E4%BC%B0/"/>
    
      <category term="xDeepFM" scheme="http://xudongyang.coding.me/tags/xDeepFM/"/>
    
  </entry>
  
  <entry>
    <title>距离玩转企业级DCN(Deep &amp; Cross Network)模型，你只差一步</title>
    <link href="http://xudongyang.coding.me/dcn/"/>
    <id>http://xudongyang.coding.me/dcn/</id>
    <published>2018-08-30T08:20:44.000Z</published>
    <updated>2019-04-02T02:43:36.809Z</updated>
    
    <content type="html"><![CDATA[<p>Deep &amp; Cross Network(DCN)在 2017 年由 google 和 Stanford 共同发表的一篇论文中被提出，类似于Wide &amp; Deep Network(WDL)，是用负杂网络预估CTR的一种方法。</p><p>特征工程一直是许多预测模型成功的关键。许多有效的特征都来自于原始特征的交叉组合。在WDL中，wide侧的交叉组合特征依然需要依靠hand-craft来完成。而DCN能对sparse和dense的输入自动学习特征交叉，可以有效地捕获有限阶（bounded degrees）上的有效特征交叉，无需人工特征工程或暴力搜索（exhaustive searching），并且计算代价较低。</p><p>本文在详细介绍Deep &amp; Cross Network网络结构的基础上，给出了高效实现DCN模型的tensorflow代码，主要点出了网络上一些主流实现中常犯的错误，让你真正能够在企业级的生产环境中玩转DCN模型。<br><a id="more"></a></p><h2 id="DCN网络结构"><a href="#DCN网络结构" class="headerlink" title="DCN网络结构"></a>DCN网络结构</h2><p>DCN模型以一个嵌入和堆叠层(embedding and stacking layer)开始，接着并列连一个cross network和一个deep network，接着通过一个combination layer将两个network的输出进行组合。</p><p><img src="/dcn/dcn.png" alt></p><h3 id="嵌入和堆叠层"><a href="#嵌入和堆叠层" class="headerlink" title="嵌入和堆叠层"></a>嵌入和堆叠层</h3><p>考虑具有稀疏和稠密特征的输入数据。在网络规模推荐系统的CTR预测任务中，输入主要是分类特征，如“country=usa”。这些特征通常是编码为独热向量如<code>[0,1,0]</code>；然而，这通常会产生超高维度的特征空间。</p><p>为了减少维数，我们采用嵌入过程将这些二进制特征转换成实数值的稠密向量（通常称为嵌入向量）：</p><script type="math/tex; mode=display">x_{embed,i} =W_{embed,i}x_i</script><p>其中$x_{embed,i}$是embedding vector，$x_i$是第$i$个category的二元输入，$W_{embed,i} \in R^{n_e \times n_v}$是对应的embedding matrix，会与网络中的其它参数一起进行优化，$n_e$,$n_v$分别是embedding size和vocabulary size。</p><p>最后，我们将嵌入向量与归一化稠密特征xdense叠加起来形成一个向量：<script type="math/tex">x_0 = [ x_{embed,1}^T, ..., X_{embed,k}^T, X_{dense}^T]。</script></p><p>这一部分在tensorflow中，使用<code>tf.feature_column</code>API可以很容易实现，大致代码结构如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">embed0 = tf.feature_column.embedding_column(...)</span><br><span class="line">...</span><br><span class="line">dense0 = tf.feature_column.indicator_column(...)</span><br><span class="line">dense1 = tf.feature_column.numeric_column(...)</span><br><span class="line">...</span><br><span class="line">columns = [embed0, ..., dense0, dense1, ...]</span><br><span class="line">x0 = tf.feature_column.input_layer(features, feature_columns)</span><br></pre></td></tr></table></figure></p><h2 id="交叉网络"><a href="#交叉网络" class="headerlink" title="交叉网络"></a>交叉网络</h2><p>交叉网络的核心思想是以有效的方式应用显式特征交叉。交叉网络由交叉层组成，每个层具有以下公式：</p><script type="math/tex; mode=display">x_{l+1} = x_0 x_l^T w_l + b_l + x_l = f(x_l, w_l, b_l) + x_l</script><p>其中:</p><ul><li>$x_l,x_{l+1}$是列向量（column vectors），分别表示来自第$l$层和第($l+1$)层cross layers的输出；</li><li>$w_l, b_l \in R^d$是第$l$层layer的weight和bias参数。</li></ul><p>在完成一个特征交叉f后，每个cross layer会将它的输入加回去，对应的mapping function $f：R^d \rightarrow R^d$，刚好等于残差$x_{l+1} - x_l$，这里借鉴了残差网络的思想。</p><p><img src="/dcn/cross_layer.jpg" alt></p><p><strong>特征的高阶交叉（high-degree interaction）</strong>：cross network的独特结构使得交叉特征的阶（the degress of cross features）随着layer的深度而增长。对于第$l$层layer，它的最高多项式阶（在输入$x_0$上）是$l+1$。 实际上，cross network由这些交叉项$x_1^{\alpha_1} x_2^{\alpha_2} … x_d^{\alpha_d}$组成，对应的阶从$1$到$l+1$。</p><p><strong>复杂度分析</strong>：假设$L_c$表示cross layers的数目，$d$表示输入$x_0$的维度。那么，在该cross network中涉及的参数数目为：$d \times L_c \times 2$。</p><p>一个cross network的时间和空间复杂度对于输入维度是线性关系。因而，比起它的deep部分，一个cross network引入的复杂度微不足道，DCN的整体复杂度与传统的DNN在同一水平线上。如此高效（efficiency）是受益于$x_0 x_l^T$的rank-one特性(两个向量的叉积)，它可以使我们生成所有的交叉项，无需计算或存储整个matrix。</p><p>搜索了网上主流的实现cross layer的方法，代码如下:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_layer</span><span class="params">(x0, x, name)</span>:</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(name):</span><br><span class="line">    input_dim = x0.get_shape().as_list()[<span class="number">1</span>]</span><br><span class="line">    w = tf.get_variable(<span class="string">"weight"</span>, [input_dim], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>))</span><br><span class="line">    b = tf.get_variable(<span class="string">"bias"</span>, [input_dim], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>))</span><br><span class="line">    xx0 = tf.expand_dims(x0, <span class="number">-1</span>)  <span class="comment"># shape &lt;?, d, 1&gt;</span></span><br><span class="line">    xx = tf.expand_dims(x, <span class="number">-1</span>)  <span class="comment"># shape &lt;?, d, 1&gt;</span></span><br><span class="line">    mat = tf.matmul(xx0, xx, transpose_b=<span class="literal">True</span>)  <span class="comment"># shape &lt;?, d, d&gt;</span></span><br><span class="line">    <span class="keyword">return</span> tf.tensordot(mat, w, <span class="number">1</span>) + b + x  <span class="comment"># shape &lt;?, d&gt;</span></span><br></pre></td></tr></table></figure></p><p>这种方法在逻辑上没有什么问题，但实际上却是<strong>非常消耗计算和存储资源</strong>的，原因在于显式地计算$x_0 x_l^T$需要非常大的内存空间来存储临时计算结果。我们来计算一下，一个cross layer仅仅是计算$x_0 x_l^T$这一个操作就需要消耗 $batch\_size \times d \times d \times 4$ 字节的内存（一个浮点数占4个字节）。在企业级的模型中，$d$通常是几千甚至几万的量级，假设$d=1k$，则需要$batch\_size \times 4M$的存储空间，这通常情况下已经是G级别的大小了，何况我们仅仅计算了一个Layer，别忘了我们总共有$L_c$个cross layer。另外，该操作的结果（一个矩阵）再和$w$向量相乘时也是非常消耗计算资源的。即使你在离线训练时通过减少cross layer的个数，减小batch_size等手段完成了模型的训练，在模型部署中线上之后，线性的打分系统依然要面临Out of Memory的风险，因为线上预测我们总是希望一次请求尽可能返回多条记录的预测分数，否则要么是影响全局的效果，要么是需要更多的请求次数，从而面临巨大的性能压力。</p><p>正确的实现方式不是先计算$x_0 x_l^T$，而是先计算$x_l^T w$，因为$x_l^T w$的计算结果是一个标量，几乎不占用存储空间。这两种方法的计算结果是一致的，因为矩阵乘法是满足结合律的： <code>(AB)C=A(BC)</code>。高效的实现代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_layer2</span><span class="params">(x0, x, name)</span>:</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(name):</span><br><span class="line">    input_dim = x0.get_shape().as_list()[<span class="number">1</span>]</span><br><span class="line">    w = tf.get_variable(<span class="string">"weight"</span>, [input_dim], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>))</span><br><span class="line">    b = tf.get_variable(<span class="string">"bias"</span>, [input_dim], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>))</span><br><span class="line">    xb = tf.tensordot(tf.reshape(x, [<span class="number">-1</span>, <span class="number">1</span>, input_dim]), w, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> x0 * xb + b + x</span><br></pre></td></tr></table></figure></p><p>在上面的实现中，我们使用了<code>tf.reshape</code>操作实现了$x_l$的转置，因为$x_l$实际上是一个向量，并不是一个矩阵，因此这种方法是可行的。下面给出构建整个交叉网络的tensorflow代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_cross_layers</span><span class="params">(x0, params)</span>:</span></span><br><span class="line">  num_layers = params[<span class="string">'num_cross_layers'</span>]</span><br><span class="line">  x = x0</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</span><br><span class="line">    x = cross_layer2(x0, x, <span class="string">'cross_&#123;&#125;'</span>.format(i))</span><br><span class="line">  <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p><p>对于cross layer可以换一种理解方式。假设$\tilde{x} \in R^d$是一个cross layer的输入，cross layer首先构建$d^2$个关于$x_i \tilde{x}_j$的pairwise交叉，接着以一种内存高效的方式将它们投影到维度$d$上。如果采用全连接Layer那样直接投影的方式会带来3次方的开销。Cross layer提供了一种有效的解决方式，将开销减小到维度$d$的量级上：考虑到$x_p = x_0 \tilde{x}^T w$等价于：</p><script type="math/tex; mode=display">x_p^T = [x_1\tilde{x}_1 ... x_1\tilde{x}_d ... x_d\tilde{x}_1 ... x_d\tilde{x}_d] \left[\begin{array}{ccc}  w&0&...&0\\  0&w&...&0\\  \vdots&\vdots&\ddots&\vdots\\  0&0&...&w\end{array}\right]</script><p>其中，行向量包含了所有$d^2$个关于$x_i \tilde{x}_j$的pairwise交叉，投影矩阵具有一个块对角化结构，其中$w \in R^d$是一个列向量。</p><h2 id="深度网络"><a href="#深度网络" class="headerlink" title="深度网络"></a>深度网络</h2><p>交叉网络的参数数目少，从而限制了模型的能力（capacity）。为了捕获高阶非线性交叉，我们平行引入了一个深度网络。</p><p>深度网络就是一个全连接的前馈神经网络，每个深度层具有如下公式：<script type="math/tex">h_{l+1} = f(W_l h_l + b_l)</script><br>其中：</p><ul><li>$h_l \in R^{n_l}, h_{l+1} \in R^{n_{l+1}}$别是第$l$层和第($l+1$)层hidden layer；</li><li>$W_l \in R^{n_{l+1} \times n_l}, b_l \in R^{n_{l+1}}$第$l$个deep layer的参数；</li><li>$f(⋅)$是ReLU function。</li></ul><p><strong>复杂度分析</strong>：出于简洁性，我们假设所有的deep layers具有相同的size。假设$L_d$表示deep layers的数目，m表示deep layer的size。那么，在该deep network中的参数的数目为：</p><script type="math/tex; mode=display">d×m+m+(m2+m)×(L_d−1)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_deep_layers</span><span class="params">(x0, params)</span>:</span></span><br><span class="line">  <span class="comment"># Build the hidden layers, sized according to the 'hidden_units' param.</span></span><br><span class="line">  net = x0</span><br><span class="line">  <span class="keyword">for</span> units <span class="keyword">in</span> params[<span class="string">'hidden_units'</span>]:</span><br><span class="line">    net = tf.layers.dense(net, units=units, activation=tf.nn.relu)</span><br><span class="line">  <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure><h2 id="Combination-Layer"><a href="#Combination-Layer" class="headerlink" title="Combination Layer"></a>Combination Layer</h2><p>最后，将两个network的输出进行拼接（concatenate），然后将该拼接向量（concatenated vector）喂给一个标准的逻辑回归模型。<script type="math/tex">p = \sigma ( [x_{L_1}^T, h_{L_2}^T] w_{logits})</script></p><p>类似于WDL模型，我们对两个network进行jointly train，在训练期间，每个独立的network会察觉到另一个。下面给出整个模型的实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dcn_model_fn</span><span class="params">(features, labels, mode, params)</span>:</span></span><br><span class="line">  x0 = tf.feature_column.input_layer(features, params[<span class="string">'feature_columns'</span>])</span><br><span class="line">  last_deep_layer = build_deep_layers(x0, params)</span><br><span class="line">  last_cross_layer = build_cross_layers(x0, params)</span><br><span class="line">  last_layer = tf.concat([last_cross_layer, last_deep_layer], <span class="number">1</span>)</span><br><span class="line">  my_head = tf.contrib.estimator.binary_classification_head(thresholds=[<span class="number">0.5</span>])</span><br><span class="line">  logits = tf.layers.dense(last_layer, units=my_head.logits_dimension)</span><br><span class="line">  optimizer = tf.train.AdagradOptimizer(learning_rate=params[<span class="string">'learning_rate'</span>])</span><br><span class="line">  <span class="keyword">return</span> my_head.create_estimator_spec(</span><br><span class="line">    features=features,</span><br><span class="line">    mode=mode,</span><br><span class="line">    labels=labels,</span><br><span class="line">    logits=logits,</span><br><span class="line">    train_op_fn=<span class="keyword">lambda</span> loss: optimizer.minimize(loss, global_step=tf.train.get_global_step())</span><br><span class="line">  )</span><br></pre></td></tr></table></figure><p><img src="/dcn/dcn_tensor.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>DCN主要有以下几点贡献：</p><ul><li>提出一种新型的交叉网络结构，可以用来提取交叉组合特征，并不需要人为设计的特征工程；</li><li>这种网络结构足够简单同时也很有效，可以获得随网络层数增加而增加的多项式阶（polynomial degree）交叉特征；</li><li>十分节约内存（依赖于正确地实现），并且易于使用；</li><li>实验结果表明，DCN相比于其他模型有更出色的效果，与DNN模型相比，较少的参数却取得了较好的效果。</li></ul><p><a href="https://github.com/yangxudong/deeplearning/tree/master/DCN" target="_blank" rel="noopener">源代码Github</a></p><h2 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h2><p><a href="https://arxiv.org/pdf/1708.05123.pdf" target="_blank" rel="noopener">Deep &amp; Cross Network for Ad Click Prediction</a></p><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/35465875" target="_blank" rel="noopener">主流CTR预估模型的演化及对比</a></li><li><a href="https://zhuanlan.zhihu.com/p/37562283" target="_blank" rel="noopener">CVR预估的新思路：完整空间多任务模型</a></li><li><a href="https://yangxudong.github.io" target="_blank" rel="noopener">个人博客</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Deep &amp;amp; Cross Network(DCN)在 2017 年由 google 和 Stanford 共同发表的一篇论文中被提出，类似于Wide &amp;amp; Deep Network(WDL)，是用负杂网络预估CTR的一种方法。&lt;/p&gt;
&lt;p&gt;特征工程一直是许多预测模型成功的关键。许多有效的特征都来自于原始特征的交叉组合。在WDL中，wide侧的交叉组合特征依然需要依靠hand-craft来完成。而DCN能对sparse和dense的输入自动学习特征交叉，可以有效地捕获有限阶（bounded degrees）上的有效特征交叉，无需人工特征工程或暴力搜索（exhaustive searching），并且计算代价较低。&lt;/p&gt;
&lt;p&gt;本文在详细介绍Deep &amp;amp; Cross Network网络结构的基础上，给出了高效实现DCN模型的tensorflow代码，主要点出了网络上一些主流实现中常犯的错误，让你真正能够在企业级的生产环境中玩转DCN模型。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="DCN" scheme="http://xudongyang.coding.me/tags/DCN/"/>
    
      <category term="CTR预估" scheme="http://xudongyang.coding.me/tags/CTR%E9%A2%84%E4%BC%B0/"/>
    
  </entry>
  
  <entry>
    <title>构建分布式Tensorflow模型系列之CVR预估案例ESMM模型</title>
    <link href="http://xudongyang.coding.me/esmm-1/"/>
    <id>http://xudongyang.coding.me/esmm-1/</id>
    <published>2018-08-16T09:03:31.000Z</published>
    <updated>2019-04-02T02:43:36.814Z</updated>
    
    <content type="html"><![CDATA[<p>本文是“基于Tensorflow高阶API构建大规模分布式深度学习模型系列”的第五篇，旨在通过一个完整的案例巩固一下前面几篇文章中提到的各类高阶API的使用方法，同时演示一下用tensorflow高阶API构建一个比较复杂的分布式深度学习模型的完整过程。</p><p>文本要实现的深度学习模式是阿里巴巴的算法工程师18年刚发表的论文《<a href="https://arxiv.org/abs/1804.07931" target="_blank" rel="noopener">Entire Space Multi-Task Model: An Eﬀective Approach for Estimating Post-Click Conversion Rate</a>》中提出的ESMM模型，关于该模型的详细介绍可以参考我之前的一篇文章：《<a href="https://zhuanlan.zhihu.com/p/37562283" target="_blank" rel="noopener">CVR预估的新思路：完整空间多任务模型</a>》。</p><a id="more"></a><p>ESMM模型是一个多任务学习（Multi-Task Learning）模型，它同时学习学习点击率和转化率两个目标，即模型直接预测展现转换率（pCTCVR）：单位流量获得成交的概率。模型的结构如图1所示。</p><p><img src="https://yangxudong.github.io/esmm/esmm.png" alt></p><p>ESMM模型有两个主要的特点:</p><ul><li>在整个样本空间建模。区别与传统的CVR预估方法通常使用“点击-&gt;成交”事情的日志来构建训练样本，ESMM模型使用“展现-&gt;点击-&gt;成交”事情的日志来构建训练样本。</li><li>共享特征表示。两个子任务（CTR预估和CVR预估）之间共享各类实体（产品、品牌、类目、商家等）ID的embedding向量表示。</li></ul><p>ESMM模型的损失函数由两部分组成，对应于pCTR 和pCTCVR 两个子任务，其形式如下：<br>\begin{align}<br>L(\theta_{cvr},\theta_{ctr}) &amp;=\sum_{i=1}^N l(y_i, f(x_i; \theta_{ctr}))\\<br>&amp;= \sum_{i=1}^N l(y_i\&amp;z_i, f(x_i; \theta_{ctr}) \times f(x_i; \theta_{cvr}))<br>\end{align}<br>其中，$\theta_{ctr}$和$\theta_{cvr}$分别是CTR网络和CVR网络的参数，$l(\cdot)$是交叉熵损失函数。在CTR任务中，有点击行为的展现事件构成的样本标记为正样本，没有点击行为发生的展现事件标记为负样本；在CTCVR任务中，同时有点击和购买行为的展现事件标记为正样本，否则标记为负样本。</p><p>ESMM模型由两个结构完全相同的子网络连接而成，我们把子网络对应的模型称之为Base模型。接下来，我们先介绍下如何用tensorflow实现Base模型。</p><h2 id="Base模型的实现"><a href="#Base模型的实现" class="headerlink" title="Base模型的实现"></a>Base模型的实现</h2><p>在Base模型的网络输入包括user field和item field两部分。user field主要由用户的历史行为序列构成，具体地说，包含了用户浏览的产品ID列表，以及用户浏览的品牌ID列表、类目ID列表等；不同的实体ID列表构成不同的field。网络的Embedding层，把这些实体ID都映射为固定长度的低维实数向量；接着之后的Field-wise Pooling层把同一个Field的所有实体embedding向量求和得到对应于当前Field的一个唯一的向量；之后所有Field的向量拼接（concat）在一起构成一个大的隐层向量；接着大的隐层向量之上再接入诺干全连接层，最后再连接到只有一个神经元的输出层。</p><h3 id="Feature-Column构建序列embedding和pooling"><a href="#Feature-Column构建序列embedding和pooling" class="headerlink" title="Feature Column构建序列embedding和pooling"></a>Feature Column构建序列embedding和pooling</h3><p>具体到tensorflow里，如何实现embedding layer以及field-wise pooling layer呢？</p><p>其实，用tensorflow的<a href="https://zhuanlan.zhihu.com/p/41663141" target="_blank" rel="noopener">Feature Column API</a>可以非常容易地实现。在详细介绍之前，建议读者先阅读一下该系列文章的上一篇：《<a href="https://zhuanlan.zhihu.com/p/41663141" target="_blank" rel="noopener">构建分布式Tensorflow模型系列:特征工程</a>》。</p><p>实现embedding layer需要用到<code>tf.feature_column.embedding_column</code>或者<code>tf.feature_column.shared_embedding_columns</code>，这里因为我们希望user field和item field的同一类型的实体共享相同的embedding映射空间，所有选用<code>tf.feature_column.shared_embedding_columns</code>。由于<code>shared_embedding_columns</code>函数只接受categorical_column列表作为参数，因此需要为原始特征数据先创建categorical_columns。</p><p>来看下具体的例子，假设在原始特征数据中，behaviorPids表示用户历史浏览过的产品ID列表；productId表示当前的候选产品ID；则构建embedding layer的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> feature_column <span class="keyword">as</span> fc</span><br><span class="line"><span class="comment"># user field</span></span><br><span class="line">pids = fc.categorical_column_with_hash_bucket(<span class="string">"behaviorPids"</span>, <span class="number">10240</span>, dtype=tf.int64)</span><br><span class="line"><span class="comment"># item field</span></span><br><span class="line">pid = fc.categorical_column_with_hash_bucket(<span class="string">"productId"</span>, <span class="number">1000000</span>, dtype=tf.int64)</span><br><span class="line"></span><br><span class="line">pid_embed = fc.shared_embedding_columns([pids, pid], <span class="number">100</span>, combiner=<span class="string">'sum'</span>, shared_embedding_collection_name=<span class="string">"pid"</span>)</span><br></pre></td></tr></table></figure><p>需要说明的是，在构建训练样本时要特别注意，behaviorPids列表必须是固定长度的，否则在使用dataset的batch方法时会报tensor shape不一致的错。然而，现实中每个用户浏览过的产品个数肯定会不一样，这时可以截取用户的最近N个浏览行为，当某些用户的浏览商品数不足N个时填充默认值-1（如果ID是用字符串表示的时候，填充空字符串）。那么为什么填充的默认值必须是-1呢？这时因为<code>categorical_column*</code>函数用默认值-1表示样本数据中未登录的值，-1表示的categorical_column经过embedding_column之后被映射到零向量，而零向量在后面的求和pooling操作中不影响结果。</p><p>那么，如何实现field-wise pooling layer呢？其实，在用<code>tf.feature_column.embedding_column</code>或者<code>tf.feature_column.shared_embedding_columns</code>API时不需要另外实现pooling layer，因为这2个函数同时实现了embedding向量映射和field-wise pooling。大家可能已经主要到了shared_embedding_columns函数的combiner=’sum’参数，这个参数就指明了当该field有多个embedding向量时融合为唯一一个向量的操作，’sum’操作即element-wise add。</p><p>上面的代码示例，仅针对产品这一实体特征进行了embedding和pooling操作，当有多个不同的实体特征时，仅需要采用相同的方法即可。</p><h3 id="实现weighted-sum-pooling操作"><a href="#实现weighted-sum-pooling操作" class="headerlink" title="实现weighted sum pooling操作"></a>实现weighted sum pooling操作</h3><p>上面的操作实现了行为序列特征的embedding和pooling，但有一个问题就是序列中的每个行为被同等对待了；某些情况下，我们可能希望行为序列中不同的实体ID在做sum pooling时有不同的权重。比如说，我们可能希望行为时间越近的产品的权重越高，或者与候选产品有相同属性（类目、品牌、商家等）的产品有更高的权重。</p><p>那么如何实现weighted sum pooling操作呢？答案就是使用<code>weighted_categorical_column</code>函数。我们必须在构建样本时添加一个额外的权重特征，权重特征表示行为序列中每个产品的权重，因此权重特征是一个与行为序列平行的列表（向量），两者的维度必须相同。另外，如果行为序列中有填充的默认值-1，那么权重特征中这些默认值对应的权重必须为0。代码示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> feature_column <span class="keyword">as</span> fc</span><br><span class="line"><span class="comment"># user field</span></span><br><span class="line">pids = fc.categorical_column_with_hash_bucket(<span class="string">"behaviorPids"</span>, <span class="number">10240</span>, dtype=tf.int64)</span><br><span class="line">pids_weighted = fc.weighted_categorical_column(pids, <span class="string">"pidWeights"</span>)</span><br><span class="line"><span class="comment"># item field</span></span><br><span class="line">pid = fc.categorical_column_with_hash_bucket(<span class="string">"productId"</span>, <span class="number">1000000</span>, dtype=tf.int64)</span><br><span class="line"></span><br><span class="line">pid_embed = fc.shared_embedding_columns([pids_weighted, pid], <span class="number">100</span>, combiner=<span class="string">'sum'</span>, shared_embedding_collection_name=<span class="string">"pid"</span>)</span><br></pre></td></tr></table></figure><h3 id="模型函数"><a href="#模型函数" class="headerlink" title="模型函数"></a>模型函数</h3><p>Base模型的其他组件就不过多介绍了，模型函数的代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_model</span><span class="params">(features, labels, mode, params)</span>:</span></span><br><span class="line">  net = fc.input_layer(features, params[<span class="string">'feature_columns'</span>])</span><br><span class="line">  <span class="comment"># Build the hidden layers, sized according to the 'hidden_units' param.</span></span><br><span class="line">  <span class="keyword">for</span> units <span class="keyword">in</span> params[<span class="string">'hidden_units'</span>]:</span><br><span class="line">    net = tf.layers.dense(net, units=units, activation=tf.nn.relu)</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'dropout_rate'</span> <span class="keyword">in</span> params <span class="keyword">and</span> params[<span class="string">'dropout_rate'</span>] &gt; <span class="number">0.0</span>:</span><br><span class="line">      net = tf.layers.dropout(net, params[<span class="string">'dropout_rate'</span>], training=(mode == tf.estimator.ModeKeys.TRAIN))</span><br><span class="line">  my_head = tf.contrib.estimator.binary_classification_head(thresholds=[<span class="number">0.5</span>])</span><br><span class="line">  <span class="comment"># Compute logits (1 per class).</span></span><br><span class="line">  logits = tf.layers.dense(net, my_head.logits_dimension, activation=<span class="literal">None</span>, name=<span class="string">"my_model_output_logits"</span>)</span><br><span class="line">  optimizer = tf.train.AdagradOptimizer(learning_rate=params[<span class="string">'learning_rate'</span>])</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_train_op_fn</span><span class="params">(loss)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> optimizer.minimize(loss, global_step=tf.train.get_global_step())</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> my_head.create_estimator_spec(</span><br><span class="line">    features=features,</span><br><span class="line">    mode=mode,</span><br><span class="line">    labels=labels,</span><br><span class="line">    logits=logits,</span><br><span class="line">    train_op_fn=_train_op_fn</span><br><span class="line">  )</span><br></pre></td></tr></table></figure></p><h2 id="ESMM模型的实现"><a href="#ESMM模型的实现" class="headerlink" title="ESMM模型的实现"></a>ESMM模型的实现</h2><p>有了Base模型之后，ESMM模型的实现就已经成功了一大半。剩下的工作就是把两个子模型连接在一块，同时定义好整个模型的损失函数和优化操作即可。</p><p>在前面的文章中，我们分享过为tensorflow estimator定义模型函数时，需要为不同mode（训练、评估、预测）下定义构建计算graph的所有操作，并返回想要的<code>tf.estimator.EstimatorSpec</code>。在前面的介绍中，我们使用了Head API来简化了创建EstimatorSpec的过程，但在实现ESMM模型时没有现成的Head可用，必须手动创建EstimatorSpec。</p><p>在不同的mode下，模型函数必须返回包含不同图操作（op）的EstimatorSpec，具体地：</p><ul><li>For mode == ModeKeys.TRAIN: required fields are <code>loss</code> and <code>train_op</code>.</li><li>For mode == ModeKeys.EVAL: required field is <code>loss</code>.</li><li>For mode == ModeKeys.PREDICT: required fields are <code>predictions</code>.</li></ul><p>另外，如果模型需要导出以便提供线上服务，这时必须在mode == ModeKeys.EVAL定义<code>export_outputs</code>操作，并添加到返回的EstimatorSpec中。</p><p>实现ESMM模型的关键点在于定义该模型独特的损失函数。上文也提到，ESMM模型的损失函数有2部分构成，一部分对应于CTR任务，另一部分对应于CTCVR任务。具体如何定义，请参考下面的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_mode</span><span class="params">(features, mode, params)</span>:</span></span><br><span class="line">  net = fc.input_layer(features, params[<span class="string">'feature_columns'</span>])</span><br><span class="line">  <span class="comment"># Build the hidden layers, sized according to the 'hidden_units' param.</span></span><br><span class="line">  <span class="keyword">for</span> units <span class="keyword">in</span> params[<span class="string">'hidden_units'</span>]:</span><br><span class="line">    net = tf.layers.dense(net, units=units, activation=tf.nn.relu)</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'dropout_rate'</span> <span class="keyword">in</span> params <span class="keyword">and</span> params[<span class="string">'dropout_rate'</span>] &gt; <span class="number">0.0</span>:</span><br><span class="line">      net = tf.layers.dropout(net, params[<span class="string">'dropout_rate'</span>], training=(mode == tf.estimator.ModeKeys.TRAIN))</span><br><span class="line">  <span class="comment"># Compute logits</span></span><br><span class="line">  logits = tf.layers.dense(net, <span class="number">1</span>, activation=<span class="literal">None</span>)</span><br><span class="line">  <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_model</span><span class="params">(features, labels, mode, params)</span>:</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'ctr_model'</span>):</span><br><span class="line">    ctr_logits = build_mode(features, mode, params)</span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'cvr_model'</span>):</span><br><span class="line">    cvr_logits = build_mode(features, mode, params)</span><br><span class="line"></span><br><span class="line">  ctr_predictions = tf.sigmoid(ctr_logits, name=<span class="string">"CTR"</span>)</span><br><span class="line">  cvr_predictions = tf.sigmoid(cvr_logits, name=<span class="string">"CVR"</span>)</span><br><span class="line">  prop = tf.multiply(ctr_predictions, cvr_predictions, name=<span class="string">"CTCVR"</span>)</span><br><span class="line">  <span class="keyword">if</span> mode == tf.estimator.ModeKeys.PREDICT:</span><br><span class="line">    predictions = &#123;</span><br><span class="line">      <span class="string">'probabilities'</span>: prop,</span><br><span class="line">      <span class="string">'ctr_probabilities'</span>: ctr_predictions,</span><br><span class="line">      <span class="string">'cvr_probabilities'</span>: cvr_predictions</span><br><span class="line">    &#125;</span><br><span class="line">    export_outputs = &#123;</span><br><span class="line">      <span class="string">'prediction'</span>: tf.estimator.export.PredictOutput(predictions)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode, predictions=predictions, export_outputs=export_outputs)</span><br><span class="line"></span><br><span class="line">  y = labels[<span class="string">'cvr'</span>]</span><br><span class="line">  cvr_loss = tf.reduce_sum(tf.keras.backend.binary_crossentropy(y, prop), name=<span class="string">"cvr_loss"</span>)</span><br><span class="line">  ctr_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels[<span class="string">'ctr'</span>], logits=ctr_logits), name=<span class="string">"ctr_loss"</span>)</span><br><span class="line">  loss = tf.add(ctr_loss, cvr_loss, name=<span class="string">"ctcvr_loss"</span>)</span><br><span class="line"></span><br><span class="line">  ctr_accuracy = tf.metrics.accuracy(labels=labels[<span class="string">'ctr'</span>], predictions=tf.to_float(tf.greater_equal(ctr_predictions, <span class="number">0.5</span>)))</span><br><span class="line">  cvr_accuracy = tf.metrics.accuracy(labels=y, predictions=tf.to_float(tf.greater_equal(prop, <span class="number">0.5</span>)))</span><br><span class="line">  ctr_auc = tf.metrics.auc(labels[<span class="string">'ctr'</span>], ctr_predictions)</span><br><span class="line">  cvr_auc = tf.metrics.auc(y, prop)</span><br><span class="line">  metrics = &#123;<span class="string">'cvr_accuracy'</span>: cvr_accuracy, <span class="string">'ctr_accuracy'</span>: ctr_accuracy, <span class="string">'ctr_auc'</span>: ctr_auc, <span class="string">'cvr_auc'</span>: cvr_auc&#125;</span><br><span class="line">  tf.summary.scalar(<span class="string">'ctr_accuracy'</span>, ctr_accuracy[<span class="number">1</span>])</span><br><span class="line">  tf.summary.scalar(<span class="string">'cvr_accuracy'</span>, cvr_accuracy[<span class="number">1</span>])</span><br><span class="line">  tf.summary.scalar(<span class="string">'ctr_auc'</span>, ctr_auc[<span class="number">1</span>])</span><br><span class="line">  tf.summary.scalar(<span class="string">'cvr_auc'</span>, cvr_auc[<span class="number">1</span>])</span><br><span class="line">  <span class="keyword">if</span> mode == tf.estimator.ModeKeys.EVAL:</span><br><span class="line">    <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Create training op.</span></span><br><span class="line">  <span class="keyword">assert</span> mode == tf.estimator.ModeKeys.TRAIN</span><br><span class="line">  optimizer = tf.train.AdagradOptimizer(learning_rate=params[<span class="string">'learning_rate'</span>])</span><br><span class="line">  train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())</span><br><span class="line">  <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)</span><br></pre></td></tr></table></figure><p>至此，实现ESMM模型需要介绍的内容就结束了，完整的代码已经在github上共享了，欢迎大家下载试用。</p><p>完整源代码：<a href="https://github.com/yangxudong/deeplearning/tree/master/esmm" target="_blank" rel="noopener">https://github.com/yangxudong/deeplearning/tree/master/esmm</a></p><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/38470806" target="_blank" rel="noopener">基于Tensorflow高阶API构建大规模分布式深度学习模型系列: 开篇</a></li><li><a href="https://zhuanlan.zhihu.com/p/38421397" target="_blank" rel="noopener">基于Tensorflow高阶API构建大规模分布式深度学习模型系列：基于Dataset API处理Input pipeline</a></li><li><a href="https://zhuanlan.zhihu.com/p/41473323" target="_blank" rel="noopener">基于Tensorflow高阶API构建大规模分布式深度学习模型系列: 自定义Estimator（以文本分类CNN模型为例）</a></li><li><a href="https://zhuanlan.zhihu.com/p/41663141" target="_blank" rel="noopener">基于Tensorflow高阶API构建大规模分布式深度学习模型系列:特征工程 Feature Column</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是“基于Tensorflow高阶API构建大规模分布式深度学习模型系列”的第五篇，旨在通过一个完整的案例巩固一下前面几篇文章中提到的各类高阶API的使用方法，同时演示一下用tensorflow高阶API构建一个比较复杂的分布式深度学习模型的完整过程。&lt;/p&gt;
&lt;p&gt;文本要实现的深度学习模式是阿里巴巴的算法工程师18年刚发表的论文《&lt;a href=&quot;https://arxiv.org/abs/1804.07931&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Entire Space Multi-Task Model: An Eﬀective Approach for Estimating Post-Click Conversion Rate&lt;/a&gt;》中提出的ESMM模型，关于该模型的详细介绍可以参考我之前的一篇文章：《&lt;a href=&quot;https://zhuanlan.zhihu.com/p/37562283&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVR预估的新思路：完整空间多任务模型&lt;/a&gt;》。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="tensorflow" scheme="http://xudongyang.coding.me/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>基于Tensorflow高阶API构建大规模分布式深度学习模型系列之特征工程Feature Columns</title>
    <link href="http://xudongyang.coding.me/tensorflow-feature-columns/"/>
    <id>http://xudongyang.coding.me/tensorflow-feature-columns/</id>
    <published>2018-08-09T08:39:10.000Z</published>
    <updated>2019-04-02T02:43:36.870Z</updated>
    
    <content type="html"><![CDATA[<p>特征工程是机器学习流程中重要的一个环节，即使是通常用来做端到端学习的深度学习模型在训练之前也免不了要做一些特征工程相关的工作。Tensorflow平台提供的FeatureColumn API为特征工程提供了强大的支持。</p><p>Feature cloumns是原始数据和Estimator模型之间的桥梁，它们被用来把各种形式的原始数据转换为模型能够使用的格式。深度神经网络只能处理数值数据，网络中的每个神经元节点执行一些针对输入数据和网络权重的乘法和加法运算。然而，现实中的有很多非数值的类别数据，比如产品的品牌、类目等，这些数据如果不加转换，神经网络是无法处理的。另一方面，即使是数值数据，在仍给网络进行训练之前有时也需要做一些处理，比如标准化、离散化等。</p><p><img src="https://www.tensorflow.org/images/feature_columns/inputs_to_model_bridge.jpg" alt><br><a id="more"></a><br>在Tensorflow中，通过调用<code>tf.feature_column</code>模块来创建feature columns。有两大类feature column，一类是生成dense tensor的Dense Column；另一类是生成sparse tensor的Categorical Column。具体地，目前tensorflow提供的feature columns如下图所示。</p><p><img src="https://www.tensorflow.org/images/feature_columns/some_constructors.jpg" alt="feature columns"></p><h2 id="Numeric-column"><a href="#Numeric-column" class="headerlink" title="Numeric column"></a>Numeric column</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.feature_column.numeric_column(</span><br><span class="line">    key,</span><br><span class="line">    shape=(1,),</span><br><span class="line">    <span class="attribute">default_value</span>=None,</span><br><span class="line">    <span class="attribute">dtype</span>=tf.float32,</span><br><span class="line">    <span class="attribute">normalizer_fn</span>=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>key: 特征的名字。也就是对应的列名称。</li><li>shape: 该key所对应的特征的shape. 默认是1，但是比如one-hot类型的，shape就不是1，而是实际的维度。总之，这里是key所对应的维度，不一定是1.</li><li>default_value: 如果不存在使用的默认值</li><li>normalizer_fn: 对该特征下的所有数据进行转换。如果需要进行normalize，那么就是使用normalize的函数.这里不仅仅局限于normalize，也可以是任何的转换方法，比如取对数，取指数，这仅仅是一种变换方法.</li></ul><p>创建numeric column的方法如下：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Represent a tf.float64 scalar.</span></span><br><span class="line"><span class="attribute">numeric_feature_column</span>=tf.feature_column.numeric_column(key="SepalLength", <span class="attribute">dtype</span>=tf.float64)</span><br></pre></td></tr></table></figure></p><p>默认情况下，numeric column创建的是一个标量值，也可以指定shape参数来创建向量、矩阵等多维数据。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Represent a 10-element vector in which each cell contains a tf.float32.</span></span><br><span class="line">vector_feature_column = tf.feature_column.numeric_column(<span class="attribute">key</span>=<span class="string">"Bowling"</span>, <span class="attribute">shape</span>=10)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Represent a 10x5 matrix in which each cell contains a tf.float32.</span></span><br><span class="line">matrix_feature_column = tf.feature_column.numeric_column(<span class="attribute">key</span>=<span class="string">"MyMatrix"</span>, shape=[10,5])</span><br></pre></td></tr></table></figure></p><p>我们还可以为numeric column指定数值变换的函数normalizer_fn，为对原始数据做一些变换操作。可以使用下面的代码测试numeric column的效果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> feature_column</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.feature_column.feature_column <span class="keyword">import</span> _LazyBuilder</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_numeric</span><span class="params">()</span>:</span></span><br><span class="line">    price = &#123;<span class="string">'price'</span>: [[<span class="number">1.</span>], [<span class="number">2.</span>], [<span class="number">3.</span>], [<span class="number">4.</span>]]&#125;  <span class="comment"># 4行样本</span></span><br><span class="line">    builder = _LazyBuilder(price)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform_fn</span><span class="params">(x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x + <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    price_column = feature_column.numeric_column(<span class="string">'price'</span>, normalizer_fn=transform_fn)</span><br><span class="line">    price_transformed_tensor = price_column._get_dense_tensor(builder)</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">        print(session.run([price_transformed_tensor]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用input_layer</span></span><br><span class="line">    price_transformed_tensor = feature_column.input_layer(price, [price_column])</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">        print(<span class="string">'use input_layer'</span> + <span class="string">'_'</span> * <span class="number">40</span>)</span><br><span class="line">        print(session.run([price_transformed_tensor]))</span><br><span class="line"></span><br><span class="line">test_numeric()</span><br></pre></td></tr></table></figure></p><p>执行后的输出为：<br><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">[array(<span class="comment">[<span class="comment">[3.]</span>,</span></span></span><br><span class="line"><span class="comment"><span class="comment">       <span class="comment">[4.]</span>,</span></span></span><br><span class="line"><span class="comment"><span class="comment">       <span class="comment">[5.]</span>,</span></span></span><br><span class="line"><span class="comment"><span class="comment">       <span class="comment">[6.]</span>]</span>, dtype=float32)]</span></span><br><span class="line">use input_layer________________________________________</span><br><span class="line"><span class="comment">[array(<span class="comment">[<span class="comment">[3.]</span>,</span></span></span><br><span class="line"><span class="comment"><span class="comment">       <span class="comment">[4.]</span>,</span></span></span><br><span class="line"><span class="comment"><span class="comment">       <span class="comment">[5.]</span>,</span></span></span><br><span class="line"><span class="comment"><span class="comment">       <span class="comment">[6.]</span>]</span>, dtype=float32)]</span></span><br></pre></td></tr></table></figure></p><h2 id="Bucketized-column"><a href="#Bucketized-column" class="headerlink" title="Bucketized column"></a>Bucketized column</h2><p>Bucketized column用来把numeric column的值按照提供的边界（boundaries)离散化为多个值。离散化是特征工程常用的一种方法。例如，把年份离散化为4个阶段，如下图所示。</p><p><img src="https://www.tensorflow.org/images/feature_columns/bucketized_column.jpg" alt="year"></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf<span class="selector-class">.feature_column</span><span class="selector-class">.bucketized_column</span>(</span><br><span class="line">    source_column,</span><br><span class="line">    boundaries</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>source_column: 必须是numeric_column</li><li>boundaries: 不同的桶。boundaries=[0., 1., 2.],产生的bucket就是, (-inf, 0.), [0., 1.), [1., 2.), and [2., +inf), 每一个区间分别表示0, 1, 2, 3,所以相当于分桶分了4个.</li></ul><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># First, convert the raw input to a numeric column.</span></span><br><span class="line">numeric_feature_column = tf.feature_column.numeric_column(<span class="string">"Year"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Then, bucketize the numeric column on the years 1960, 1980, and 2000.</span></span><br><span class="line">bucketized_feature_column = tf.feature_column.bucketized_column(</span><br><span class="line">    source_column = numeric_feature_column,</span><br><span class="line">    boundaries = [1960, 1980, 2000])</span><br></pre></td></tr></table></figure><p>按照上述代码，可以把Year字段离散化为下表所示的结果。</p><div class="table-container"><table><thead><tr><th>Date Range</th><th>Represented as…</th></tr></thead><tbody><tr><td>&lt; 1960</td><td>[1, 0, 0, 0]</td></tr><tr><td>>= 1960 but &lt; 1980</td><td>[0, 1, 0, 0]</td></tr><tr><td>>= 1980 but &lt; 2000</td><td>[0, 0, 1, 0]</td></tr><tr><td>>= 2000</td><td>[0, 0, 0, 1]</td></tr></tbody></table></div><p>我们可以进一步做一些测试：<br><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def test_bucketized_column():</span><br><span class="line">    price = &#123;<span class="string">'price'</span>: [[<span class="number">5.</span>], [<span class="number">15.</span>], [<span class="number">25.</span>], [<span class="number">35.</span>]]&#125;  # <span class="number">4</span>行样本</span><br><span class="line">    price_column = feature_column.numeric_column(<span class="string">'price'</span>)</span><br><span class="line">    bucket_price = feature_column.bucketized_column(price_column, [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>])</span><br><span class="line">    price_bucket_tensor = feature_column.input_layer(price, [bucket_price])</span><br><span class="line">    with tf.<span class="symbol">Session</span>() as session:</span><br><span class="line">        print(session.run([price_bucket_tensor]))</span><br><span class="line"></span><br><span class="line">test_bucketized_column()</span><br></pre></td></tr></table></figure></p><p>测试结果为：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[array([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>]], dtype=float32)]</span><br></pre></td></tr></table></figure></p><h2 id="Categorical-identity-column"><a href="#Categorical-identity-column" class="headerlink" title="Categorical identity column"></a>Categorical identity column</h2><p>与Bucketized column类似，Categorical identity column用单个唯一值表示bucket。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create categorical output for an integer feature named "my_feature_b",</span></span><br><span class="line"><span class="comment"># The values of my_feature_b must be &gt;= 0 and &lt; num_buckets</span></span><br><span class="line">identity_feature_column = tf.feature_column.categorical_column_with_identity(</span><br><span class="line">    <span class="attribute">key</span>=<span class="string">'my_feature_b'</span>,</span><br><span class="line">    <span class="attribute">num_buckets</span>=4) # Values [0, 4)</span><br></pre></td></tr></table></figure></p><p>上述代码可以生成下图所示的效果。<br><img src="https://www.tensorflow.org/images/feature_columns/categorical_column_with_identity.jpg" alt></p><h2 id="Categorical-vocabulary-column"><a href="#Categorical-vocabulary-column" class="headerlink" title="Categorical vocabulary column"></a>Categorical vocabulary column</h2><p>顾名思义，Categorical vocabulary column把一个vocabulary中的string映射为数值型的类别特征，是做one-hot编码的很好的方法。在tensorflow中有两种提供词汇表的方法，一种是用list，另一种是用file，对应的feature column分别为：</p><ul><li>tf.feature_column.categorical_column_with_vocabulary_list</li><li>tf.feature_column.categorical_column_with_vocabulary_file</li></ul><p>两者的定义如下：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">    key,</span><br><span class="line">    vocabulary_list,</span><br><span class="line">    <span class="attribute">dtype</span>=None,</span><br><span class="line">    <span class="attribute">default_value</span>=-1,</span><br><span class="line">    <span class="attribute">num_oov_buckets</span>=0</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><ul><li>key: feature名字</li><li>vocabulary_list: 对于category来说，进行转换的list.也就是category列表.</li><li>dtype: 仅仅string和int被支持，其他的类型是无法进行这个操作的.</li><li>default_value: 当不在vocabulary_list中的默认值，这时候num_oov_buckets必须是0.</li><li>num_oov_buckets: 用来处理那些不在vocabulary_list中的值，如果是0，那么使用default_value进行填充;如果大于0，则会在[len(vocabulary_list), len(vocabulary_list)+num_oov_buckets]这个区间上重新计算当前特征的值.</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.feature_column.categorical_column_with_vocabulary_file(</span><br><span class="line">    key,</span><br><span class="line">    vocabulary_file,</span><br><span class="line">    <span class="attribute">vocabulary_size</span>=None,</span><br><span class="line">    <span class="attribute">num_oov_buckets</span>=0,</span><br><span class="line">    <span class="attribute">default_value</span>=None,</span><br><span class="line">    <span class="attribute">dtype</span>=tf.string</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>vocabulary_file: 存储词汇表的文件名</li><li>其他参数的含义与<code>tf.feature_column.categorical_column_with_vocabulary_list</code>相同</li></ul><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Given input "feature_name_from_input_fn" which is a string,</span></span><br><span class="line"><span class="comment"># create a categorical feature by mapping the input to one of</span></span><br><span class="line"><span class="comment"># the elements in the vocabulary list.</span></span><br><span class="line">vocabulary_feature_column =</span><br><span class="line">    tf.feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">        key=feature_name_from_input_fn,</span><br><span class="line">        vocabulary_list=[<span class="string">"kitchenware"</span>, <span class="string">"electronics"</span>, <span class="string">"sports"</span>])</span><br></pre></td></tr></table></figure><p>上述代码得到的结果如下：<br><img src="https://www.tensorflow.org/images/feature_columns/categorical_column_with_vocabulary.jpg" alt></p><p>为了加深理解，进一步做一些测试：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def test_categorical_column_with_vocabulary_list():</span><br><span class="line">    color_data = &#123;<span class="string">'color'</span>: [[<span class="string">'R'</span>, <span class="string">'R'</span>], [<span class="string">'G'</span>, <span class="string">'R'</span>], [<span class="string">'B'</span>, <span class="string">'G'</span>], [<span class="string">'A'</span>, <span class="string">'A'</span>]]&#125;  # <span class="number">4</span>行样本</span><br><span class="line">    builder = _LazyBuilder(color_data)</span><br><span class="line">    color_column = feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">        <span class="string">'color'</span>, [<span class="string">'R'</span>, <span class="string">'G'</span>, <span class="string">'B'</span>], dtype=<span class="keyword">tf</span>.<span class="built_in">string</span>, default_value=-<span class="number">1</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    color_column_tensor = color_column._get_sparse_tensors(builder)</span><br><span class="line">    with <span class="keyword">tf</span>.Session() <span class="keyword">as</span> session:</span><br><span class="line">        session.run(<span class="keyword">tf</span>.global_variables_initializer())</span><br><span class="line">        session.run(<span class="keyword">tf</span>.tables_initializer())</span><br><span class="line">        <span class="keyword">print</span>(session.run([color_column_tensor.id_tensor]))</span><br><span class="line"></span><br><span class="line">    # 将稀疏的转换成dense，也就是one-hot形式，只是multi-hot</span><br><span class="line">    color_column_identy = feature_column.indicator_column(color_column)</span><br><span class="line">    color_dense_tensor = feature_column.input_layer(color_data, [color_column_identy])</span><br><span class="line">    with <span class="keyword">tf</span>.Session() <span class="keyword">as</span> session:</span><br><span class="line">        session.run(<span class="keyword">tf</span>.global_variables_initializer())</span><br><span class="line">        session.run(<span class="keyword">tf</span>.tables_initializer())</span><br><span class="line">        <span class="keyword">print</span>(<span class="string">'use input_layer'</span> + <span class="string">'_'</span> * <span class="number">40</span>)</span><br><span class="line">        <span class="keyword">print</span>(session.run([color_dense_tensor]))</span><br><span class="line"></span><br><span class="line">test_categorical_column_with_vocabulary_list()</span><br></pre></td></tr></table></figure></p><p>注意:</p><ul><li>input_layer: 只接受dense tensor</li><li>tables_initializer: 在sparser的时候使用的，如果不进行初始化会出现 Table not initialized. [Node: hash_table_Lookup = LookupTableFindV2 这样的异常</li></ul><p>结果如下：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[SparseTensorValue(indices=array([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">1</span>]], dtype=int64), values=array([ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">0</span>,  <span class="number">2</span>,  <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>], dtype=int64), dense_shape=array([<span class="number">4</span>, <span class="number">2</span>], dtype=int64))]</span><br><span class="line">use input_layer________________________________________</span><br><span class="line">[array([[<span class="number">2.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]], dtype=float32)]</span><br></pre></td></tr></table></figure></p><h2 id="Hashed-Column"><a href="#Hashed-Column" class="headerlink" title="Hashed Column"></a>Hashed Column</h2><p>为类别特征提供词汇表有时候会过于繁琐，特别是在词汇表非常大的时候，词汇表会非常消耗内存。<code>tf.feature_column.categorical_column_with_hash_bucket</code>允许用户指定类别的总数，通过hash的方式来得到最终的类别ID。伪代码如下：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pseudocode</span></span><br><span class="line"><span class="attr">feature_id</span> = hash(raw_feature) % hash_buckets_size</span><br></pre></td></tr></table></figure></p><p>用hash的方式产生类别ID，不可避免地会遇到hash冲突的问题，即可有多多个原来不相同的类别会产生相同的类别ID。因此，设置hash_bucket_size参数会显得比较重要。实践表明，hash冲突不会对神经网络模型造成太大的影响，因为模型可以通过其他特征作进一步区分。</p><p><img src="https://www.tensorflow.org/images/feature_columns/hashed_column.jpg" alt></p><p>同样来做一些测试，看看最终效果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_categorical_column_with_hash_bucket</span><span class="params">()</span>:</span></span><br><span class="line">    color_data = &#123;<span class="string">'color'</span>: [[<span class="number">2</span>], [<span class="number">5</span>], [<span class="number">-1</span>], [<span class="number">0</span>]]&#125;  <span class="comment"># 4行样本</span></span><br><span class="line">    builder = _LazyBuilder(color_data)</span><br><span class="line">    color_column = feature_column.categorical_column_with_hash_bucket(<span class="string">'color'</span>, <span class="number">7</span>, dtype=tf.int32)</span><br><span class="line">    color_column_tensor = color_column._get_sparse_tensors(builder)</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">        session.run(tf.global_variables_initializer())</span><br><span class="line">        session.run(tf.tables_initializer())</span><br><span class="line">        print(session.run([color_column_tensor.id_tensor]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将稀疏的转换成dense，也就是one-hot形式，只是multi-hot</span></span><br><span class="line">    color_column_identy = feature_column.indicator_column(color_column)</span><br><span class="line">    color_dense_tensor = feature_column.input_layer(color_data, [color_column_identy])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">        session.run(tf.global_variables_initializer())</span><br><span class="line">        session.run(tf.tables_initializer())</span><br><span class="line">        print(<span class="string">'use input_layer'</span> + <span class="string">'_'</span> * <span class="number">40</span>)</span><br><span class="line">        print(session.run([color_dense_tensor]))</span><br><span class="line"></span><br><span class="line">test_categorical_column_with_hash_bucket()</span><br></pre></td></tr></table></figure></p><p>运行结果如下：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[SparseTensorValue(indices=array([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">0</span>]], dtype=int64), values=array([<span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>], dtype=int64), dense_shape=array([<span class="number">4</span>, <span class="number">1</span>], dtype=int64))]</span><br><span class="line">use input_layer________________________________________</span><br><span class="line">[array([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]], dtype=float32)]</span><br></pre></td></tr></table></figure></p><p>需要注意的是，使用hash bucket的时候，原始值中-1或者空字符串””会被忽略，不会输出结果。</p><h2 id="Crossed-column"><a href="#Crossed-column" class="headerlink" title="Crossed column"></a>Crossed column</h2><p>交叉组合特征也是一种很常用的特征工程手段，尤其是在使用LR模型时。Crossed column仅仅适用于sparser特征，产生的依然是sparsor特征。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf<span class="selector-class">.feature_column</span><span class="selector-class">.crossed_column</span>(</span><br><span class="line">    keys,</span><br><span class="line">    hash_bucket_size,</span><br><span class="line">    hash_key=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p>具体地，Crossed特征对keys的笛卡尔积执行hash操作，再把hash的结果对hash_bucket_size取模得到最终的结果：<code>Hash(cartesian product of features) % hash_bucket_size</code>。</p><p>测试代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_crossed_column</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">""" crossed column测试 """</span></span><br><span class="line">    featrues = &#123;</span><br><span class="line">        <span class="string">'price'</span>: [[<span class="string">'A'</span>], [<span class="string">'B'</span>], [<span class="string">'C'</span>]],</span><br><span class="line">        <span class="string">'color'</span>: [[<span class="string">'R'</span>], [<span class="string">'G'</span>], [<span class="string">'B'</span>]]</span><br><span class="line">    &#125;</span><br><span class="line">    price = feature_column.categorical_column_with_vocabulary_list(<span class="string">'price'</span>, [<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'D'</span>])</span><br><span class="line">    color = feature_column.categorical_column_with_vocabulary_list(<span class="string">'color'</span>, [<span class="string">'R'</span>, <span class="string">'G'</span>, <span class="string">'B'</span>])</span><br><span class="line">    p_x_c = feature_column.crossed_column([price, color], <span class="number">16</span>)</span><br><span class="line">    p_x_c_identy = feature_column.indicator_column(p_x_c)</span><br><span class="line">    p_x_c_identy_dense_tensor = feature_column.input_layer(featrues, [p_x_c_identy])</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">        session.run(tf.global_variables_initializer())</span><br><span class="line">        session.run(tf.tables_initializer())</span><br><span class="line">        print(session.run([p_x_c_identy_dense_tensor]))</span><br><span class="line">test_crossed_column()</span><br></pre></td></tr></table></figure></p><p>结果：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[array([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line">      dtype=float32)]</span><br></pre></td></tr></table></figure></p><h2 id="Indicator-and-embedding-columns"><a href="#Indicator-and-embedding-columns" class="headerlink" title="Indicator and embedding columns"></a>Indicator and embedding columns</h2><p>Indicator columns 和 embedding columns 不能直接作用在原始特征上，而是作用在categorical columns上。</p><p>在前面的众多例子中，我们已经使用过indicator_column来把categorical column得到的稀疏tensor转换为one-hot或者multi-hot形式的稠密tensor，这里就不赘述了。</p><p>当某些特征的类别数量非常大时，使用indicator_column来把原始数据转换为神经网络的输入就变得非常不灵活，这时通常使用embedding column把原始特征映射为一个低维稠密的实数向量。同一类别的embedding向量间的距离通常可以用来度量类别直接的相似性。</p><p>Embedding column与indicator column之间的区别可以用下图表示。<br><img src="https://www.tensorflow.org/images/feature_columns/embedding_vs_indicator.jpg" alt></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.feature_column.embedding_column(</span><br><span class="line">    categorical_column,</span><br><span class="line">    dimension,</span><br><span class="line">    <span class="attribute">combiner</span>=<span class="string">'mean'</span>,</span><br><span class="line">    <span class="attribute">initializer</span>=None,</span><br><span class="line">    <span class="attribute">ckpt_to_load_from</span>=None,</span><br><span class="line">    <span class="attribute">tensor_name_in_ckpt</span>=None,</span><br><span class="line">    <span class="attribute">max_norm</span>=None,</span><br><span class="line">    <span class="attribute">trainable</span>=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>categorical_column: 使用categoryical_column产生的sparsor column</li><li>dimension: 定义embedding的维数</li><li>combiner: 对于多个entries进行的推导。默认是meam, 但是 sqrtn在词袋模型中，有更好的准确度。</li><li>initializer: 初始化方法，默认使用高斯分布来初始化。</li><li>tensor_name_in_ckpt: 可以从check point中恢复</li><li>ckpt_to_load_from: check point file，这是在 tensor_name_in_ckpt 不为空的情况下设置的.</li><li>max_norm: 默认是l2</li><li>trainable: 是否可训练的，默认是true</li></ul><p>测试代码：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def test_embedding():</span><br><span class="line">    <span class="keyword">tf</span>.set_random_seed(<span class="number">1</span>)</span><br><span class="line">    color_data = &#123;<span class="string">'color'</span>: [[<span class="string">'R'</span>, <span class="string">'G'</span>], [<span class="string">'G'</span>, <span class="string">'A'</span>], [<span class="string">'B'</span>, <span class="string">'B'</span>], [<span class="string">'A'</span>, <span class="string">'A'</span>]]&#125;  # <span class="number">4</span>行样本</span><br><span class="line">    builder = _LazyBuilder(color_data)</span><br><span class="line">    color_column = feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">        <span class="string">'color'</span>, [<span class="string">'R'</span>, <span class="string">'G'</span>, <span class="string">'B'</span>], dtype=<span class="keyword">tf</span>.<span class="built_in">string</span>, default_value=-<span class="number">1</span></span><br><span class="line">    )</span><br><span class="line">    color_column_tensor = color_column._get_sparse_tensors(builder)</span><br><span class="line"></span><br><span class="line">    color_embeding = feature_column.embedding_column(color_column, <span class="number">4</span>, combiner=<span class="string">'sum'</span>)</span><br><span class="line">    color_embeding_dense_tensor = feature_column.input_layer(color_data, [color_embeding])</span><br><span class="line"></span><br><span class="line">    with <span class="keyword">tf</span>.Session() <span class="keyword">as</span> session:</span><br><span class="line">        session.run(<span class="keyword">tf</span>.global_variables_initializer())</span><br><span class="line">        session.run(<span class="keyword">tf</span>.tables_initializer())</span><br><span class="line">        <span class="keyword">print</span>(session.run([color_column_tensor.id_tensor]))</span><br><span class="line">        <span class="keyword">print</span>(<span class="string">'embeding'</span> + <span class="string">'_'</span> * <span class="number">40</span>)</span><br><span class="line">        <span class="keyword">print</span>(session.run([color_embeding_dense_tensor]))</span><br><span class="line"></span><br><span class="line">test_embedding()</span><br></pre></td></tr></table></figure></p><p>测试结果：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[SparseTensorValue(indices=array([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">1</span>]], dtype=int64), values=array([ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">1</span>, <span class="number">-1</span>,  <span class="number">2</span>,  <span class="number">2</span>, <span class="number">-1</span>, <span class="number">-1</span>], dtype=int64), dense_shape=array([<span class="number">4</span>, <span class="number">2</span>], dtype=int64))]</span><br><span class="line">embeding________________________________________</span><br><span class="line">[array([[<span class="number">-0.8339818</span> , <span class="number">-0.4975947</span> ,  <span class="number">0.09368954</span>,  <span class="number">0.16094571</span>],</span><br><span class="line">       [<span class="number">-0.6342659</span> , <span class="number">-0.19216162</span>,  <span class="number">0.18877633</span>,  <span class="number">0.17648602</span>],</span><br><span class="line">       [ <span class="number">1.5531666</span> ,  <span class="number">0.27847385</span>,  <span class="number">0.12863553</span>,  <span class="number">1.2628161</span> ],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ]],</span><br><span class="line">      dtype=float32)]</span><br></pre></td></tr></table></figure></p><p>从上面的测试结果可以看出不在vocabulary里的数据’A’在经过<code>categorical_column_with_vocabulary_list</code>操作时映射为默认值-1，而<strong>默认值-1在embeding column时映射为0向量</strong>，这是一个很有用的特性，可以用-1来填充一个不定长的ID序列，这样可以得到定长的序列，然后经过embedding column之后，填充的-1值不影响原来的结果。在下一篇文章中，我会通过一个例子来演示这个特性。</p><p>有时候在同一个网络模型中，有多个特征可能需要共享相同的embeding映射空间，比如用户历史行为序列中的商品ID和候选商品ID，这时候可以用到<code>tf.feature_column.shared_embedding_columns</code>。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.feature_column.shared_embedding_columns(</span><br><span class="line">    categorical_columns,</span><br><span class="line">    dimension,</span><br><span class="line">    <span class="attribute">combiner</span>=<span class="string">'mean'</span>,</span><br><span class="line">    <span class="attribute">initializer</span>=None,</span><br><span class="line">    <span class="attribute">shared_embedding_collection_name</span>=None,</span><br><span class="line">    <span class="attribute">ckpt_to_load_from</span>=None,</span><br><span class="line">    <span class="attribute">tensor_name_in_ckpt</span>=None,</span><br><span class="line">    <span class="attribute">max_norm</span>=None,</span><br><span class="line">    <span class="attribute">trainable</span>=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>categorical_columns 为需要共享embeding映射空间的类别特征列表</li><li>其他参数与embedding column类似</li></ul><p>测试代码：<br><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def test_shared_embedding_column_with_hash_bucket():</span><br><span class="line">    color_data = &#123;<span class="string">'color'</span>: <span class="string">[[2, 2], [5, 5], [0, -1], [0, 0]]</span>,</span><br><span class="line">                  <span class="string">'color2'</span>: <span class="string">[[2], [5], [-1], [0]]</span>&#125;  # <span class="number">4</span>行样本</span><br><span class="line">    builder = _LazyBuilder(color_data)</span><br><span class="line">    color_column = feature_column.categorical_column_with_hash_bucket(<span class="string">'color'</span>, <span class="number">7</span>, dtype=tf.int32)</span><br><span class="line">    color_column_tensor = color_column._get_sparse_tensors(builder)</span><br><span class="line">    color_column2 = feature_column.categorical_column_with_hash_bucket(<span class="string">'color2'</span>, <span class="number">7</span>, dtype=tf.int32)</span><br><span class="line">    color_column_tensor2 = color_column2._get_sparse_tensors(builder)</span><br><span class="line">    with tf.Session() as session:</span><br><span class="line">        session.run(tf.global_variables_initializer())</span><br><span class="line">        session.run(tf.tables_initializer())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'not use input_layer'</span> + <span class="string">'_'</span> * <span class="number">40</span>)</span><br><span class="line">        <span class="built_in">print</span>(session.run([color_column_tensor.id_tensor]))</span><br><span class="line">        <span class="built_in">print</span>(session.run([color_column_tensor2.id_tensor]))</span><br><span class="line"></span><br><span class="line">    # 将稀疏的转换成dense，也就是one-hot形式，只是multi-hot</span><br><span class="line">    color_column_embed = feature_column.shared_embedding_columns([color_column2, color_column], <span class="number">3</span>, combiner=<span class="string">'sum'</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(color_column_embed))</span><br><span class="line">    color_dense_tensor = feature_column.input_layer(color_data, color_column_embed)</span><br><span class="line"></span><br><span class="line">    with tf.Session() as session:</span><br><span class="line">        session.run(tf.global_variables_initializer())</span><br><span class="line">        session.run(tf.tables_initializer())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'use input_layer'</span> + <span class="string">'_'</span> * <span class="number">40</span>)</span><br><span class="line">        <span class="built_in">print</span>(session.run(color_dense_tensor))</span><br><span class="line"></span><br><span class="line">test_shared_embedding_column_with_hash_bucket()</span><br></pre></td></tr></table></figure></p><p>测试结果：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">not use input_layer________________________________________</span><br><span class="line">[SparseTensorValue(indices=array([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">1</span>]], dtype=int64), values=array([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], dtype=int64), dense_shape=array([<span class="number">4</span>, <span class="number">2</span>], dtype=int64))]</span><br><span class="line">[SparseTensorValue(indices=array([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">0</span>]], dtype=int64), values=array([<span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>], dtype=int64), dense_shape=array([<span class="number">4</span>, <span class="number">1</span>], dtype=int64))]</span><br><span class="line">&lt;class '<span class="type">list</span>'&gt;</span><br><span class="line">use input_layer________________________________________</span><br><span class="line">[[ <span class="number">0.37802923</span> <span class="number">-0.27973637</span>  <span class="number">0.11547407</span>  <span class="number">0.75605845</span> <span class="number">-0.55947274</span>  <span class="number">0.23094814</span>]</span><br><span class="line"> [<span class="number">-0.5264772</span>   <span class="number">0.86587846</span> <span class="number">-0.36023238</span> <span class="number">-1.0529544</span>   <span class="number">1.7317569</span>  <span class="number">-0.72046477</span>]</span><br><span class="line"> [ <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>         <span class="number">-0.9269535</span>  <span class="number">-0.17690836</span>  <span class="number">0.42011076</span>]</span><br><span class="line"> [<span class="number">-0.9269535</span>  <span class="number">-0.17690836</span>  <span class="number">0.42011076</span> <span class="number">-1.853907</span>   <span class="number">-0.35381672</span>  <span class="number">0.8402215</span> ]]</span><br></pre></td></tr></table></figure></p><p>需要注意的是，<strong><code>tf.feature_column.shared_embedding_columns</code>的返回值是一个与参数categorical_columns维数相同的列表</strong>。</p><h2 id="Weighted-categorical-column"><a href="#Weighted-categorical-column" class="headerlink" title="Weighted categorical column"></a>Weighted categorical column</h2><p>有时候我们需要给一个类别特征赋予一定的权重，比如给用户行为序列按照行为发生的时间到某个特定时间的差来计算不同的权重，这是可以用到weighted_categorical_column。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf<span class="selector-class">.feature_column</span><span class="selector-class">.weighted_categorical_column</span>(</span><br><span class="line">    categorical_column,</span><br><span class="line">    weight_feature_key,</span><br><span class="line">    dtype=tf.float32</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p>测试代码：<br><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def test_weighted_categorical_column():</span><br><span class="line">    color_data = &#123;<span class="string">'color'</span>: [[<span class="string">'R'</span>], [<span class="string">'G'</span>], [<span class="string">'B'</span>], [<span class="string">'A'</span>]],</span><br><span class="line">                  <span class="string">'weight'</span>: [[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">4.0</span>], [<span class="number">8.0</span>]]&#125;  # <span class="number">4</span>行样本</span><br><span class="line">    color_column = feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">        <span class="string">'color'</span>, [<span class="string">'R'</span>, <span class="string">'G'</span>, <span class="string">'B'</span>], dtype=tf.string, default_value=<span class="number">-1</span></span><br><span class="line">    )</span><br><span class="line">    color_weight_categorical_column = feature_column.weighted_categorical_column(color_column, <span class="string">'weight'</span>)</span><br><span class="line">    builder = <span class="symbol">_LazyBuilder</span>(color_data)</span><br><span class="line">    with tf.<span class="symbol">Session</span>() as session:</span><br><span class="line">        id_tensor, weight = color_weight_categorical_column.<span class="symbol">_get_sparse_tensors</span>(builder)</span><br><span class="line">        session.run(tf.global_variables_initializer())</span><br><span class="line">        session.run(tf.tables_initializer())</span><br><span class="line">        print(<span class="string">'weighted categorical'</span> + <span class="string">'-'</span> * <span class="number">40</span>)</span><br><span class="line">        print(session.run([id_tensor]))</span><br><span class="line">        print(<span class="string">'-'</span> * <span class="number">40</span>)</span><br><span class="line">        print(session.run([weight]))</span><br><span class="line">test_weighted_categorical_column()</span><br></pre></td></tr></table></figure></p><p>测试结果：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">weighted categorical----------------------------------------</span><br><span class="line">[SparseTensorValue(indices=array([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">0</span>]], dtype=int64), values=array([ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>, <span class="number">-1</span>], dtype=int64), dense_shape=array([<span class="number">4</span>, <span class="number">1</span>], dtype=int64))]</span><br><span class="line">----------------------------------------</span><br><span class="line">[SparseTensorValue(indices=array([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">0</span>]], dtype=int64), values=array([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">4.</span>, <span class="number">8.</span>], dtype=float32), dense_shape=array([<span class="number">4</span>, <span class="number">1</span>], dtype=int64))]</span><br></pre></td></tr></table></figure></p><p>可以看到，相对于前面其他categorical_column来说多了weight这个tensor。weighted_categorical_column的一个用例就是，<strong>weighted_categorical_column的结果传入给shared_embedding_columns可以对ID序列的embeding向量做加权融合</strong>。限于篇幅的原因，完整的使用案例请期待下一篇博文。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.tensorflow.org/guide/feature_columns?hl=zh-cn" target="_blank" rel="noopener">https://www.tensorflow.org/guide/feature_columns?hl=zh-cn</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;特征工程是机器学习流程中重要的一个环节，即使是通常用来做端到端学习的深度学习模型在训练之前也免不了要做一些特征工程相关的工作。Tensorflow平台提供的FeatureColumn API为特征工程提供了强大的支持。&lt;/p&gt;
&lt;p&gt;Feature cloumns是原始数据和Estimator模型之间的桥梁，它们被用来把各种形式的原始数据转换为模型能够使用的格式。深度神经网络只能处理数值数据，网络中的每个神经元节点执行一些针对输入数据和网络权重的乘法和加法运算。然而，现实中的有很多非数值的类别数据，比如产品的品牌、类目等，这些数据如果不加转换，神经网络是无法处理的。另一方面，即使是数值数据，在仍给网络进行训练之前有时也需要做一些处理，比如标准化、离散化等。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.tensorflow.org/images/feature_columns/inputs_to_model_bridge.jpg&quot; alt&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="tensorflow" scheme="http://xudongyang.coding.me/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>基于Tensorflow高阶API构建大规模分布式深度学习模型系列之自定义Estimator（以文本分类CNN模型为例）</title>
    <link href="http://xudongyang.coding.me/tensorflow-word-cnn/"/>
    <id>http://xudongyang.coding.me/tensorflow-word-cnn/</id>
    <published>2018-08-07T06:08:44.000Z</published>
    <updated>2019-04-02T02:43:36.872Z</updated>
    
    <content type="html"><![CDATA[<p>Tensorflow在1.4版本中引入了<code>tf.estimator.train_and_evaluate</code>函数，用来替换老版中Experiment类提供的功能。<code>tf.estimator.train_and_evaluate</code>简化了训练、评估和导出Estimator模型的过程，抽象了模型分布式训练和评估的细节，使得同样的代码在本地与分布式集群上的行为一致。</p><p>这意味着使用<code>train_and_evaluate</code> API，我们可以在本地和分布式集群上、不同的设备和硬件上跑同样的代码，而不需要修改代码已适应不同的部署环境。而且训练之后的模型可以很方便地导出以便在打分服务（tensorflow serving）中使用。</p><p>本文简要介绍如何自定义Estimator模型并通过使用<code>tf.estimator.train_and_evaluate</code>完成训练和评估。</p><p>主要步骤：</p><ol><li>构建自己的Estimator模型</li><li>定义在训练和测试过程中数据如何输入给模型</li><li>定义传递给<code>tf.estimator.train_and_evaluate</code>函数的训练、评估和导出的详述参数(TrainSpec and EvalSpec)</li><li>使用<code>tf.estimator.train_and_evaluate</code>训练并评估模型</li></ol><a id="more"></a><h2 id="文本分类任务"><a href="#文本分类任务" class="headerlink" title="文本分类任务"></a>文本分类任务</h2><p>在介绍详细的模型开发步骤之前，先看一下我们要完成的作品的目标：文本分类。文本分类在很多场景都有应用，这里就不详述了。</p><p>我们使用的数据集是《<a href="https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz" target="_blank" rel="noopener">DBPedia Ontology Classification Dataset</a>》(可点击下载)，是从数据集DBpedia 2014中挑选的14个类别的互不重叠的本体（Company, EducationalInstitution, Artist, Athlete, OfficeHolder, MeanOfTransportation, Building, NaturalPlace, Village, Animal, Plant, Album, Film, WrittenWork），每个本体类别随机选择了40,000 个训练样本和5,000个测试样本。因此，总共有560,000个训练样本和70,000个测试样本。</p><blockquote><p>The files train.csv and test.csv contain all the training samples as comma-sparated values. There are 3 columns in them, corresponding to class index (1 to 14), title and content. The title and content are escaped using double quotes (“), and any internal double quote is escaped by 2 double quotes (“”). There are no new lines in title or content.</p></blockquote><p>在训练过程了，我们丢弃了title字段，仅使用content字段的内容去拟合类标签。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>文本实现一种基于CNN网络的文本分类模型，该模型由Yoon Kim在论文《<a href="https://arxiv.org/abs/1408.5882" target="_blank" rel="noopener">Convolutional Neural Networks for Sentence Classification</a>》中提出，网络结构如下图所示。</p><p><img src="/tensorflow-word-cnn/arch.png" alt="img"></p><p>具体地，网络的输入层为一个定长为$n$的句子（不定长的句子用固定的tokenerm填充），句子中的每个词经过embedding层映射为一个$m$维的实数向量，这样每个句子就转换为一个二维的$n \times m$的矩阵。</p><p>然后在此矩阵上应用多个不同大小的卷积操作，得到多个特征映射（feature map）。例如，一个窗口大小为$h \times m$的卷积过滤器$w \in R^{hm}$作用在包含$h$个词向量的窗口上，得到一个新的特征$c_i=f(w \cdot x_{i:i+h-1} + b)$，其中$b$是偏重，$f$是一个非线性函数，比如tanh函数。该卷积过滤器作用在当前句子对应的每一个可能的词向量窗口$\{x_{1:h},x_{2:h+1}, \cdots, x_{n-h+1:n} \}$上，得到一个特征映射${\bf {c}}=[ c_1, c_2, \cdots, c_{n-h+1} ]$ 。</p><p>网络会同时应用多个不同size $h$的卷积操作，这样就能得到多组特征映射，每组特征的维数为$R^{n-h+1}$。假设一共有$k$组特征映射。接着在每组特征映射上执行一个最大池化操作（max-overtime<br>pooling operation），即$\hat{c}=max\{ {\bf {c}} \}$。最大池化操作用来捕捉每一组特征映射中最重要的特征，同时使得不同size的特征映射塌陷到一个固定维数为1的特征上。这样$k$组特征映射就映射到一个$k$维的特征表示向量上。</p><p>最后，经过了前面的embedding层，卷积层，池化层之后，再接几个全连接层，这样网络结构就完整了。</p><p><img src="/tensorflow-word-cnn/word_cnn.png" alt="tensorboard"></p><h2 id="构建Estimator"><a href="#构建Estimator" class="headerlink" title="构建Estimator"></a>构建Estimator</h2><p>Tensorflow的Estimator类提供了分布式模型训练和评估的内置支持，屏蔽了不同底层硬件平台（CPU、GPU、TPU）的差异。因此，建议大家总是使用Estimator来封装自己的模型。Tensorflow还提供了一些“Pre-made”的Estimator的子类可以用来高效地创建一些常用的标准模型，比如常用的“wide and deep”模型就可以用<code>DNNLinearCombinedClassifier</code>来创建。</p><p>Estimator的核心是模型函数（model function），模型函数构建训练、评估和预测用的计算图。当使用pre-made的Estimator时，模型函数已经为我们实现好了。当我们使用自定义的Estimator来创建自己的模型时，最重要的工作就是编写自己的模型函数。</p><p>模型函数的签名如下：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def my_model_fn(</span><br><span class="line">   features, # This is batch_features <span class="keyword">from</span> input_fn</span><br><span class="line">   labels,   # This is batch_labels <span class="keyword">from</span> input_fn</span><br><span class="line">   mode,     # An<span class="built_in"> instance </span>of tf.estimator.ModeKeys</span><br><span class="line">   params):  # Additional configuration</span><br></pre></td></tr></table></figure></p><p>前两个参数是输入函数(input_fn)返回的特性和标签，mode参数表明调用者是在训练、预测还是评估，params是其他一些自定义参数，通常是一个dict。</p><div class="table-container"><table><thead><tr><th>Estimator method</th><th>Estimator Mode</th></tr></thead><tbody><tr><td>train()</td><td>ModeKeys.TRAIN</td></tr><tr><td>evaluate()</td><td>ModeKeys.EVAL</td></tr><tr><td>predict()</td><td>ModeKeys.PREDICT</td></tr></tbody></table></div><p>在模型函数内部，需要定义网络对应的计算图（graph)，并为模型在三个不同的阶段（训练、评估、预测）指定额外的操作，通过EstimatorSpec对象返回。</p><p>在训练阶段返回的EstimatorSpec对象需要包含计算loss和最小化loss的操作（op）；在评估阶段返回的EstimatorSpec对象需要包含计算metrics的操作，已经跟模型导出有个的操作；在预测阶段返回的EstimatorSpec对象需要包含跟获取预测结果有个的操作。具体如何定义这些EstimatorSpec对象可以参考<a href="https://www.tensorflow.org/guide/custom_estimators?hl=zh-cn" target="_blank" rel="noopener">官方文档</a>。</p><p>通常情况下，自己定义不同阶段的EstimatorSpec对象比较麻烦，这时可以用到另一个高阶API Head来帮忙简化开发任务。</p><p>我们的整个模型函数的代码如下：<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">def my_model(features, labels, mode, params):</span><br><span class="line">  <span class="attr">sentence</span> = features['sentence']</span><br><span class="line">  <span class="comment"># Get word embeddings for each token in the sentence</span></span><br><span class="line">  <span class="attr">embeddings</span> = tf.get_variable(<span class="attr">name="embeddings",</span> <span class="attr">dtype=tf.float32,</span></span><br><span class="line">                               <span class="attr">shape=[params["vocab_size"],</span> FLAGS.embedding_size])</span><br><span class="line">  <span class="attr">sentence</span> = tf.nn.embedding_lookup(embeddings, sentence) <span class="comment"># shape:(batch, sentence_len, embedding_size)</span></span><br><span class="line">  <span class="comment"># add a channel dim, required by the conv2d and max_pooling2d method</span></span><br><span class="line">  <span class="attr">sentence</span> = tf.expand_dims(sentence, -<span class="number">1</span>) <span class="comment"># shape:(batch, sentence_len/height, embedding_size/width, channels=1)</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">pooled_outputs</span> = []</span><br><span class="line">  for filter_size <span class="keyword">in</span> params[<span class="string">"filter_sizes"</span>]:</span><br><span class="line">      <span class="attr">conv</span> = tf.layers.conv2d(</span><br><span class="line">          sentence,</span><br><span class="line">          <span class="attr">filters=FLAGS.num_filters,</span></span><br><span class="line">          <span class="attr">kernel_size=[filter_size,</span> FLAGS.embedding_size],</span><br><span class="line">          <span class="attr">strides=(1,</span> <span class="number">1</span>),</span><br><span class="line">          <span class="attr">padding="VALID",</span></span><br><span class="line">          <span class="attr">activation=tf.nn.relu)</span></span><br><span class="line">      <span class="attr">pool</span> = tf.layers.max_pooling2d(</span><br><span class="line">          conv,</span><br><span class="line">          <span class="attr">pool_size=[FLAGS.sentence_max_len</span> - filter_size + <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">          <span class="attr">strides=(1,</span> <span class="number">1</span>),</span><br><span class="line">          <span class="attr">padding="VALID")</span></span><br><span class="line">      pooled_outputs.append(pool)</span><br><span class="line">  <span class="attr">h_pool</span> = tf.concat(pooled_outputs, <span class="number">3</span>) <span class="comment"># shape: (batch, 1, len(filter_size) * embedding_size, 1)</span></span><br><span class="line">  <span class="attr">h_pool_flat</span> = tf.reshape(h_pool, [-<span class="number">1</span>, FLAGS.num_filters * len(params[<span class="string">"filter_sizes"</span>])]) <span class="comment"># shape: (batch, len(filter_size) * embedding_size)</span></span><br><span class="line">  <span class="keyword">if</span> 'dropout_rate' <span class="keyword">in</span> params <span class="literal">and</span> params['dropout_rate'] &gt; <span class="number">0.0</span>:</span><br><span class="line">    <span class="attr">h_pool_flat</span> = tf.layers.dropout(h_pool_flat, params['dropout_rate'], <span class="attr">training=(mode</span> == tf.estimator.ModeKeys.TRAIN))</span><br><span class="line">  <span class="attr">logits</span> = tf.layers.dense(h_pool_flat, FLAGS.num_classes, <span class="attr">activation=None)</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">optimizer</span> = tf.train.AdagradOptimizer(<span class="attr">learning_rate=params['learning_rate'])</span></span><br><span class="line">  def _train_op_fn(loss):</span><br><span class="line">    return optimizer.minimize(loss, <span class="attr">global_step=tf.train.get_global_step())</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">my_head</span> = tf.contrib.estimator.multi_class_head(<span class="attr">n_classes=FLAGS.num_classes)</span></span><br><span class="line">  return my_head.create_estimator_spec(</span><br><span class="line">    <span class="attr">features=features,</span></span><br><span class="line">    <span class="attr">mode=mode,</span></span><br><span class="line">    <span class="attr">labels=labels,</span></span><br><span class="line">    <span class="attr">logits=logits,</span></span><br><span class="line">    <span class="attr">train_op_fn=_train_op_fn</span></span><br><span class="line">  )</span><br></pre></td></tr></table></figure></p><h2 id="定义输入流-input-pipeline"><a href="#定义输入流-input-pipeline" class="headerlink" title="定义输入流 input pipeline"></a>定义输入流 input pipeline</h2><p>在Tensorflow中定义网络输入推荐使用<a href="https://zhuanlan.zhihu.com/p/38421397" target="_blank" rel="noopener">Dataset API</a>。</p><p>在本文的文本分类任务中，原始训练数据的格式在前面已经介绍过，每一行是逗号分隔的3列的csv格式的数据。第一列是类标签，第三列是我们用来作为输入的句子。首先我们需要做一下预处理，把特殊符号和一些非字母类的其他文本内容去掉，然后构建完整的词汇表。词汇表用来把词映射到唯一的一个数字ID。在处理输入的过程中，我们需要依赖词到ID的映射来处理原始输入。最终构建的训练样本的形式为&lt;[word_id list], class_label&gt;。</p><p>输入函数的代码如下，完整的代码请访问github。输入函数中，我们使用了<code>tf.contrib.lookup.index_table_from_file</code>函数来负责把词汇表映射到唯一的ID。</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def input_fn(path_csv, path_vocab, shuffle_buffer_size, num_oov_buckets):</span><br><span class="line">  vocab = tf.contrib.<span class="keyword">lookup</span>.index_table_from_file(path_vocab, num_oov_buckets=num_oov_buckets)</span><br><span class="line">  # Load csv <span class="keyword">file</span>, <span class="keyword">one</span> example per <span class="keyword">line</span></span><br><span class="line">  dataset = tf.data.TextLineDataset(path_csv)</span><br><span class="line">  # Convert <span class="keyword">line</span> into <span class="keyword">list</span> of tokens, splitting <span class="keyword">by</span> white space; then convert each <span class="keyword">token</span> to <span class="keyword">an</span> unique id</span><br><span class="line">  dataset = dataset.map(lambda <span class="keyword">line</span>: parse_line(<span class="keyword">line</span>, vocab))</span><br><span class="line">  <span class="keyword">if</span> shuffle_buffer_size &gt; 0:</span><br><span class="line">    dataset = dataset.shuffle(shuffle_buffer_size).<span class="keyword">repeat</span>()</span><br><span class="line">  dataset = dataset.batch(FLAGS.batch_size).prefetch(1)</span><br><span class="line">  <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure><h2 id="模型的训练"><a href="#模型的训练" class="headerlink" title="模型的训练"></a>模型的训练</h2><p>定义好模型函数与输入函数之后，就可以用Estimator封装好分类器。同时需要定义estimator需要的TrainSpec和EvalSpec，把训练数据和评估数据喂给模型，这样就万事俱备了，最后只需要调用<code>tf.estimator.train_and_evaluate</code>就可以开始训练和评估模型了。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">classifier = tf<span class="selector-class">.estimator</span><span class="selector-class">.Estimator</span>(</span><br><span class="line">  model_fn=my_model,</span><br><span class="line">  params=&#123;</span><br><span class="line">    <span class="string">'vocab_size'</span>: config[<span class="string">"vocab_size"</span>],</span><br><span class="line">    <span class="string">'filter_sizes'</span>: map(int, FLAGS<span class="selector-class">.filter_sizes</span><span class="selector-class">.split</span>(<span class="string">','</span>)),</span><br><span class="line">    <span class="string">'learning_rate'</span>: FLAGS<span class="selector-class">.learning_rate</span>,</span><br><span class="line">    <span class="string">'dropout_rate'</span>: FLAGS.dropout_rate</span><br><span class="line">  &#125;,</span><br><span class="line">  config=tf<span class="selector-class">.estimator</span><span class="selector-class">.RunConfig</span>(model_dir=FLAGS<span class="selector-class">.model_dir</span>, save_checkpoints_steps=FLAGS.save_checkpoints_steps)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_spec = tf<span class="selector-class">.estimator</span><span class="selector-class">.TrainSpec</span>(</span><br><span class="line">  input_fn=lambda: input_fn(path_train, path_words, FLAGS<span class="selector-class">.shuffle_buffer_size</span>, config[<span class="string">"num_oov_buckets"</span>]),</span><br><span class="line">  max_steps=FLAGS.train_steps</span><br><span class="line">)</span><br><span class="line">input_fn_for_eval = lambda: input_fn(path_eval, path_words, <span class="number">0</span>, config[<span class="string">"num_oov_buckets"</span>])</span><br><span class="line">eval_spec = tf<span class="selector-class">.estimator</span><span class="selector-class">.EvalSpec</span>(input_fn=input_fn_for_eval, throttle_secs=<span class="number">300</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf<span class="selector-class">.estimator</span><span class="selector-class">.train_and_evaluate</span>(classifier, train_spec, eval_spec)</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文以文本分类任务为例，详细介绍了如何创建一个基于Estimator接口的tensorflow模型，并使用<code>tf.estimator.train_and_evaluate</code>来完成模型训练和评估的完整过程。</p><p>完整源代码链接：<a href="https://github.com/yangxudong/deeplearning/tree/master/word_cnn" target="_blank" rel="noopener">yangxudong/deeplearning</a></p><p>论文《Convolutional Neural Networks for Sentence Classification》下载地址：<a href="https://arxiv.org/abs/1408.5882" target="_blank" rel="noopener">https://arxiv.org/abs/1408.5882</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://cloud.google.com/blog/products/gcp/easy-distributed-training-with-tensorflow-using-tfestimatortrain-and-evaluate-on-cloud-ml-engine" target="_blank" rel="noopener">Easy distributed training with TensorFlow using tf.estimator.train_and_evaluate on Cloud ML Engine</a></li><li><a href="https://www.tensorflow.org/guide/custom_estimators?hl=zh-cn" target="_blank" rel="noopener">Tensorflow官方文档： Creating Custom Estimators</a></li></ol><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/38470806" target="_blank" rel="noopener">基于Tensorflow高阶API构建大规模分布式深度学习模型系列: 开篇</a></li><li><a href="https://zhuanlan.zhihu.com/p/38421397" target="_blank" rel="noopener">基于Tensorflow高阶API构建大规模分布式深度学习模型系列：基于Dataset API处理Input pipeline</a></li><li><a href="https://zhuanlan.zhihu.com/p/41473323" target="_blank" rel="noopener">基于Tensorflow高阶API构建大规模分布式深度学习模型系列: 自定义Estimator（以文本分类CNN模型为例）</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Tensorflow在1.4版本中引入了&lt;code&gt;tf.estimator.train_and_evaluate&lt;/code&gt;函数，用来替换老版中Experiment类提供的功能。&lt;code&gt;tf.estimator.train_and_evaluate&lt;/code&gt;简化了训练、评估和导出Estimator模型的过程，抽象了模型分布式训练和评估的细节，使得同样的代码在本地与分布式集群上的行为一致。&lt;/p&gt;
&lt;p&gt;这意味着使用&lt;code&gt;train_and_evaluate&lt;/code&gt; API，我们可以在本地和分布式集群上、不同的设备和硬件上跑同样的代码，而不需要修改代码已适应不同的部署环境。而且训练之后的模型可以很方便地导出以便在打分服务（tensorflow serving）中使用。&lt;/p&gt;
&lt;p&gt;本文简要介绍如何自定义Estimator模型并通过使用&lt;code&gt;tf.estimator.train_and_evaluate&lt;/code&gt;完成训练和评估。&lt;/p&gt;
&lt;p&gt;主要步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;构建自己的Estimator模型&lt;/li&gt;
&lt;li&gt;定义在训练和测试过程中数据如何输入给模型&lt;/li&gt;
&lt;li&gt;定义传递给&lt;code&gt;tf.estimator.train_and_evaluate&lt;/code&gt;函数的训练、评估和导出的详述参数(TrainSpec and EvalSpec)&lt;/li&gt;
&lt;li&gt;使用&lt;code&gt;tf.estimator.train_and_evaluate&lt;/code&gt;训练并评估模型&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="tensorflow" scheme="http://xudongyang.coding.me/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>基于Tensorflow高阶API构建大规模分布式深度学习模型系列:基于Dataset API处理Input pipeline</title>
    <link href="http://xudongyang.coding.me/tensorflow-dataset/"/>
    <id>http://xudongyang.coding.me/tensorflow-dataset/</id>
    <published>2018-07-08T02:23:02.000Z</published>
    <updated>2019-04-02T02:43:36.870Z</updated>
    
    <content type="html"><![CDATA[<p>在TensorFlow 1.3版本之前，读取数据一般有两种方法：</p><ul><li>使用placeholder + feed_dict读内存中的数据</li><li>使用文件名队列（string_input_producer）与内存队列（reader）读硬盘中的数据</li></ul><p>Dataset API同时支持从内存和硬盘的数据读取，相比之前的两种方法在语法上更加简洁易懂。Dataset API可以更方便地与其他高阶API配合，快速搭建网络模型。此外，如果想要用到TensorFlow新出的Eager模式，就必须要使用Dataset API来读取数据。</p><p><strong>Dataset可以看作是相同类型“元素”的有序列表</strong>。在实际使用时，单个“元素”可以是向量，也可以是字符串、图片，甚至是tuple或者dict。</p><a id="more"></a><h3 id="从内存中读取数据"><a href="#从内存中读取数据" class="headerlink" title="从内存中读取数据"></a>从内存中读取数据</h3><p>用tf.data.Dataset.from_tensor_slices创建了一个最简单的Dataset：<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="title">dataset</span> = tf.<span class="class"><span class="keyword">data</span>.<span class="type">Dataset</span>.from_tensor_slices(<span class="title">np</span>.<span class="title">array</span>([1.0, 2.0, 3.0, 4.0, 5.0]))</span></span><br></pre></td></tr></table></figure></p><p>如何将这个dataset中的元素取出呢？方法是从Dataset中实例化一个Iterator，然后对Iterator进行迭代。<br><figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">iterator</span> = dataset.make_one_shot_iterator()</span><br><span class="line">one_element = <span class="keyword">iterator</span>.get_next()</span><br><span class="line"><span class="keyword">with</span> tf.<span class="type">Session</span>() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        print(sess.run(one_element))</span><br></pre></td></tr></table></figure></p><p>由于Tensorflow采用了符号式编程（symbolic style programs）模式，而非常见的命令式编程（imperative style programs）模式，因此必须创建一个Session对象才能运行程序。上述代码中，one_element只是一个Tensor，并不是一个实际的值。调用sess.run(one_element)后，才能真正地取出一个值。如果一个dataset中元素被读取完了，再尝试sess.run(one_element)的话，就会抛出tf.errors.OutOfRangeError异常，这个行为与使用队列方式读取数据的行为是一致的。</p><p>其实，<strong>tf.data.Dataset.from_tensor_slices的功能不止如此，它的真正作用是切分传入Tensor的第一个维度，生成相应的dataset</strong>。例如：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset = tf<span class="selector-class">.data</span><span class="selector-class">.Dataset</span><span class="selector-class">.from_tensor_slices</span>(np<span class="selector-class">.random</span><span class="selector-class">.uniform</span>(size=(<span class="number">5</span>, <span class="number">2</span>)))</span><br></pre></td></tr></table></figure></p><p>传入的数值是一个矩阵，它的形状为(5, 2)，tf.data.Dataset.from_tensor_slices就会切分它形状上的第一个维度，最后生成的dataset中一个含有5个元素，每个元素的形状是(2, )，即每个元素是矩阵的一行。</p><p>下面我们来看看如何从Dict中构建dataset:<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataset = tf<span class="selector-class">.data</span><span class="selector-class">.Dataset</span><span class="selector-class">.from_tensor_slices</span>(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">"a"</span>: np.array([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>]),</span><br><span class="line">        <span class="string">"b"</span>: np<span class="selector-class">.random</span><span class="selector-class">.uniform</span>(size=(<span class="number">5</span>, <span class="number">2</span>))</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p>这时函数会分别切分”a”中的数值以及”b”中的数值，最终dataset中的一个元素就是类似于{“a”: 1.0, “b”: [0.9, 0.1]}的形式。</p><h3 id="从文件中读取数据"><a href="#从文件中读取数据" class="headerlink" title="从文件中读取数据"></a>从文件中读取数据</h3><p>在实际应用中，模型的训练和评估数据总是以文件的形式存在文件系统中，目前Dataset API提供了三种从文件读取数据并创建Dataset的方式，分别用来读取不同存储格式的文件。<br><img src="https://pic2.zhimg.com/80/v2-f9f42cc5c00573f7baaa815795f1ce45_hd.jpg" alt></p><ul><li>tf.data.TextLineDataset()：这个函数的输入是一个文件的列表，输出是一个dataset。dataset中的每一个元素就对应了文件中的一行。可以使用这个函数来读入CSV文件。</li><li>tf.data.FixedLengthRecordDataset()：这个函数的输入是一个文件的列表和一个record_bytes，之后dataset的每一个元素就是文件中固定字节数record_bytes的内容。通常用来读取以二进制形式保存的文件，如CIFAR10数据集就是这种形式。</li><li>tf.data.TFRecordDataset()：顾名思义，这个函数是用来读TFRecord文件的，dataset中的每一个元素就是一个TFExample。</li></ul><p>需要说明的是，这三种读取文件数据创建dataset的方法，不仅能读取本地文件系统中的文件，还能读取分布式文件系统（如HDFS）中的文件，这为模型的分布式训练创造了良好的条件。</p><h3 id="Dataset的常用Transformation操作"><a href="#Dataset的常用Transformation操作" class="headerlink" title="Dataset的常用Transformation操作"></a>Dataset的常用Transformation操作</h3><p>一个Dataset通过数据变换操作可以生成一个新的Dataset。下面介绍数据格式变换、过滤、数据打乱、生产batch和epoch等常用Transformation操作。</p><h4 id="（1）map"><a href="#（1）map" class="headerlink" title="（1）map"></a>（1）map</h4><p>map接收一个函数，Dataset中的每个元素都会被当作这个函数的输入，并将函数返回值作为新的Dataset，如我们可以对dataset中每个元素的值取平方：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset = tf.data.Dataset.from_tensor_slices(np.array([1.0, 2.0, 3.0, 4.0, 5.0]))</span><br><span class="line">dataset = dataset.map(lambda x: x * x) <span class="comment"># 1.0, 4.0, 9.0, 16.0, 25.0</span></span><br></pre></td></tr></table></figure></p><h4 id="（2）filter"><a href="#（2）filter" class="headerlink" title="（2）filter"></a>（2）filter</h4><p>filter操作可以过滤掉dataset不满足条件的元素，它接受一个布尔函数作为参数，dataset中的每个元素都作为该布尔函数的参数，布尔函数返回True的元素保留下来，布尔函数返回False的元素则被过滤掉。<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">dataset</span> = dataset.filter(filter_func)</span><br></pre></td></tr></table></figure></p><h4 id="（3）shuffle"><a href="#（3）shuffle" class="headerlink" title="（3）shuffle"></a>（3）shuffle</h4><p>shuffle功能为打乱dataset中的元素，它有一个参数buffer_size，表示打乱时使用的buffer的大小：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">dataset</span> = dataset.shuffle(buffer_size=<span class="number">10000</span>)</span><br></pre></td></tr></table></figure></p><h4 id="（4）repeat"><a href="#（4）repeat" class="headerlink" title="（4）repeat"></a>（4）repeat</h4><p>repeat的功能就是将整个序列重复多次，主要用来处理机器学习中的epoch，假设原先的数据是一个epoch，使用repeat(5)就可以将之变成5个epoch：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">dataset</span> = dataset.repeat(<span class="number">5</span>)</span><br></pre></td></tr></table></figure></p><p>如果直接调用repeat()的话，生成的序列就会无限重复下去，没有结束，因此也不会抛出tf.errors.OutOfRangeError异常。</p><h4 id="（5）batch"><a href="#（5）batch" class="headerlink" title="（5）batch"></a>（5）batch</h4><p>batch就是将多个元素组合成batch，如下面的程序将dataset中的每个元素组成了大小为32的batch：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">dataset</span> = dataset.batch(<span class="number">32</span>)</span><br></pre></td></tr></table></figure></p><p>需要注意的是，必须要保证dataset中每个元素拥有相同的shape才能调用batch方法，否则会抛出异常。在调用map方法转换元素格式的时候尤其要注意这一点。</p><h3 id="Dataset元素变换案例"><a href="#Dataset元素变换案例" class="headerlink" title="Dataset元素变换案例"></a>Dataset元素变换案例</h3><h4 id="1-解析CSV文件"><a href="#1-解析CSV文件" class="headerlink" title="1. 解析CSV文件"></a>1. 解析CSV文件</h4><p>假设我们有一个Tab分隔4个字段的文件，则可用如下的代码解析并生成dataset。<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">_CSV_COLUMNS = [<span class="string">'field1'</span>, <span class="string">'field2'</span>, <span class="string">'field3'</span>, <span class="string">'field4'</span>]</span><br><span class="line">_CSV_COLUMN_DEFAULTS=[[<span class="string">''</span>], [<span class="string">''</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>]]</span><br><span class="line"></span><br><span class="line">def input_fn(data_file, shuffle, batch_size):</span><br><span class="line">  def parse_csv(value):</span><br><span class="line">    columns = <span class="keyword">tf</span>.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS, field_delim=<span class="string">'\t'</span>)</span><br><span class="line">    features = dict(zip(_CSV_COLUMNS, columns))</span><br><span class="line">    labels = features.<span class="keyword">pop</span>(<span class="string">'ctr_flag'</span>)</span><br><span class="line">    <span class="keyword">return</span> features, <span class="keyword">tf</span>.equal(labels, <span class="string">'1.0'</span>)</span><br><span class="line"></span><br><span class="line">  # Extract lines from <span class="built_in">input</span> <span class="keyword">files</span> using the Dataset API.</span><br><span class="line">  dataset = <span class="keyword">tf</span>.data.TextLineDataset(data_file)</span><br><span class="line">  <span class="keyword">if</span> shuffle: dataset = dataset.shuffle(buffer_size=<span class="number">100000</span>)</span><br><span class="line">  dataset = dataset.<span class="keyword">map</span>(parse_csv, num_parallel_calls=<span class="number">100</span>)</span><br><span class="line">  # We <span class="keyword">call</span> <span class="built_in">repeat</span> after shuffling, rather than before, <span class="keyword">to</span> prevent separate</span><br><span class="line">  # epochs from blending together.</span><br><span class="line">  dataset = dataset.<span class="built_in">repeat</span>()</span><br><span class="line">  dataset = dataset.batch(batch_size)</span><br><span class="line">  <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure></p><p>上述代码主要利用tf.decode_csv函数来把CSV文件记录转换为Tensors列表，每一列对应一个Tensor。</p><h4 id="2-解析特殊格式的文本文件"><a href="#2-解析特殊格式的文本文件" class="headerlink" title="2. 解析特殊格式的文本文件"></a>2. 解析特殊格式的文本文件</h4><p>有时候我们的训练数据可能有特殊的格式，比如CVS文件其中某些字段是JSON格式的字符串，我们要把JSON字符串的内容也解析出来，这个时候tf.decode_csv函数就不够用了。</p><p>是时候请万能函数tf.py_func上场了，tf.py_func函数能够把一个任意的python函数封装成tensorflow的op，提供了极大的灵活性，其定义如下：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.py_func(</span><br><span class="line">    func,</span><br><span class="line">    inp,</span><br><span class="line">    Tout,</span><br><span class="line">    <span class="attribute">stateful</span>=<span class="literal">True</span>,</span><br><span class="line">    <span class="attribute">name</span>=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p>tf.py_func的核心是一个func函数(由用户自己定义)，该函数被封装成graph中的一个节点（op)。第二个参数inp是一个由Tensor组成的list，在执行时，inp的各个Tensor的值被取出来传给func作为参数。func的返回值会被tf.py_func转换为Tensors，这些Tensors的类型由Tout指定。当func只有一个返回值时，Tout是一个单独的tensorflow数据类型；当func函数有多个返回值时，Tout是一个tensorflow数据类型组成的元组或列表。参数stateful表示func函数是否有状态（产生副作用）。</p><p>在使用过程中，有几个需要注意的地方：</p><ul><li>func函数的返回值类型一定要和Tout指定的tensor类型一致。</li><li>tf.py_func中的func是脱离Graph的，在func中不能定义可训练的参数参与网络训练(反传)。</li><li>tf.py_func操作只能在CPU上运行；如果使用分布式TensorFlow，tf.py_func操作必须放在与客户端相同进程的CPU设备上。</li><li>tf.py_func操作返回的tensors是没有定义形状（shape）的，必须调用set_shape方法为各个返回值设置shape，才能参与后续的计算。</li></ul><p>先来看一个简单的示例，func函数接受单个参数并产生单个返回值的情况。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter_func</span><span class="params">(line)</span>:</span></span><br><span class="line">  fields = line.decode().split(<span class="string">"\t"</span>)</span><br><span class="line">  <span class="keyword">if</span> len(fields) &lt; <span class="number">8</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">  <span class="keyword">for</span> field <span class="keyword">in</span> fields:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> field:</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">dataset = dataset.filter(<span class="keyword">lambda</span> x: tf.py_func(filter_func, [x], tf.bool, <span class="literal">False</span>))</span><br></pre></td></tr></table></figure></p><p>再来看一个稍微复杂一点的例子，该例子解析一个带有json格式字段的CSV文件，json字段被平铺开来和其他字段并列作为返回值。<br><figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">def parse_line(line):</span><br><span class="line">  _COLUMNS = [<span class="string">"sellerId"</span>, <span class="string">"brandId"</span>, <span class="string">"cateId"</span>]</span><br><span class="line">  _INT_COLUMNS = [<span class="string">"click"</span>, <span class="string">"productId"</span>, <span class="string">"matchType"</span>, <span class="string">"position"</span>, <span class="string">"hour"</span>]</span><br><span class="line">  _FLOAT_COLUMNS = [<span class="string">"matchScore"</span>, <span class="string">"popScore"</span>, <span class="string">"brandPrefer"</span>, <span class="string">"catePrefer"</span>]</span><br><span class="line">  _STRING_COLUMNS = [<span class="string">"phoneResolution"</span>, <span class="string">"phoneBrand"</span>, <span class="string">"phoneOs"</span>]</span><br><span class="line">  _SEQ_COLUMNS = [<span class="string">"behaviorC1ids"</span>, <span class="string">"behaviorBids"</span>, <span class="string">"behaviorCids"</span>, <span class="string">"behaviorPids"</span>]</span><br><span class="line"></span><br><span class="line">  def get_content(record):</span><br><span class="line">    <span class="keyword">import</span> datetime</span><br><span class="line">    fields = record.decode().split(<span class="string">"\t"</span>)</span><br><span class="line">    <span class="keyword">if</span> len(fields) &lt; <span class="number">8</span>:</span><br><span class="line">      <span class="keyword">raise</span> <span class="type">ValueError</span>(<span class="string">"invalid record %s"</span> % record)</span><br><span class="line">    <span class="keyword">for</span> field <span class="keyword">in</span> fields:</span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> field:</span><br><span class="line">        <span class="keyword">raise</span> <span class="type">ValueError</span>(<span class="string">"invalid record %s"</span> % record)</span><br><span class="line">    fea = json.loads(fields[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> fea[<span class="string">"time"</span>]:</span><br><span class="line">      dt = datetime.datetime.fromtimestamp(fea[<span class="string">"time"</span>])</span><br><span class="line">      fea[<span class="string">"hour"</span>] = dt.hour</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      fea[<span class="string">"hour"</span>] = <span class="number">0</span></span><br><span class="line">    seq_len = <span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> _SEQ_COLUMNS:</span><br><span class="line">      sequence = fea.setdefault(x, [])</span><br><span class="line">      n = len(sequence)</span><br><span class="line">      <span class="keyword">if</span> n &lt; seq_len:</span><br><span class="line">        sequence.extend([-<span class="number">1</span>] * (seq_len - n))</span><br><span class="line">      <span class="keyword">elif</span> n &gt; seq_len:</span><br><span class="line">        fea[x] = sequence[:seq_len]</span><br><span class="line">      seq_len = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">    elems = [np.<span class="built_in">int64</span>(fields[<span class="number">2</span>]), np.<span class="built_in">int64</span>(fields[<span class="number">3</span>]), np.<span class="built_in">int64</span>(fields[<span class="number">4</span>]), np.<span class="built_in">int64</span>(fields[<span class="number">6</span>]), fields[<span class="number">7</span>]]</span><br><span class="line">    elems += [np.<span class="built_in">int64</span>(fea.get(x, <span class="number">0</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> _INT_COLUMNS]</span><br><span class="line">    elems += [np.<span class="built_in">float32</span>(fea.get(x, <span class="number">0</span>.<span class="number">0</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> _FLOAT_COLUMNS]</span><br><span class="line">    elems += [fea.get(x, <span class="string">""</span>) <span class="keyword">for</span> x <span class="keyword">in</span> _STRING_COLUMNS]</span><br><span class="line">    elems += [np.<span class="built_in">int64</span>(fea[x]) <span class="keyword">for</span> x <span class="keyword">in</span> _SEQ_COLUMNS]</span><br><span class="line">    <span class="keyword">return</span> elems</span><br><span class="line"></span><br><span class="line">  out_type = [tf.<span class="built_in">int64</span>] * <span class="number">4</span> + [tf.<span class="built_in">string</span>] + [tf.<span class="built_in">int64</span>] * len(_INT_COLUMNS) + [tf.<span class="built_in">float32</span>] * len(_FLOAT_COLUMNS) + [</span><br><span class="line">    tf.<span class="built_in">string</span>] * len(_STRING_COLUMNS) + [tf.<span class="built_in">int64</span>] * len(_SEQ_COLUMNS)</span><br><span class="line">  <span class="literal">result</span> = tf.py_func(get_content, [line], out_type)</span><br><span class="line">  n = len(<span class="literal">result</span>) - len(_SEQ_COLUMNS)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    <span class="literal">result</span>[i].set_shape([])</span><br><span class="line">  <span class="literal">result</span>[n].set_shape([<span class="number">10</span>])</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>, len(<span class="literal">result</span>)):</span><br><span class="line">    <span class="literal">result</span>[i].set_shape([<span class="number">20</span>])</span><br><span class="line">  columns = _COLUMNS + _INT_COLUMNS + _FLOAT_COLUMNS + _STRING_COLUMNS + _SEQ_COLUMNS</span><br><span class="line">  features = dict(zip(columns, <span class="literal">result</span>))</span><br><span class="line">  labels = features.pop('click')</span><br><span class="line">  <span class="keyword">return</span> features, labels</span><br><span class="line"></span><br><span class="line">def my_input_fn(filenames, batch_size, shuffle_buffer_size):</span><br><span class="line">  dataset = tf.data.<span class="type">TextLineDataset</span>(filenames)</span><br><span class="line">  dataset = dataset.filter(lambda x: tf.py_func(filter_func, [x], tf.<span class="built_in">bool</span>, <span class="type">False</span>))</span><br><span class="line">  dataset = dataset.map(parse_line, num_parallel_calls=<span class="number">100</span>)</span><br><span class="line">  <span class="comment"># Shuffle, repeat, and batch the examples.</span></span><br><span class="line">  <span class="keyword">if</span> shuffle_buffer_size &gt; <span class="number">0</span>:</span><br><span class="line">    dataset = dataset.shuffle(shuffle_buffer_size)</span><br><span class="line">  dataset = dataset.repeat().batch(batch_size)</span><br><span class="line">  <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure></p><h4 id="3-解析TFRECORD文件"><a href="#3-解析TFRECORD文件" class="headerlink" title="3. 解析TFRECORD文件"></a>3. 解析TFRECORD文件</h4><p>Tfrecord是tensorflow官方推荐的训练数据存储格式，它更容易与网络应用架构相匹配。</p><p>Tfrecord本质上是二进制的Protobuf数据，因而其读取、传输的速度更快。Tfrecord文件的每一条记录都是一个<code>tf.train.Example</code>的实例。<code>tf.train.Example</code>的proto格式的定义如下：<br><figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Example</span> </span>&#123;</span><br><span class="line">  Features features = <span class="number">1</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Features</span> </span>&#123;</span><br><span class="line">  map&lt;<span class="built_in">string</span>, Feature&gt; feature = <span class="number">1</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Feature</span> </span>&#123;</span><br><span class="line">  <span class="keyword">oneof</span> kind &#123;</span><br><span class="line">    BytesList bytes_list = <span class="number">1</span>;</span><br><span class="line">    FloatList float_list = <span class="number">2</span>;</span><br><span class="line">    Int64List int64_list = <span class="number">3</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>使用tfrecord文件格式的另一个好处是数据结构统一，屏蔽了底层的数据结构。在类似于图像分类的任务中，原始数据是各个图片以单独的小文件的形式存在，label又以文件夹的形式存在，处理这样的数据比较麻烦，比如随机打乱，分batch等操作；而所有原始数据转换为一个或几个单独的tfrecord文件后处理起来就会比较方便。</p><p>来看看tensorflow读取tfrecord文件并转化为训练features和labels的代码：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def parse_exmp(serial_exmp):</span><br><span class="line">  features = &#123;</span><br><span class="line">    <span class="string">"click"</span>: <span class="keyword">tf</span>.FixedLenFeature([], <span class="keyword">tf</span>.int64),</span><br><span class="line">    <span class="string">"behaviorBids"</span>: <span class="keyword">tf</span>.FixedLenFeature([<span class="number">20</span>], <span class="keyword">tf</span>.int64),</span><br><span class="line">    <span class="string">"behaviorCids"</span>: <span class="keyword">tf</span>.FixedLenFeature([<span class="number">20</span>], <span class="keyword">tf</span>.int64),</span><br><span class="line">    <span class="string">"behaviorC1ids"</span>: <span class="keyword">tf</span>.FixedLenFeature([<span class="number">10</span>], <span class="keyword">tf</span>.int64),</span><br><span class="line">    <span class="string">"behaviorSids"</span>: <span class="keyword">tf</span>.FixedLenFeature([<span class="number">20</span>], <span class="keyword">tf</span>.int64),</span><br><span class="line">    <span class="string">"behaviorPids"</span>: <span class="keyword">tf</span>.FixedLenFeature([<span class="number">20</span>], <span class="keyword">tf</span>.int64),</span><br><span class="line">    <span class="string">"productId"</span>: <span class="keyword">tf</span>.FixedLenFeature([], <span class="keyword">tf</span>.int64),</span><br><span class="line">    <span class="string">"sellerId"</span>: <span class="keyword">tf</span>.FixedLenFeature([], <span class="keyword">tf</span>.int64),</span><br><span class="line">    <span class="string">"brandId"</span>: <span class="keyword">tf</span>.FixedLenFeature([], <span class="keyword">tf</span>.int64),</span><br><span class="line">    <span class="string">"cate1Id"</span>: <span class="keyword">tf</span>.FixedLenFeature([], <span class="keyword">tf</span>.int64),</span><br><span class="line">    <span class="string">"cateId"</span>: <span class="keyword">tf</span>.FixedLenFeature([], <span class="keyword">tf</span>.int64),</span><br><span class="line">    <span class="string">"tab"</span>: <span class="keyword">tf</span>.FixedLenFeature([], <span class="keyword">tf</span>.<span class="built_in">string</span>),</span><br><span class="line">    <span class="string">"matchType"</span>: <span class="keyword">tf</span>.FixedLenFeature([], <span class="keyword">tf</span>.int64)</span><br><span class="line">  &#125;</span><br><span class="line">  feats = <span class="keyword">tf</span>.parse_single_example(serial_exmp, features=features)</span><br><span class="line">  labels = feats.<span class="keyword">pop</span>(<span class="string">'click'</span>)</span><br><span class="line">  <span class="keyword">return</span> feats, labels</span><br><span class="line"></span><br><span class="line">def train_input_fn(filenames, batch_size, shuffle_buffer_size):</span><br><span class="line">  dataset = <span class="keyword">tf</span>.data.TFRecordDataset(filenames)</span><br><span class="line">  dataset = dataset.<span class="keyword">map</span>(parse_exmp, num_parallel_calls=<span class="number">100</span>)</span><br><span class="line">  # Shuffle, <span class="built_in">repeat</span>, <span class="built_in">and</span> batch the examples.</span><br><span class="line">  <span class="keyword">if</span> shuffle_buffer_size &gt; <span class="number">0</span>:</span><br><span class="line">    dataset = dataset.shuffle(shuffle_buffer_size)</span><br><span class="line">  dataset = dataset.<span class="built_in">repeat</span>().batch(batch_size)</span><br><span class="line">  <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure></p><p>这里我们再说说如何把原始数据转换为tfrecord文件格式，请参考下面的代码片段：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 建立tfrecorder writer</span><br><span class="line">writer = <span class="keyword">tf</span>.python_io.TFRecordWriter(<span class="string">'csv_train.tfrecords'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i in xrange(train_values.shape[<span class="number">0</span>]):</span><br><span class="line">    image_raw = train_values[i].tostring()</span><br><span class="line"></span><br><span class="line">    # build example protobuf</span><br><span class="line">    example = <span class="keyword">tf</span>.train.Example(</span><br><span class="line">      features=<span class="keyword">tf</span>.train.Features(feature=&#123;</span><br><span class="line">        <span class="string">'image_raw'</span>:  <span class="keyword">tf</span>.train.Feature(bytes_list=<span class="keyword">tf</span>.train.BytesList(value=[image_raw])),</span><br><span class="line">        <span class="string">'label'</span>: <span class="keyword">tf</span>.train.Feature(int64_list=<span class="keyword">tf</span>.train.Int64List(value=[train_labels[i]]))</span><br><span class="line">    &#125;))</span><br><span class="line">    writer.<span class="keyword">write</span>(record=example.SerializeToString())</span><br><span class="line"></span><br><span class="line">writer.<span class="keyword">close</span>()</span><br></pre></td></tr></table></figure></p><p>然而，大规模的训练数据用这种方式转换格式会比较低效，更好的实践是用hadoop或者spark这种分布式计算平台，并行实现数据转换任务。这里给出一个用Hadoop MapReduce编程模式转换为tfrecord文件格式的开源实现：<a href="https://github.com/tensorflow/ecosystem/tree/master/hadoop" target="_blank" rel="noopener">Hadoop MapReduce InputFormat/OutputFormat for TFRecords</a>。由于该实现指定了protobuf的版本，因而可能会跟自己真正使用的hadoop平台自己的protobuf版本不一致，hadoop在默认情况下总是优先使用HADOOP_HOME/lib下的jar包，从而导致运行时错误，遇到这种情况时，只需要设置<code>mapreduce.task.classpath.user.precedence=true</code>参数，优先使用自己指定版本的jar包即可。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/30751039" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/30751039</a><br><a href="https://www.skcript.com/svr/why-every-tensorflow-developer-should-know-about-tfrecord/" target="_blank" rel="noopener">https://www.skcript.com/svr/why-every-tensorflow-developer-should-know-about-tfrecord/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在TensorFlow 1.3版本之前，读取数据一般有两种方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用placeholder + feed_dict读内存中的数据&lt;/li&gt;
&lt;li&gt;使用文件名队列（string_input_producer）与内存队列（reader）读硬盘中的数据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dataset API同时支持从内存和硬盘的数据读取，相比之前的两种方法在语法上更加简洁易懂。Dataset API可以更方便地与其他高阶API配合，快速搭建网络模型。此外，如果想要用到TensorFlow新出的Eager模式，就必须要使用Dataset API来读取数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dataset可以看作是相同类型“元素”的有序列表&lt;/strong&gt;。在实际使用时，单个“元素”可以是向量，也可以是字符串、图片，甚至是tuple或者dict。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="tensorflow" scheme="http://xudongyang.coding.me/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>基于Tensorflow高阶API构建大规模分布式深度学习模型系列</title>
    <link href="http://xudongyang.coding.me/tensorflow-high-level-api/"/>
    <id>http://xudongyang.coding.me/tensorflow-high-level-api/</id>
    <published>2018-06-25T07:00:28.000Z</published>
    <updated>2019-04-02T02:43:36.871Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Tensorflow高阶API简介"><a href="#Tensorflow高阶API简介" class="headerlink" title="Tensorflow高阶API简介"></a>Tensorflow高阶API简介</h2><p>在tensorflow高阶API（Estimator、Dataset、Layer、FeatureColumn等）问世之前，用tensorflow开发、训练、评估、部署深度学习模型，并没有统一的规范和高效的标准流程。Tensorflow的实践者们基于低阶API开发的代码在可移植性方面可能会遇到各种困难。例如，单机可以运行的模型希望改成能够分布式环境下运行需要对代码做额外的改动，如果在一个异构的环境中训练模型，则还需要额外花精力处理哪些部分跑在CPU上，哪些部分跑在GPU上。当不同的机器有不同数量的GPU数量时，问题更加复杂。</p><p>为了能够快速支持新的网络架构的实验测试，深度学习框架都很重视网络架构搭建的灵活性需求，因此能让用户随心所欲地自定义代码实现是很重要的一块功能。</p><p>模型构建的灵活性与简洁性需求看似是矛盾的。从开发者的视角，简洁性意味着当模型架构确定时实现不应该需要太多额外的技能要求，不必对深度学习框架有很深刻的洞察，就能够实验不同的模型特性。在内置简洁性属性的框架下开发者能够较轻松地开发出高质量的鲁棒性较好的模型软件，不会一不小心就踩到坑里。另一方面，灵活性意味着开发者能够实现任意的想要的模型结构，这需要框架能够提供一些相对低价的API。类似于Caffe这样的深度学习框架提供了DSL（domain specific language）来描述模型的结构，虽然搭建已知的成熟的模型架构比较方便，但却不能轻松搭建任意想要的模型结构。这就好比用积木搭建房子，如果现在需要一个特殊的以前没有出现过的积木块以便搭建一个特殊的房子，那就无计可施了。</p><p>Tensorflow高阶API正是为了同时满足模型构建的灵活性与简洁性需求应运而生的，它能够让开发者快速搭建出高质量的模型，又能够使用结合低阶API实现不受限制的模型结构。<br><a id="more"></a><br>下面就来看看tensorflow中有哪些常用的高阶API吧。<br><img src="https://3.bp.blogspot.com/-l2UT45WGdyw/Wbe7au1nfwI/AAAAAAAAD1I/GeQcQUUWezIiaFFRCiMILlX2EYdG49C0wCLcBGAs/s1600/image6.png" alt></p><h3 id="Estimator（估算器）"><a href="#Estimator（估算器）" class="headerlink" title="Estimator（估算器）"></a>Estimator（估算器）</h3><p>Estimator类是机器学习模型的抽象，其设计灵感来自于典典大名的Python机器学习库Scikit-learn。Estimator允许开发者自定义任意的模型结构、损失函数、优化方法以及如何对这个模型进行训练、评估和导出等内容，同时屏蔽了与底层硬件设备、分布式网络数据传输等相关的细节。</p><p><img src="/tensorflow-high-level-api/estimator.png" alt></p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.estimator.Estimator(</span><br><span class="line">    <span class="attr">model_fn=model_fn,</span>  <span class="comment"># First-class function</span></span><br><span class="line">    <span class="attr">params=params,</span>  <span class="comment"># HParams</span></span><br><span class="line">    <span class="attr">config=run_config</span>  <span class="comment"># RunConfig</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>要创建Estimator，需要传入一个模型函数、一组参数和一些配置。</p><ul><li>传入的参数应该是模型超参数的一个集合，可以是一个dictionary。</li><li>传入的配置用于指定模型如何运行训练和评估，以及在哪里存储结果。这个配置是一个RunConfig对象，该对象会把模型运行环境相关的信息告诉Estimator。</li><li>模型函数是一个Python函数，它根据给定的输入构建模型。</li></ul><p>Estimator类有三个主要的方法：train/fit、evaluate、predict，分别表示模型的训练、评估和预测。三个方法都接受一个用户自定义的输入函数input_fn，执行input_fn获取输入数据。Estimator的这三个方法最终都会调用模型函数（model_fn）执行具体的操作，不同方法被调用时，传递给model_fn的mode参数也是不同的，如下一小节中描述的那样，mode参数是让用户在编写模型函数时知道当前定义的操作是用在模型生命周期的哪一个阶段。</p><p>Tensorflow本身还提供了很多内置的开箱即用的Estimator，内置的 Estimator 是 tf.estimator.Estimator 基类的子类，而自定义 Estimator 是 tf.estimator.Estimator 的实例，如下图所示。<br><img src="/tensorflow-high-level-api/estimator_types.png" alt></p><h3 id="模型函数"><a href="#模型函数" class="headerlink" title="模型函数"></a>模型函数</h3><p>模型函数是用户自定义的一个python函数，它定义了模型训练、评估和预测所需的计算图节点（op）。</p><p>模型函数接受输入特征和标签作为参数，同时用mode参数来告知用户模型是在训练、评估或是在执行推理。mode是tf.estimator.ModeKeys对象，它有三个可取的值：TRAIN、EVAL、PREDICT。模型函数的最后一个参数是超参数集合，它们与传递给Estimator的超参数集合相同。模型函数返回一个EstimatorSpec对象，该对象定义了一个完整的模型。EstimatorSpec对象用于对操作进行预测、损失、训练和评估，因此，它定义了一个用于训练、评估和推理的完整的模型图。</p><p>一个简单的模型函数示例如下：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def model_fn(features, target, <span class="keyword">mode</span>, params)</span><br><span class="line">  predictions = <span class="keyword">tf</span>.stack(<span class="keyword">tf</span>.fully_connected, [<span class="number">50</span>, <span class="number">50</span>, <span class="number">1</span>])</span><br><span class="line">  loss = <span class="keyword">tf</span>.losses.mean_squared_error(target, predictions)</span><br><span class="line">  train_op = <span class="keyword">tf</span>.train.create_train_op(</span><br><span class="line">    loss, <span class="keyword">tf</span>.train.get_global_step(),</span><br><span class="line">    params[’learning_rate’], params[’optimizer’])</span><br><span class="line">  <span class="keyword">return</span> EstimatorSpec(<span class="keyword">mode</span>=<span class="keyword">mode</span>,</span><br><span class="line">                       predictions=predictions,</span><br><span class="line">                       loss=loss,</span><br><span class="line">                       train_op=train_op)</span><br></pre></td></tr></table></figure><h3 id="Dataset（数据集）"><a href="#Dataset（数据集）" class="headerlink" title="Dataset（数据集）"></a>Dataset（数据集）</h3><p>在tensorflow中，构建模型输入流水线的最佳实践就是使用Dataset API。Dataset API底层使用C++实现，能够绕过python的一些性能限制，性能很好。</p><p>Dataset是对训练、评估、预测阶段所用的数据的抽象表示，其提供了数据读取、解析、打乱（shuffle）、过滤、分批（batch）等操作，是构建模型输入管道的利器，我将会在另外一篇文章《<a href="https://zhuanlan.zhihu.com/p/38421397" target="_blank" rel="noopener">基于Tensorflow高阶API构建大规模分布式深度学习模型系列：基于Dataset API处理Input pipeline</a>》中详细介绍。</p><h3 id="Feature-Columns（特征列）"><a href="#Feature-Columns（特征列）" class="headerlink" title="Feature Columns（特征列）"></a>Feature Columns（特征列）</h3><p>Feature Columns是特征工程的利器，其能够方便地把原始数据转换为模型的输入数据，并提供了一些常用的数据变换操作，如特征交叉、one-hot编码、embedding编码等。关于Feature Column，也将会在另外一篇文章中详细介绍。</p><h3 id="Layers"><a href="#Layers" class="headerlink" title="Layers"></a>Layers</h3><p>Layer是一组简单的可重复利用的代码，表示神经网络模型中的“层”这个概率。Tensorflow中的layer可以认为是一系列操作（op）的集合，与op一样也是输入tensor并输出tensor的（tensor-in-tensor-out)。Tensorflow中即内置了全连接这样的简单layer，也有像inception网络那样的复杂layer。使用layers来搭建网络模型会更加方便。</p><h3 id="Head"><a href="#Head" class="headerlink" title="Head"></a>Head</h3><p>Head API对网络最后一个隐藏层之后的部分进行了抽象，它的主要设计目标是简化模型函数（model_fn）的编写。Head知道如何计算损失（loss）、评估度量标准（metric)、预测结果（prediction）。为了支持不同的模型，Head接受logits和labels作为参数，并生成表示loss、metric和prediction的张量。有时为了避免计算完整的logit张量，Head也接受最后一个隐藏的激活值作为输入。</p><p>一个使用Head简化model_fn编写的例子如下：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def model_fn(features, target, <span class="keyword">mode</span>, params):</span><br><span class="line">  last_layer = <span class="keyword">tf</span>.stack(<span class="keyword">tf</span>.fully_connected, [<span class="number">50</span>, <span class="number">50</span>])</span><br><span class="line">  head = <span class="keyword">tf</span>.multi_class_head(n_classes=<span class="number">10</span>)</span><br><span class="line">  <span class="keyword">return</span> head.create_estimator_spec(</span><br><span class="line">    features, <span class="keyword">mode</span>, last_layer,</span><br><span class="line">    label=target,</span><br><span class="line">    train_op_fn=lambda los<span class="variable">s:</span> my_optimizer.minimize(loss, <span class="keyword">tf</span>.train.get_global_step())</span><br></pre></td></tr></table></figure></p><p>我们也可以用一个Heads列表来创建一个特殊类型的Head，来完成多目标学习的任务，如下面的例子那样。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def model_fn(features, target, mode, params):</span><br><span class="line">  last_layer = tf.stack(tf.fully_connected, [50, 50])</span><br><span class="line">  head1 = tf.multi_class_head(<span class="attribute">n_classes</span>=2,label_name=’y’, <span class="attribute">head_name</span>=’h1’)</span><br><span class="line">  head2 = tf.multi_class_head(<span class="attribute">n_classes</span>=10,label_name=’z’, <span class="attribute">head_name</span>=’h2’)</span><br><span class="line">  head = tf.multi_head([head1, head2])</span><br><span class="line">  return head.create_model_fn_ops(features,</span><br><span class="line">    features, mode, last_layer,</span><br><span class="line">    <span class="attribute">label</span>=target,</span><br><span class="line">    <span class="attribute">train_op_fn</span>=lambda loss: my_optimizer.minimize(loss, tf.train.get_global_step())</span><br></pre></td></tr></table></figure></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Tensorflow高阶API简化了模型代码的编写过程，大大降价了新手的入门门槛，使我们能够用一种标准化的方法开发出实验与生产环境部署的代码。使用Tensorflow高阶API能够使我们避免走很多弯路，提高深度学习的实践效率，我们应该尽可能使用高阶API来开发模型。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><a href="https://arxiv.org/abs/1708.02637" target="_blank" rel="noopener">TensorFlow Estimators: Managing Simplicity vs. Flexibility in High-Level Machine Learning Frameworks</a></li><li><a href="https://www.tensorflow.org/get_started/custom_estimators" target="_blank" rel="noopener">自定义estimators</a></li><li><a href="https://www.tensorflow.org/get_started/feature_columns" target="_blank" rel="noopener">Feature columns</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Tensorflow高阶API简介&quot;&gt;&lt;a href=&quot;#Tensorflow高阶API简介&quot; class=&quot;headerlink&quot; title=&quot;Tensorflow高阶API简介&quot;&gt;&lt;/a&gt;Tensorflow高阶API简介&lt;/h2&gt;&lt;p&gt;在tensorflow高阶API（Estimator、Dataset、Layer、FeatureColumn等）问世之前，用tensorflow开发、训练、评估、部署深度学习模型，并没有统一的规范和高效的标准流程。Tensorflow的实践者们基于低阶API开发的代码在可移植性方面可能会遇到各种困难。例如，单机可以运行的模型希望改成能够分布式环境下运行需要对代码做额外的改动，如果在一个异构的环境中训练模型，则还需要额外花精力处理哪些部分跑在CPU上，哪些部分跑在GPU上。当不同的机器有不同数量的GPU数量时，问题更加复杂。&lt;/p&gt;
&lt;p&gt;为了能够快速支持新的网络架构的实验测试，深度学习框架都很重视网络架构搭建的灵活性需求，因此能让用户随心所欲地自定义代码实现是很重要的一块功能。&lt;/p&gt;
&lt;p&gt;模型构建的灵活性与简洁性需求看似是矛盾的。从开发者的视角，简洁性意味着当模型架构确定时实现不应该需要太多额外的技能要求，不必对深度学习框架有很深刻的洞察，就能够实验不同的模型特性。在内置简洁性属性的框架下开发者能够较轻松地开发出高质量的鲁棒性较好的模型软件，不会一不小心就踩到坑里。另一方面，灵活性意味着开发者能够实现任意的想要的模型结构，这需要框架能够提供一些相对低价的API。类似于Caffe这样的深度学习框架提供了DSL（domain specific language）来描述模型的结构，虽然搭建已知的成熟的模型架构比较方便，但却不能轻松搭建任意想要的模型结构。这就好比用积木搭建房子，如果现在需要一个特殊的以前没有出现过的积木块以便搭建一个特殊的房子，那就无计可施了。&lt;/p&gt;
&lt;p&gt;Tensorflow高阶API正是为了同时满足模型构建的灵活性与简洁性需求应运而生的，它能够让开发者快速搭建出高质量的模型，又能够使用结合低阶API实现不受限制的模型结构。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="tensorflow" scheme="http://xudongyang.coding.me/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>完整空间多任务模型：CVR预估的有效方法</title>
    <link href="http://xudongyang.coding.me/esmm/"/>
    <id>http://xudongyang.coding.me/esmm/</id>
    <published>2018-05-16T05:25:58.000Z</published>
    <updated>2019-04-02T02:43:36.815Z</updated>
    
    <content type="html"><![CDATA[<p>在诸如信息检索、推荐系统、在线广告投放系统等工业级的应用中准确预估转化率（post-click conversion rate，CVR）是至关重要的。例如，在电商平台的推荐系统中，最大化场景商品交易总额（GMV）是平台的重要目标之一，而GMV可以拆解为流量×点击率×转化率×客单价，可见转化率是优化目标的重要因子；从用户体验的角度来说准确预估的转换率被用来平衡用户的点击偏好与购买偏好。</p><p>阿里妈妈算法团队最近发表了一篇关于CVR预估的论文《Entire Space Multi-Task Model: An Eﬀective Approach for Estimating Post-Click Conversion Rate》，提出了一种新颖的CVR预估模型，称之为“完整空间多任务模型”（Entire Space Multi-Task Model，ESMM），下文简称为ESMM模型。ESMM模型创新地利用用户行为序列数据，在完整的样本数据空间同时学习点击率和转化率（post-view clickthrough&amp;conversion rate，CTCVR），解决了传统CVR预估模型难以克服的样本选择偏差（sample selection bias）和训练数据过于稀疏（data sparsity ）的问题。<br><a id="more"></a><br>以电子商务平台为例，用户在观察到系统展现的推荐商品列表后，可能会点击自己感兴趣的商品，进而产生购买行为。换句话说，用户行为遵循一定的顺序决策模式：impression → click → conversion。CVR模型旨在预估用户在观察到曝光商品进而点击到商品详情页之后购买此商品的概率，即pCVR = p(conversion|click,impression)。</p><p>假设训练数据集为$S=\{(x_i,y_i \to z_i)\}|_{i=1}^N$，其中的样本$(x,y \to z)$是从域$X \times Y \times Z$中按照某种分布采样得到的，$X$是特征空间，$Y$和$Z$是标签空间，$N$ 为数据集中的样本总数量。在CVR预估任务中，$x$ 是高维稀疏多域的特征向量，$y$ 和 $z$ 的取值为0或1，分别表示是否点击和是否购买。$y \to z$揭示了用户行为的顺序性，即点击事情一般发生在购买事件之前。CVR模型的目标是预估条件概率pCVR ，与其相关的两个概率为点击率pCTR 和点击且转换率 pCTCVR ，它们之间的关系如下：<br><img src="/esmm/ctcvr.png" alt="图片"></p><p>传统的CVR预估任务通常采用类似于CTR预估的技术，比如最近很流行的深度学习模型。然而，有别于CTR预估任务，CVR预估任务面临一些特有的挑战：1) 样本选择偏差；2) 训练数据稀疏；3) 延迟反馈等。</p><p><img src="/esmm/sample-space.png" alt="图片"> 图1. 训练样本空间</p><p>延迟反馈的问题不在本文讨论的范围内，下面简单介绍一下样本选择偏差与训练数据稀疏的问题。如图1所示，最外面的大椭圆为整个样本空间$S$，其中有点击事件（$y=1$）的样本组成的集合为$S_c=\{(x_j, z_j)|y_j = 1\}|_{j=1}^M$，对应图中的阴影区域，传统的CVR模型就是用此集合中的样本来训练的，同时训练好的模型又需要在整个样本空间做预测推断。由于点击事件相对于展现事件来说要少很多，因此$S_c$只是样本空间$S$的一个很小的子集，从$S_c$上提取的特征相对于从$S$中提取的特征而言是有偏的，甚至是很不相同。从而，按这种方法构建的训练样本集相当于是从一个与真实分布不一致的分布中采样得到的，这一定程度上违背了机器学习算法之所以有效的前提：训练样本与测试样本必须独立地采样自同一个分布，即独立同分布的假设。总结一下，训练样本从整体样本空间的一个较小子集中提取，而训练得到的模型却需要对整个样本空间中的样本做推断预测的现象称之为样本选择偏差。样本选择偏差会伤害学到的模型的泛化性能。</p><p>推荐系统展现给用户的商品数量要远远大于被用户点击的商品数量，同时有点击行为的用户也仅仅只占所有用户的一小部分，因此有点击行为的样本空间图片: $S_c$相对于整个样本空间$S$ 来说是很小的，通常来讲，量级要少1~3个数量级。如表1所示，在淘宝公开的训练数据集上，$S_c$只占整个样本空间$S$的4%。这就是所谓的训练数据稀疏的问题，高度稀疏的训练数据使得模型的学习变得相当困难。<br><img src="/esmm/dataset.png" alt="图片"></p><p>阿里妈妈的算法同学提出的ESMM模型借鉴了多任务学习的思路，引入了两个辅助的学习任务，分别用来拟合pCTR和pCTCVR，从而同时消除了上文提到的两个挑战。ESMM模型能够充分利用用户行为的顺序性模式，其模型架构如图2所示。</p><p><img src="/esmm/esmm.png" alt="图片"> 图2. ESMM模型</p><p>整体来看，对于一个给定的展现，ESMM模型能够同时输出预估的pCTR、pCVR 和pCTCVR。它主要由两个子神经网络组成，左边的子网络用来拟合pCVR ，右边的子网络用来拟合pCTR。两个子网络的结构是完全相同的，这里把子网络命名为BASE模型。两个子网络的输出结果相乘之后即得到pCTCVR，并作为整个任务的输出。</p><p>需要强调的是，ESMM模型有两个主要的特点，使其区别于传统的CVR预估模型，分别阐述如下。</p><ol><li><p>在整个样本空间建模。由下面的等式可以看出，pCVR 可以在先估计出pCTR 和pCTCVR之后推导出来。从原理上来说，相当于分别单独训练两个模型拟合出pCTR 和pCTCVR，再通过pCTCVR 除以pCTR 得到最终的拟合目标pCVR 。<br><img src="/esmm/ctcvr-devision.png" alt="图片"><br> 但是，由于pCTR 通常很小，除以一个很小的浮点数容易引起数组不稳定问题（计算内存溢出）。所以ESMM模型采用了乘法的形式，而没有采用除法形式。<br> pCTR 和pCTCVR 是ESMM模型需要估计的两个主要因子，而且是在整个样本空间上建模得到的，pCVR 只是一个中间变量。由此可见，ESMM模型是在整个样本空间建模，而不像传统CVR预估模型那样只在点击样本空间建模。</p></li><li><p>共享特征表示。ESMM模型借鉴迁移学习的思路，在两个子网络的embedding层共享embedding向量（特征表示）词典。网络的embedding层把大规模稀疏的输入数据映射到低维的表示向量，该层的参数占了整个网络参数的绝大部分，需要大量的训练样本才能充分学习得到。由于CTR任务的训练样本量要大大超过CVR任务的训练样本量，ESMM模型中特征表示共享的机制能够使得CVR子任务也能够从只有展现没有点击的样本中学习，从而能够极大地有利于缓解训练数据稀疏性问题。</p></li></ol><p>需要补充说明的是，ESMM模型的损失函数由两部分组成，对应于pCTR 和pCTCVR 两个子任务，其形式如下：</p><script type="math/tex; mode=display">L(\theta_{cvr},\theta_{ctr}) =\sum_{i=1}^N l(y_i, f(x_i; \theta_{ctr}))+ \sum_{i=1}^N l(y_i\&z_i, f(x_i; \theta_{ctr}) \times f(x_i; \theta_{cvr}))</script><p>其中，$\theta_{ctr}$和$\theta_{cvr}$分别是CTR网络和CVR网络的参数，$l(\cdot)$是交叉熵损失函数。在CTR任务中，有点击行为的展现事件构成的样本标记为正样本，没有点击行为发生的展现事件标记为负样本；在CTCVR任务中，同时有点击和购买行为的展现事件标记为正样本，否则标记为负样本。</p><p>由于ESMM模型创新性地利用了用户的序列行为做完模型的训练样本，因此并没有公开的数据集可供测试，阿里的技术同学从淘宝的日志中采样了一部分数据，作为公开的测试集，下载地址为：<a href="https://tianchi.aliyun.com/datalab/dataSet.html?dataId=408" target="_blank" rel="noopener">https://tianchi.aliyun.com/datalab/dataSet.html?dataId=408</a> 。阿里妈妈的工程师们分别在公开的数据集和淘宝生产环境的数据集上做了测试，相对于其他几个主流的竞争模型，都取得了更好的性能。</p><p><img src="/esmm/comparison-public.png" alt></p><p>表2是在公开数据集上的不同算法AUC效果对比情况，其中BASE模型是ESMM模型中左边的子神经网络模型，由于其只在点击样本空间训练，会遭遇样本选择偏差和数据稀疏的问题，因为效果也是较差的。DIVISION模型是先分别训练出拟合CTR和CTCVR的模型，再拿CTCVR模型的预测结果除以CTR模型的预测结果得到对CVR模型的预估。ESMM-NS模型是ESMM模型的一个变种模型，其在ESMM模型的基础上去掉了特征表示共享的机制。AMAN、OVERSAMPLING、UNBIAS是三个竞争模型。</p><p><img src="/esmm/comparison-product.png" alt> 图3.在淘宝生产环境数据集上几种不同算法的性能测试对比</p><p>图3是ESMM模型在淘宝生产环境数据集上的测试效果对比。相对于BASE模型，ESMM模型在CVR任务中AUC指标提升了 2.18%，在CTCVR任务中AUC指标提升了2.32%。通常AUC指标提升0.1%就可认为是一个显著的改进。</p><p>综上所述，ESMM模型是一个新颖的CVR预估方法，其首创了利用用户行为序列数据在完整样本空间建模，避免了传统CVR模型经常遭遇的样本选择偏差和训练数据稀疏的问题，取得了显著的效果。另一方面，ESMM模型的贡献在于其提出的利用学习CTR和CTCVR的辅助任务，迂回地学习CVR的思路。ESMM模型中的BASE子网络可以替换为任意的学习模型，因此ESMM的框架可以非常容易地和其他学习模型集成，从而吸收其他学习模型的优势，进一步提升学习效果，想象空间巨大。</p><p>原文链接：<a href="https://arxiv.org/abs/1804.07931" target="_blank" rel="noopener">https://arxiv.org/abs/1804.07931</a></p><p>关于ESMM模型的实现，请参考另一篇文章：<a href="/esmm-1/" title="构建分布式Tensorflow模型系列之CVR预估案例ESMM模型">构建分布式Tensorflow模型系列之CVR预估案例ESMM模型</a></p><p>完整源代码：<a href="https://github.com/yangxudong/deeplearning/tree/master/esmm" target="_blank" rel="noopener">https://github.com/yangxudong/deeplearning/tree/master/esmm</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在诸如信息检索、推荐系统、在线广告投放系统等工业级的应用中准确预估转化率（post-click conversion rate，CVR）是至关重要的。例如，在电商平台的推荐系统中，最大化场景商品交易总额（GMV）是平台的重要目标之一，而GMV可以拆解为流量×点击率×转化率×客单价，可见转化率是优化目标的重要因子；从用户体验的角度来说准确预估的转换率被用来平衡用户的点击偏好与购买偏好。&lt;/p&gt;
&lt;p&gt;阿里妈妈算法团队最近发表了一篇关于CVR预估的论文《Entire Space Multi-Task Model: An Eﬀective Approach for Estimating Post-Click Conversion Rate》，提出了一种新颖的CVR预估模型，称之为“完整空间多任务模型”（Entire Space Multi-Task Model，ESMM），下文简称为ESMM模型。ESMM模型创新地利用用户行为序列数据，在完整的样本数据空间同时学习点击率和转化率（post-view clickthrough&amp;amp;conversion rate，CTCVR），解决了传统CVR预估模型难以克服的样本选择偏差（sample selection bias）和训练数据过于稀疏（data sparsity ）的问题。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="CTR预估" scheme="http://xudongyang.coding.me/tags/CTR%E9%A2%84%E4%BC%B0/"/>
    
      <category term="CVR预估" scheme="http://xudongyang.coding.me/tags/CVR%E9%A2%84%E4%BC%B0/"/>
    
  </entry>
  
  <entry>
    <title>Contextual Bandit算法在推荐系统中的实现及应用</title>
    <link href="http://xudongyang.coding.me/linucb/"/>
    <id>http://xudongyang.coding.me/linucb/</id>
    <published>2018-04-16T11:51:15.000Z</published>
    <updated>2019-04-02T02:43:36.848Z</updated>
    
    <content type="html"><![CDATA[<p>推荐系统选择商品展现给用户，并期待用户的正向反馈（点击、成交）。然而推荐系统并不能提前知道用户在观察到商品之后如何反馈，也就是不能提前获得本次推荐的收益，唯一能做的就是不停地尝试，并实时收集反馈以便更新自己试错的策略。目的是使得整个过程损失的收益最小。这一过程就类似与一个赌徒在赌场里玩老虎机赌博。赌徒要去摇老虎机，走进赌场一看，一排老虎机，外表一模一样，但是每个老虎机吐钱的概率可不一样，他不知道每个老虎机吐钱的概率分布是什么，那么每次该选择哪个老虎机可以做到最大化收益呢？这就是多臂赌博机问题（Multi-armed bandit problem, MAB）。</p><p>MAB问题的难点是Exploitation-Exploration(E&amp;E)两难的问题：对已知的吐钱概率比较高的老虎机，应该更多的去尝试(exploitation)，以便获得一定的累计收益；对未知的或尝试次数较少的老虎机，还要分配一定的尝试机会（exploration），以免错失收益更高的选择，但同时较多的探索也意味着较高的风险（机会成本）。</p><p>Bandit算法是一类用来实现Exploitation-Exploration机制的策略。根据是否考虑上下文特征，Bandit算法分为context-free bandit和contextual bandit两大类。<br><a id="more"></a></p><h2 id="1-UCB"><a href="#1-UCB" class="headerlink" title="1. UCB"></a>1. UCB</h2><p>Context-free Bandit算法有很多种，比如$\epsilon-greedy$、softmax、Thompson Sampling、UCB(Upper Confidence Bound)等。</p><p>在此，重点介绍一下UCB方法的基本思想。在统计学中，对于一个未知量的估计，总能找到一种量化其置信度的方法。最普遍的分布正态分布（或曰高斯分布）$N(μ,δ)$，其中的$μ$就是估计量的期望，而$δ$则表示其不确定性（$δ$越大则表示越不可信）。比如你掷一个标准的6面色子，它的平均值是3.5，而如果你只掷一次，比如说到2，那你对平均值的估计只能是2，但是这个置信度应该很低，我们可以知道，这个色子的预估平均值是2，而以95%的置信区间在[1.4,5.2]。</p><p>UCB（Upper Confidence Bound - 置信上限）就是以收益（bonus）均值的置信区间上限代表对该arm未来收益的预估值：</p><script type="math/tex; mode=display">\hat{\mu_i} + \sqrt{\frac{2ln(n)}{n_i}}</script><p>其中$\hat{\mu_i}$是对arm $i$期望收益的预估，$n$是总的选择次数，${n_i}$是对arm $i$的尝试次数，可以看到尝试越多，其预估值与置信上限的差值就越小，也就是越有置信度。</p><p>UCB在此时的决策是选择置信区间上界最大的arm。这个策略的好处是，能让没有机会尝试的arm得到更多尝试的机会，是骡子是马拉出来溜溜！</p><ul><li>对于未知或较少尝试的arm，尽管其均值可能很低，但是由于其不确定性会导致置信区间的上界较大，从而有较大的概率触发exploration</li><li>对于已经很熟悉的arm(尝试过较多次)，更多的是触发exploitation机制：如果其均值很高，会获得更多的利用机会；反之，则会减少对其尝试的机会</li></ul><h2 id="2-LinUCB"><a href="#2-LinUCB" class="headerlink" title="2. LinUCB"></a>2. LinUCB</h2><p>在推荐系统中，通常把待推荐的商品作为MAB问题的arm。UCB这样的context-free类算法，没有充分利用推荐场景的上下文信息，为所有用户的选择展现商品的策略都是相同的，忽略了用户作为一个个活生生的个性本身的兴趣点、偏好、购买力等因素都是不同的，因而，同一个商品在不同的用户、不同的情景下接受程度是不同的。故在实际的推荐系统中，context-free的MAB算法基本都不会被采用。</p><p>与context-free MAB算法对应的是Contextual Bandit算法，顾名思义，这类算法在实现E&amp;E时考虑了上下文信息，因而更加适合实际的个性化推荐场景。</p><p>形式化地说，在时间步$t$，<strong>contextual-bandit算法</strong>观察到当前用户$u_t$，以及每个可选择的商品（arm）$a$的特征向量$x_{t,a}$。$x_{t,a}$称之为上下文信息，它概况了用户和商品两方面的信息。算法根据之前观察到的反馈结果选择一个商品$a_t$展现给用户，并接受到用户的反馈收益$r_{t,a_t}$，$r_{t,a_t}$的期望取决于用户和商品两个方面。接着，算法根据新的观察$(x_{t,a},a_t,r_{t,a_t})$改进自身选择商品展现的策略，目标是使得整个过程中损失的收益最小，即regret值$R_A(T)$最小。$R_A(T)$的定义如下：</p><script type="math/tex; mode=display">R_A(T)=E\left[ \sum_{t=1}^T r,a_t^* \right]-E\left[ \sum_{t=1}^T r,a_t \right]</script><p>其中，$T$为实验的总步数；$a_t^*$为在时间步$t$时有最大期望收益的arm，不能提前得知。</p><p>LinUCB是处理Contextual Bandit的一个方法，在LinUCB中，设定每个arm的期望收益为该arm的特征向量(context)的线性函数，如下：</p><p>\begin{align}<br>E\left[r_{t,a}|x_{t,a}\right] = x_{t,a}^T\theta_a<br>\end{align}</p><p>$\theta_a$是LinUCB模型的参数，维度为$d$。每个arm维护一个$\theta_a$</p><p>对于单个arm $a$，以其前$m$个context向量为行向量组成的矩阵称为$D_a$，维度为$m \times n$。前$m$个收益（reward）组成的向量称为$c_a$。采用平方损失函数</p><script type="math/tex; mode=display">loss=\sum_{i=1}^m\left(c_{a,i}-\sum_{j=0}^d \theta_{a,j}x_{ij} \right)^2 + \lambda\sum_{j=0}^d \theta_{a,j}^2</script><p>，其中$\lambda$为正则项系数。求损失函数的最小值，令损失函数对$\theta_a$求导，结果为</p><script type="math/tex; mode=display">\nabla_{\theta_a}loss=2D_a^T(c_a-D_a\theta_a)-2\lambda\theta_a</script><p>令$\nabla_{\theta_a}loss=0, \lambda=1$，可得</p><script type="math/tex; mode=display">\theta_a = (D_a^TD_a + I)^{-1}D_a^Tc_a</script><p>使用岭回归（ridge regression）方法，可以得到$\theta_a$的概率分布为高斯分布：</p><p>\begin{align}<br>\theta_a \sim N \left((D_a^TD_a + I)^{-1}D_a^Tc_a, (D_a^TD_a + I)^{-1}\right)<br>\end{align}</p><p>为了符号简洁，令</p><p>\begin{align}<br>\hat{\theta}_a &amp;= (D_a^TD_a + I)^{-1}D_a^Tc_a \\<br>A_a &amp;= D_a^TD_a + I<br>\end{align}</p><p>于是$\theta_a$的概率分布可表示为$\theta_a \sim N(\hat{\theta}_a, A_a^{-1})$</p><p>于是在第$t$次时可以得到<script type="math/tex">x_{t,a}^T\theta_a \sim N(x_{t,a}^T\hat{\theta}_a, x_{t,a}^TA_a^{-1}x_{t,a})</script>，也就是<script type="math/tex">r_{t,a} \sim N(x_{t,a}^T\hat{\theta}_a, x_{t,a}^TA_a^{-1}x_{t,a})</script></p><p>根据高斯分布的性质，得到置信上界后就可以使用普通UCB规则了，即每次选择 $x_{t,a}^T\hat{\theta}_a + \alpha \sqrt{x_{t,a}^TA_a^{-1}x_{t,a})}$最大的arm。</p><p>需要注意的是，$A_a$与$D_a^Tc_a$可以增量异步更新，于是标准流程如下：</p><ul><li>设定$\alpha$</li><li>For $t$ = 1,2,3,…<ul><li>对所有的arm获得本次的context向量</li><li>For all $a$<ul><li>if $a$ is new<ul><li>设置$A_a$为单位矩阵</li><li>设置$b_a$为$d$维零向量</li></ul></li><li>计算$\hat{\theta}_a = A_a^{-1}b_a$</li><li>计算上界 $p_{t,a}=x_{t,a}^T\hat{\theta}_a + \alpha \sqrt{x_{t,a}^TA_a^{-1}x_{t,a})}$</li></ul></li><li>选择最大上界$p_{t,a}$对应的arm即$a_t$，并得到对应的$r_t$</li><li>更新$A_{a_t} = A_{a_t} + x_{t,a_t}x_{t,a_t}^T$</li><li>更新$b_{a_t} = b_{a_t} + r_tx_{t,a_t}$</li></ul></li></ul><p>LinUCB算法的优势：</p><ul><li>计算复杂度与arm的数量成线性关系</li><li>支持动态变化的候选arm集合</li></ul><h2 id="3-业务场景介绍"><a href="#3-业务场景介绍" class="headerlink" title="3. 业务场景介绍"></a>3. 业务场景介绍</h2><p>在我们的电商平台App首页，有一个商品瀑布流推荐场景，每次大概展示30个商品左右。商品候选集都是运营人工精选的历史销售情况较好，在更多流量刺激下有可能成为爆款的商品，并且每天都会汰换掉一部分，加入一些新品进来。</p><p>用过实现LinUcb算法，系统会对每个商品做充分的exploration和exploitation，从而发掘出真正有销售潜力的商品，逐渐淘汰掉不够理想的商品，纠正运营人工选品的局限。经过考验的商品，说明在一段时间内销量还是不错的，这些商品运营可以深度控价，要求商家提供更多的优惠和让利给用户，从而形成良性循环，同时也给其他商家树立标杆，促进平台更加健康地发展。</p><h2 id="4-系统架构"><a href="#4-系统架构" class="headerlink" title="4. 系统架构"></a>4. 系统架构</h2><p>在我们的系统中，LinUCB算法的实现分为两个部分：一部分实现在推荐引擎里，主要完成特征向量提取、获取每个商品的$A_a$矩阵和$b_a$向量数据、完成置信区间上届的计算、并选择最终展现的商品；另一部分逻辑实现在实时计算平台Storm上，这部分任务实时解析场景的曝光、点击和购买行为日志，每条日志里包含了商品ID、时间戳和特征向量等信息，根据公式更新每个商品的$A_a$矩阵和$b_a$向量，并把更新后的结果写到Redis缓存里，供推荐引擎获取。</p><p><img src="/linucb/LinUCB.png" alt="linucb"></p><h2 id="5-核心代码逻辑"><a href="#5-核心代码逻辑" class="headerlink" title="5. 核心代码逻辑"></a>5. 核心代码逻辑</h2><p>每个商品都维护一个队列queue，用于临时存放接受到的行为事件。之所以不在接受到事情的时候立马处理掉，主要有两个原因：一是用户对每个展现商品的反馈并不是原子操作，比如反馈是曝光未点击，或是曝光且点击但未购买，还是即曝光又点击最后还购买了，这些操作不是一步就能完成的，而是保护了好几步，因此在仅仅收到曝光事情或者点击事情时，我们不知道用户的反馈序列操作有没有结束，也就不能准确设置反馈的收益值；二是由于客户端对曝光事情有缓存不是实时上报的，因而不能保证曝光事情一定在点击事情之前被上传到日志服务器。</p><p>具体实现时，设置一个时间间隔（比如，5分钟），一个曝光或者点击事情到达Storm计算节点时，先缓存在队列里，等待设置好的时间间隔后，再消费掉。如果在时间间隔内高优先级的事情到达，则会直接移除掉低优先级的事情。优先级顺序为购买大于点击、点击大于曝光。另外，为了防止日志重复上报，还会根据推荐引擎的请求ID（不同请求不同）对接受到是事情去重。当然，为了容错，如果预设的事情队列满了，则会动态把时间间隔缩短为原来的二分之一，把新的时间间隔之前的事情强制消费掉，这个过程会递归执行，直到队列的长度小于预设的大小为止。</p><p>Storm任务中，某个商品的参数（$A_a$矩阵和$b_a$向量）更新逻辑：<br><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> int consume(Event <span class="keyword">new</span><span class="type">Event</span>, Config config) &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">new</span><span class="type">Event</span>.getPid() != pid)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    reset(config);</span><br><span class="line"></span><br><span class="line">    int showConsumedCount = <span class="number">0</span>;</span><br><span class="line">    int clickConsumedCount = <span class="number">0</span>;</span><br><span class="line">    long current = System.currentTimeMillis();</span><br><span class="line">    Iterator&lt;Event&gt; iter = queue.iterator();</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">        Event event = iter.next();</span><br><span class="line">        <span class="keyword">if</span> (event.isSameFlow(<span class="keyword">new</span><span class="type">Event</span>)) &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">new</span><span class="type">Event</span>.isShow()) &#123;</span><br><span class="line">                <span class="keyword">new</span><span class="type">Event</span> = <span class="literal">null</span>; <span class="comment">// 重复曝光，或者点击事件在曝光事件前到达</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">new</span><span class="type">Event</span>.isClick() &amp;&amp; event.isClick()) &#123;</span><br><span class="line">                <span class="keyword">new</span><span class="type">Event</span> = <span class="literal">null</span>; <span class="comment">// 重复点击事件</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            iter.remove(); <span class="comment">// 高优先级的事情会覆盖低优先级的事情</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="literal">null</span> == <span class="keyword">new</span><span class="type">Event</span>.getItemTrackData()) &#123;</span><br><span class="line">                <span class="keyword">new</span><span class="type">Event</span>.setItemTrackData(event.getItemTrackData());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (current - event.getTimestamp() &lt; config.cacheTimeSpan)</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">        INDArray xt = event.getFeature(config);</span><br><span class="line">        INDArray x = xt.transpose();</span><br><span class="line">        A.addi(x.mmul(xt));</span><br><span class="line">        double reward = event.getReward();</span><br><span class="line">        <span class="keyword">if</span> (reward &gt; <span class="number">0.0</span>)</span><br><span class="line">            b.addi(x.muli(reward));</span><br><span class="line">        iter.remove();</span><br><span class="line">        <span class="keyword">if</span> (event.isShow())</span><br><span class="line">            showConsumedCount ++;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (event.isClick())</span><br><span class="line">            clickConsumedCount ++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (<span class="literal">null</span> == <span class="keyword">new</span><span class="type">Event</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">new</span><span class="type">Event</span>.getType().equals(EventType.BUY)) &#123;</span><br><span class="line">        <span class="comment">// 购买事情立即消费</span></span><br><span class="line">        INDArray xt = <span class="keyword">new</span><span class="type">Event</span>.getFeature(config);</span><br><span class="line">        <span class="keyword">if</span> (<span class="literal">null</span> == xt) &#123;</span><br><span class="line">            LogPushUtil.push(<span class="string">"BuyFeatureMissingEvent"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (xt.length() == b.length()) &#123;</span><br><span class="line">            INDArray x = xt.transpose();</span><br><span class="line">            A.addi(x.mmul(xt));</span><br><span class="line">            double reward = <span class="keyword">new</span><span class="type">Event</span>.getReward();</span><br><span class="line">            b.addi(x.muli(reward));</span><br><span class="line">            LogPushUtil.push(<span class="string">"BuyEventConsumed"</span>);</span><br><span class="line">            records += config.updateRecords;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (StringUtils.isNotEmpty(<span class="keyword">new</span><span class="type">Event</span>.getItemTrackData())) &#123;</span><br><span class="line">        queue.add(<span class="keyword">new</span><span class="type">Event</span>);</span><br><span class="line">        checkQueueIsFullOrNot(config.cacheTimeSpan / <span class="number">2</span>, config);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (showConsumedCount &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        records += showConsumedCount;</span><br><span class="line">        LogPushUtil.push(<span class="string">"ShowEventConsumed"</span>, showConsumedCount);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (clickConsumedCount &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        records += clickConsumedCount;</span><br><span class="line">        LogPushUtil.push(<span class="string">"ClickEventConsumed"</span>, clickConsumedCount);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (records &gt;= config.updateRecords) &#123;</span><br><span class="line">        writeToRedis(config);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> records;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在线推荐引擎计算每个商品的ucb分数的代码如下：<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> parallelGetLinUcbScore(Session session, INDArray features, List&lt;Item&gt; items) throws InterruptedException &#123;</span><br><span class="line">    <span class="keyword">int</span> concurrency = Math.<span class="built_in">min</span>(session.<span class="built_in">config</span>.linUcbConcurrency, items.<span class="built_in">size</span>());</span><br><span class="line">    <span class="keyword">int</span> num = items.<span class="built_in">size</span>() / concurrency;</span><br><span class="line"></span><br><span class="line">    ExecutorService es = session.context.getThreadExecutorService(session.reqId, session.reqId.getSceneId());</span><br><span class="line">    final CountDownLatch countDownLatch = <span class="keyword">new</span> CountDownLatch(concurrency);</span><br><span class="line">    <span class="keyword">int</span> left = items.<span class="built_in">size</span>() - num * concurrency;</span><br><span class="line">    <span class="keyword">int</span> start = <span class="number">0</span>, <span class="built_in">end</span> = num;</span><br><span class="line">    <span class="built_in">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; concurrency; ++i) &#123;</span><br><span class="line">        <span class="built_in">if</span> (i &lt; left)</span><br><span class="line">            <span class="built_in">end</span>++;</span><br><span class="line">        Logger.debug(<span class="string">"prepare to start linucb score sub thread: ["</span> + start + <span class="string">", "</span> + <span class="built_in">end</span> + <span class="string">"]"</span>);</span><br><span class="line">        <span class="keyword">int</span> finalStart = start;</span><br><span class="line">        <span class="keyword">int</span> finalEnd = <span class="built_in">end</span>;</span><br><span class="line">        Runnable runnable = () -&gt; &#123;</span><br><span class="line">            <span class="keyword">long</span> startTime = System.currentTimeMillis();</span><br><span class="line">            <span class="built_in">try</span> &#123;</span><br><span class="line">                getLinUcbScore(session, features, items, finalStart, finalEnd);</span><br><span class="line">            &#125; <span class="built_in">catch</span> (Exception e) &#123;</span><br><span class="line">                <span class="keyword">String</span> exception = ExceptionUtils.getFullStackTrace(e);</span><br><span class="line">                Logger.error(<span class="string">"linucb worker exception:"</span>, exception);</span><br><span class="line">            &#125;</span><br><span class="line">            finally &#123;</span><br><span class="line">                countDownLatch.countDown();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">long</span> time = System.currentTimeMillis() - startTime;</span><br><span class="line">            Logger.info(<span class="string">"[Timer] compute sub linUcb scores ["</span>, finalStart, <span class="string">", "</span>, finalEnd, <span class="string">"] taken"</span>, time, <span class="string">"ms"</span>);</span><br><span class="line">        &#125;;</span><br><span class="line">        es.execute(TtlRunnable.<span class="built_in">get</span>(runnable));</span><br><span class="line">        start = <span class="built_in">end</span>;</span><br><span class="line">        <span class="built_in">end</span> = start + num;</span><br><span class="line">    &#125;</span><br><span class="line">    countDownLatch.await(session.<span class="built_in">config</span>.linUcbTimeOut, TimeUnit.MILLISECONDS);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> getLinUcbScore(Session session, INDArray features, List&lt;Item&gt; items, <span class="keyword">int</span> start, <span class="keyword">int</span> <span class="built_in">end</span>) throws IOException &#123;</span><br><span class="line">    <span class="built_in">if</span> (items.isEmpty() || null == features)</span><br><span class="line">        <span class="built_in">return</span>;</span><br><span class="line">    StopWatch stopWatch = StopWatch.CreateStopWatchAndStart();</span><br><span class="line">    <span class="built_in">for</span> (<span class="keyword">int</span> i = start; i &lt; <span class="built_in">end</span>; ++i) &#123;</span><br><span class="line">        stopWatch.restart();</span><br><span class="line">        Item item = items.<span class="built_in">get</span>(i);</span><br><span class="line">        <span class="keyword">long</span> pid = item.getProductId();</span><br><span class="line">        <span class="keyword">byte</span>[] matrix = getLinUcbMatrix(session, pid);</span><br><span class="line"></span><br><span class="line">        INDArray feature = features.getRow(i);</span><br><span class="line">        INDArray featureT = feature.transpose();</span><br><span class="line">        Logger.detail(<span class="string">"&lt;"</span>, i, <span class="string">"&gt;"</span>, pid, <span class="string">"item feature:"</span>, item.getTrackInfo(), <span class="string">"vector:"</span>, feature);</span><br><span class="line">        <span class="built_in">if</span> (matrix == null) &#123;</span><br><span class="line">            <span class="keyword">double</span> p = session.<span class="built_in">config</span>.linUcbAlpha * FastMath.<span class="built_in">sqrt</span>(feature.mmul(featureT).getDouble(<span class="number">0</span>));</span><br><span class="line">            item.setMatchScore(p);</span><br><span class="line">            Logger.info(<span class="string">"&lt;"</span>, i, <span class="string">"&gt; linucb new item:"</span>, pid, <span class="string">"ucb score:"</span>, p);</span><br><span class="line">            <span class="built_in">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ByteArrayInputStream in = <span class="keyword">new</span> ByteArrayInputStream(matrix);</span><br><span class="line">        INDArray[] Ab = ByteSerde.fromByteArrayStream(in);</span><br><span class="line">        <span class="built_in">if</span> (Ab.length &lt; <span class="number">2</span>)</span><br><span class="line">            <span class="built_in">continue</span>;</span><br><span class="line">        INDArray invertA = Ab[<span class="number">0</span>];</span><br><span class="line">        INDArray b = Ab[<span class="number">1</span>];</span><br><span class="line">        Logger.detail(<span class="string">"&lt;"</span>, i, <span class="string">"&gt;"</span>, pid, <span class="string">"[Timer] fetch matrix taken"</span>, stopWatch.<span class="built_in">click</span>(), <span class="string">"ms"</span>);</span><br><span class="line">        <span class="built_in">if</span> (b.length() != feature.length()) &#123;</span><br><span class="line">            Logger.error(<span class="string">"the length of b is"</span>, b.length(), <span class="string">"should be"</span>, feature.length());</span><br><span class="line">            <span class="keyword">double</span> p = session.<span class="built_in">config</span>.linUcbAlpha * FastMath.<span class="built_in">sqrt</span>(feature.mmuli(featureT).getFloat(<span class="number">0</span>));</span><br><span class="line">            item.setMatchScore(p);</span><br><span class="line">            <span class="built_in">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        INDArray theta = invertA.mmul(b);</span><br><span class="line">        INDArray thetaX = theta.transposei().mmul(featureT);</span><br><span class="line">        INDArray temp = feature.mmuli(invertA).mmul(featureT);</span><br><span class="line">        <span class="keyword">double</span> p = thetaX.getDouble(<span class="number">0</span>);</span><br><span class="line">        item.setPreferScore(p);</span><br><span class="line">        p += session.<span class="built_in">config</span>.linUcbAlpha * FastMath.<span class="built_in">sqrt</span>(temp.getDouble(<span class="number">0</span>));</span><br><span class="line">        item.setMatchScore(p);</span><br><span class="line">        Logger.detail(<span class="string">"&lt;"</span>, i, <span class="string">"&gt;"</span>, pid, <span class="string">"[Timer] one iteration linucb score taken"</span>, stopWatch.<span class="built_in">click</span>(), <span class="string">"ms. match score:"</span>, p);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>实现中矩阵计算的部分，用了ND4J（<a href="https://nd4j.org/）的库。" target="_blank" rel="noopener">https://nd4j.org/）的库。</a></p><h2 id="6-业务效果"><a href="#6-业务效果" class="headerlink" title="6. 业务效果"></a>6. 业务效果</h2><p>经过线上充分的A/B测试，最终测得LinUCB算法的UV CTR相对基准桶提升25%+，UV价值提升20%+。并且算法能够很好地支持商品动态上下架。</p><h2 id="7-参考资料"><a href="#7-参考资料" class="headerlink" title="7. 参考资料"></a>7. 参考资料</h2><p>[Lihong Li, et al, 2010] A Contextual-Bandit Approach to Personalized News Article Recommendation.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;推荐系统选择商品展现给用户，并期待用户的正向反馈（点击、成交）。然而推荐系统并不能提前知道用户在观察到商品之后如何反馈，也就是不能提前获得本次推荐的收益，唯一能做的就是不停地尝试，并实时收集反馈以便更新自己试错的策略。目的是使得整个过程损失的收益最小。这一过程就类似与一个赌徒在赌场里玩老虎机赌博。赌徒要去摇老虎机，走进赌场一看，一排老虎机，外表一模一样，但是每个老虎机吐钱的概率可不一样，他不知道每个老虎机吐钱的概率分布是什么，那么每次该选择哪个老虎机可以做到最大化收益呢？这就是多臂赌博机问题（Multi-armed bandit problem, MAB）。&lt;/p&gt;
&lt;p&gt;MAB问题的难点是Exploitation-Exploration(E&amp;amp;E)两难的问题：对已知的吐钱概率比较高的老虎机，应该更多的去尝试(exploitation)，以便获得一定的累计收益；对未知的或尝试次数较少的老虎机，还要分配一定的尝试机会（exploration），以免错失收益更高的选择，但同时较多的探索也意味着较高的风险（机会成本）。&lt;/p&gt;
&lt;p&gt;Bandit算法是一类用来实现Exploitation-Exploration机制的策略。根据是否考虑上下文特征，Bandit算法分为context-free bandit和contextual bandit两大类。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法模型" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%AE%97%E6%B3%95%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="推荐系统" scheme="http://xudongyang.coding.me/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="LinUCB" scheme="http://xudongyang.coding.me/tags/LinUCB/"/>
    
      <category term="在线学习" scheme="http://xudongyang.coding.me/tags/%E5%9C%A8%E7%BA%BF%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="E&amp;E" scheme="http://xudongyang.coding.me/tags/E-E/"/>
    
  </entry>
  
  <entry>
    <title>商品人气分模型</title>
    <link href="http://xudongyang.coding.me/item-pop-score/"/>
    <id>http://xudongyang.coding.me/item-pop-score/</id>
    <published>2018-04-10T09:27:29.000Z</published>
    <updated>2019-04-02T02:43:36.846Z</updated>
    
    <content type="html"><![CDATA[<p>在电商平台中，量化每个商品的静态质量及受欢迎的程度有着重要的意义。我们把这个量化值称之为商品人气分。商品人气分在搜索排序、个性化推荐排序及推荐候选集截断、竞价广告系统中都有着重要的应用。</p><p>商品人气分受哪些因素的影响，以及这些因素最终如何共同决定商品人气分值？本文总结了一个实际系统中人气分模型的构建过程，从特征和模型两个角度来回答上述两个问题。<br><a id="more"></a></p><h2 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h2><p>多维度交叉统计特征是一类重要的特征来源，主要有以下4个维度：实体、时间段、行为类型、统计量。</p><ol><li>实体维度：商品、品牌、商家（店铺）、叶子类目、一级类目等。</li><li>时间维度：1天、3天、7天、14天、30天、时间衰减加权等。</li><li>行为维度：曝光、浏览、收藏、加购、购买、评级、商详页停留时间、是否退货等。</li><li>统计维度：数量、人数、频率、排名（百分比）、点击率、转化率、金额等。</li></ol><p>具体地，每个特征从以上4个维度中各取一到两个进行组合，再从历史数据中统计该组合特征最终的特征值。比如，商品（实体）最近1天（时间）的曝光（行为）量（统计指标）；商品所在店铺（实体）最近30天（时间）的销量（行为类型+统计维度）；商品（实体）最近7天（时间）的平均成交（行为）单价（统计）在同一叶子类目下的排名百分比（统计）；等等。由以上方法产生的特征数量级相当于4个维度的笛卡尔积。可以看出这些特征覆盖了大家常说的销售额、销售量、转化率、评论数、好评率、差评率、退货率、加购数、收藏关注度、详情页访问深度等方方面面。</p><p>另外还有一些不便归类到上述4个维度的特征，比如商品“年龄”（从上架到目前为止的时间间隔），DQC，店铺DSR，ID类特征（如一级类目）等。</p><h2 id="样本"><a href="#样本" class="headerlink" title="样本"></a>样本</h2><p>最近1天的成交日志留着生成训练目标（该商品当天有成交则为正样本，当天没有成交则为负样本），1天前的一个月的各类日志用来统计特征值，这样便可以生成1天的训练数据。</p><p>为了有足够数量的训练样本，我们取7天的训练数据的并集作为最终的训练集，同时留一到两天的数据作为验证集合测试集。流程如下图所示。</p><p><img src="/item-pop-score/sample.png" alt></p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>选用了GBDT作为待拟合的模型，损失函数为binaryLogLoss函数，最终模型的结果解释为商品未来1天有成交的概率。之所以选择GBDT模型，是因为一方面我们需要足够的非线性拟合能力；另外一方面是因为GBDT算法足够简单，在大多数任务中都表现良好，并且开源的工具也比较成熟。</p><p>模型调参基本遵循业界的经验值，同时各个参数在小范围内用GridSearch的方法交叉验证取得。特别值得一提的是设置样本权重对最终效果的提升有很大帮助，具体地，所有负样本的权重设置为1，正样本的权重设置为该商品在当天的成交件数。</p><h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><p>在我们公司多个推荐场景的实测结果，与没有人气分的基准桶相比，UV点击率和UV价值都取得了5%~10%左右的提升。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在电商平台中，量化每个商品的静态质量及受欢迎的程度有着重要的意义。我们把这个量化值称之为商品人气分。商品人气分在搜索排序、个性化推荐排序及推荐候选集截断、竞价广告系统中都有着重要的应用。&lt;/p&gt;
&lt;p&gt;商品人气分受哪些因素的影响，以及这些因素最终如何共同决定商品人气分值？本文总结了一个实际系统中人气分模型的构建过程，从特征和模型两个角度来回答上述两个问题。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="GBDT" scheme="http://xudongyang.coding.me/tags/GBDT/"/>
    
  </entry>
  
  <entry>
    <title>主流CTR预估模型的演化及对比</title>
    <link href="http://xudongyang.coding.me/ctr-models/"/>
    <id>http://xudongyang.coding.me/ctr-models/</id>
    <published>2018-03-30T08:33:48.000Z</published>
    <updated>2019-04-02T02:43:36.797Z</updated>
    
    <content type="html"><![CDATA[<p>学习和预测用户的反馈对于个性化推荐、信息检索和在线广告等领域都有着极其重要的作用。在这些领域，用户的反馈行为包括点击、收藏、购买等。本文以点击率（CTR）预估为例，介绍常用的CTR预估模型，试图找出它们之间的关联和演化规律。<br><a id="more"></a></p><h2 id="数据特点"><a href="#数据特点" class="headerlink" title="数据特点"></a>数据特点</h2><p>在电商领域，CTR预估模型的原始特征数据通常包括多个类别，比如[Weekday=Tuesday,<br>Gender=Male, City=London, CategoryId=16]，这些原始特征通常以独热编码（one-hot encoding）的方式转化为高维稀疏二值向量，多个域（类别）对应的编码向量链接在一起构成最终的特征向量。<br><img src="/ctr-models/one-hot-encoding.png" alt="one-hot-encoding"></p><p><strong>高维</strong>、<strong>稀疏</strong>、<strong>多Field</strong>是输入给CTR预估模型的特征数据的典型特点。以下介绍的模型都假设特征数据满足上述规律，那些只适用于小规模数据量的模型就不介绍了。</p><h2 id="Embedding表示"><a href="#Embedding表示" class="headerlink" title="Embedding表示"></a>Embedding表示</h2><p>由于即将要介绍的大部分模型都或多或少使用了特征的embedding表示，这里做一个简单的介绍。</p><p>Embedding表示也叫做Distributed representation，起源于神经网络语言模型（NNLM）对语料库中的word的一种表示方法。相对于高维稀疏的one-hot编码表示，embedding-based的方法，学习一个低维稠密实数向量（low-dimensional dense embedding）。类似于hash方法，embedding方法把位数较多的稀疏数据压缩到位数较少的空间，不可避免会有冲突；然而，embedding学到的是类似主题的语义表示，对于item的“冲突”是希望发生的，这有点像软聚类，这样才能解决稀疏性的问题。</p><p>Google公司开源的word2vec工具让embedding表示方法广为人知。Embedding表示通常用神经网络模型来学习，当然也有其他学习方法，比如矩阵分解（MF）、因子分解机（FM)等。这里详细介绍一下基于神经网络的embedding学习方法。</p><p>通常Embedding向量并不是通过一个专门的任务学习得到的，而是其他学习任务的附属产出。如下图所示，网络的输入层是实体ID（categorical特征）的one-hot编码向量。与输入层相连的一层就是Embedding层，两层之间通过全连接的方式相连。Embedding层的神经元个数即Embeeding向量的维数（$m$）。输入层与Embedding层的链接对应的权重矩阵$M(n \times m)$，即对应$n$个输入实体的m维embedding向量。由于one-hot向量同一时刻只会有一个元素值为1，其他值都是0，因此对于当前样本，只有与值为1的输入节点相连的边上的权重会被更新，即不同ID的实体所在的样本训练过程中只会影响与该实体对应的embedding表示。假设某实体ID的one-hot向量中下标为$i$的值为1，则该实体的embedding向量为权重矩阵$M$的第$i$行。<br><img src="/ctr-models/embedding.png" alt="embedding"></p><h2 id="常用模型"><a href="#常用模型" class="headerlink" title="常用模型"></a>常用模型</h2><h3 id="1-LR"><a href="#1-LR" class="headerlink" title="1. LR"></a>1. LR</h3><p>LR模型是广义线性模型，从其函数形式来看，LR模型可以看做是一个没有隐层的神经网络模型（感知机模型）。</p><script type="math/tex; mode=display">y=\frac{1}{1+e^{-(wx+b)}}</script><p><img src="/ctr-models/lr.jpg" alt="lr"></p><p>LR模型一直是CTR预估问题的benchmark模型，由于其简单、易于并行化实现、可解释性强等优点而被广泛使用。然而由于线性模型本身的局限，不能处理特征和目标之间的非线性关系，因此模型效果严重依赖于算法工程师的特征工程经验。</p><p>为了让线性模型能够学习到原始特征与拟合目标之间的非线性关系，通常需要对原始特征做一些非线性转换。常用的转换方法包括：连续特征离散化、特征之间的交叉等。</p><p>连续特征离散化的方法一般是把原始连续值的值域范围划分为多个区间，比如等频划分或等间距划分，更好的划分方法是利用监督学习的方式训练一个简单的单特征的决策树桩模型，即用信息增益指标来决定分裂点。特征分区间之后，每个区间上目标（y）的分布可能是不同的，从而每个区间对应的新特征在模型训练结束后都能拥有独立的权重系数。特征离散化相当于把线性函数变成了分段线性函数，从而引入了非线性结构。比如不同年龄段的用户的行为模式可能是不同的，但是并不意味着年龄越大就对拟合目标（比如，点击率）的贡献越大，因此直接把年龄作为特征值训练就不合适。而把年龄分段后，模型就能够学习到不同年龄段的用户的不同偏好模式。离散化的其他好处还包括对数据中的噪音有更好的鲁棒性（异常值也落在一个划分区间，异常值本身的大小不会过度影响模型预测结果）；离散化还使得模型更加稳定，特征值本身的微小变化（只有还落在原来的划分区间）不会引起模型预测值的变化。</p><p>特征交叉是另一种常用的引入非线性性的特征工程方法。通常CTR预估涉及到用户、物品、上下文等几方面的特征，往往单个特征对目标判定的贡献是较弱的，而不同类型的特征组合在一起就能够对目标的判定产生较强的贡献。比如用户性别和商品类目交叉就能够刻画例如“女性用户偏爱美妆类目”，“男性用户喜欢男装类目”的知识。特征交叉是算法工程师把领域知识融入模型的一种方式。</p><p>LR模型的不足在于特征工程耗费了大量的精力，而且即使有经验的工程师也很难穷尽所有的特征交叉组合。</p><h3 id="2-LR-GBDT"><a href="#2-LR-GBDT" class="headerlink" title="2. LR + GBDT"></a>2. LR + GBDT</h3><p>既然特征工程很难，那能否自动完成呢？模型级联提供了一种思路，典型的例子就是Facebook 2014年的论文中介绍的通过<a href="/gbdt/" title="GBDT（Gradient Boost Decision Tree）">GBDT（Gradient Boost Decision Tree）</a>模型解决LR模型的特征组合问题。思路很简单，特征工程分为两部分，一部分特征用于训练一个<a href="/gbdt/" title="GBDT">GBDT</a>模型，把GBDT模型每颗树的叶子节点编号作为新的特征，加入到原始特征集中，再用LR模型训练最终的模型。<br><img src="/ctr-models/lr+gbdt.jpg" alt="lr+gbdt"></p><p>GBDT模型能够学习高阶非线性特征组合，对应树的一条路径（用叶子节点来表示）。通常把一些连续值特征、值空间不大的categorical特征都丢给GBDT模型；空间很大的ID特征（比如商品ID）留在LR模型中训练，既能做高阶特征组合又能利用线性模型易于处理大规模稀疏数据的优势。</p><h3 id="3-FM、FFM"><a href="#3-FM、FFM" class="headerlink" title="3. FM、FFM"></a>3. FM、FFM</h3><p>因子分解机(Factorization Machines, FM)通过特征对之间的隐变量内积来提取特征组合，其函数形式如下：</p><script type="math/tex; mode=display">y=w_0 + \sum_{i=1}^{n}w_i x_i + \sum_{i=1}^{n}\sum_{j=i+1}^n \langle v_i,v_j \rangle x_i x_j</script><p>FM和基于树的模型（e.g. GBDT）都能够自动学习特征交叉组合。基于树的模型适合连续中低度稀疏数据，容易学到高阶组合。但是树模型却不适合学习高度稀疏数据的特征组合，一方面高度稀疏数据的特征维度一般很高，这时基于树的模型学习效率很低，甚至不可行；另一方面树模型也不能学习到训练数据中很少或没有出现的特征组合。相反，FM模型因为通过隐向量的内积来提取特征组合，对于训练数据中很少或没有出现的特征组合也能够学习到。例如，特征$i$和特征$j$在训练数据中从来没有成对出现过，但特征$i$经常和特征$p$成对出现，特征$j$也经常和特征$p$成对出现，因而在FM模型中特征$i$和特征$j$也会有一定的相关性。毕竟所有包含特征$i$的训练样本都会导致模型更新特征$i$的隐向量$v_i$，同理，所有包含特征$j$的样本也会导致模型更新隐向量$v_j$，这样$\langle v_i,v_j \rangle$就不太可能为0。</p><p>在推荐系统中，常用矩阵分解（MF）的方法把User-Item评分矩阵分解为两个低秩矩阵的乘积，这两个低秩矩阵分别为User和Item的隐向量集合。通过User和Item隐向量的点积来预测用户对未见过的物品的兴趣。矩阵分解也是生成embedding表示的一种方法，示例图如下：<br><img src="/ctr-models/mf.jpg" alt="mf"></p><p>MF方法可以看作是FM模型的一种特例，即MF可以看作特征只有userId和itemId的FM模型。FM的优势是能够将更多的特征融入到这个框架中，并且可以同时使用一阶和二阶特征；而MF只能使用两个实体的二阶特征。<br><img src="/ctr-models/fm.jpg" alt="fm"></p><p>在二分类问题中，采用LogLoss损失函数时，FM模型可以看做是LR模型和MF方法的融合，如下图所示：<br><img src="/ctr-models/fm-equation.jpg" alt></p><p>FFM（Field-aware Factorization Machine）模型是对FM模型的扩展，通过引入field的概念，FFM把相同性质的特征归于同一个field。例如，“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15”这三个特征都是代表日期的，可以放到同一个field中。同理，商品的末级品类编码也可以放到同一个field中。简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field，包括用户性别、职业、品类偏好等。在FFM中，每一维特征$x_i$，针对其它特征的每一种field $f_j$，都会学习一个隐向量 $v_{i,f_j}$。因此，隐向量不仅与特征相关，也与field相关。假设样本的$n$个特征属于$f$ 个field，那么FFM的二次项有$nf$个隐向量。</p><script type="math/tex; mode=display">y=w_0 + \sum_{i=1}^{n}w_i x_i + \sum_{i=1}^{n}\sum_{j=i+1}^n \langle v_{i,f_j},v_{j,f_i} \rangle x_i x_j</script><p>FM可以看作FFM的特例，在FM模型中，每一维特征的隐向量只有一个，即FM是把所有特征都归属到一个field时的FFM模型。</p><h3 id="4-混合逻辑回归（MLR）"><a href="#4-混合逻辑回归（MLR）" class="headerlink" title="4. 混合逻辑回归（MLR）"></a>4. 混合逻辑回归（MLR）</h3><p>MLR算法是alibaba在2012年提出并使用的广告点击率预估模型，2017年发表出来。MLR模型是对线性LR模型的推广，它利用分片线性方式对数据进行拟合。基本思路是采用分而治之的策略：如果分类空间本身是非线性的，则按照合适的方式把空间分为多个区域，每个区域里面可以用线性的方式进行拟合，最后MLR的输出就变为了多个子区域预测值的加权平均。如下图(C)所示，就是使用4个分片的MLR模型学到的结果。<br><img src="/ctr-models/mlr.png" alt></p><script type="math/tex; mode=display">f(x)=\sum_{i=1}^m \pi_i(x,\mu)\cdot \eta_i(x,w)=\sum_{i=1}^m \frac{e^{\mu_i^T x}}{\sum_{j=1}^m e^{\mu_j^T x}} \cdot \frac{1}{1+e^{-w^Tx}}</script><p>上式即为MLR的目标函数，其中$m$为分片数（当$m=1$时，MLR退化为LR模型）；$\pi_i(x,\mu)= \frac{e^{\mu_i^T x}}{\sum_{j=1}^m e^{\mu_j^T x}}$是聚类参数，决定分片空间的划分，即某个样本属于某个特定分片的概率；$\eta_i(x,w) = \frac{1}{1+e^{-w^Tx}}$是分类参数，决定分片空间内的预测；$\mu$和$w$都是待学习的参数。最终模型的预测值为所有分片对应的子模型的预测值的期望。</p><p>MLR模型在大规模稀疏数据上探索和实现了非线性拟合能力，在分片数足够多时，有较强的非线性能力；同时模型复杂度可控，有较好泛化能力；同时保留了LR模型的自动特征选择能力。</p><p>MLR模型的思路非常简单，难点和挑战在于MLR模型的目标函数是非凸非光滑的，使得传统的梯度下降算法并不适用。相关的细节内容查询论文：Gai et al, “Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction” 。</p><p>另一方面，MLR模型可以看作带有一个隐层的神经网络。如下图，$x$是大规模的稀疏输入数据，MLR模型第一步是做了一个Embedding操作，分为两个部分，一种叫聚类Embedding（绿色），另一种是分类Embedding（红色）。两个投影都投到低维的空间，维度为$m$，是MLR模型中的分片数。完成投影之后，通过很简单的内积（Inner Product）操作便可以进行预测，得到输出$y$。<br><img src="/ctr-models/mlr.jpg" alt="MLR"></p><h3 id="5-WDL（Wide-amp-Deep-Learning）"><a href="#5-WDL（Wide-amp-Deep-Learning）" class="headerlink" title="5. WDL（Wide &amp; Deep Learning）"></a>5. WDL（Wide &amp; Deep Learning）</h3><p>像LR这样的wide模型学习特征与目标之间的直接相关关系，偏重记忆（memorization），如在推荐系统中，wide模型产生的推荐是与用户历史行为的物品直接相关的物品。这样的模型缺乏刻画特征之间的关系的能力，比如模型无法感知到“土豆”和“马铃薯”是相同的实体，在训练样本中没有出现的特征组合自然就无法使用，因此可能模型学习到某种类型的用户喜欢“土豆”，但却会判定该类型的用户不喜欢“马铃薯”。</p><p>WDL是Google在2016年的paper中提出的模型，其巧妙地将传统的特征工程与深度模型进行了强强联合。模型结构如下:<br><img src="/ctr-models/wdl.png" alt="wdl"></p><p>WDL分为wide和deep两部分联合训练，单看wide部分与LR模型并没有什么区别；deep部分则是先对不同的ID类型特征做embedding，在embedding层接一个全连接的MLP（多层感知机），用于学习特征之间的高阶交叉组合关系。由于Embedding机制的引入，WDL相对于单纯的wide模型有更强的泛化能力。</p><h3 id="6-FNN-Factorization-machine-supported-Neural-Network"><a href="#6-FNN-Factorization-machine-supported-Neural-Network" class="headerlink" title="6. FNN (Factorization-machine supported Neural Network)"></a>6. FNN (Factorization-machine supported Neural Network)</h3><p>除了神经网络模型，FM模型也可以用来学习到特征的隐向量（embedding表示），因此一个自然的想法就是先用FM模型学习到特征的embedding表示，再用学到的embedding向量代替原始特征作为最终模型的特征。这个思路类似于LR+GBDT，整个学习过程分为两个阶段：第一个阶段先用一个模型做特征工程；第二个阶段用第一个阶段学习到新特征训练最终的模型。</p><p>FNN模型就是用FM模型学习到的embedding向量初始化MLP，再由MLP完成最终学习，其模型结构如下：<br><img src="/ctr-models/fnn.jpg" alt="fnn"></p><h3 id="7-PNN（Product-based-Neural-Networks）"><a href="#7-PNN（Product-based-Neural-Networks）" class="headerlink" title="7. PNN（Product-based Neural Networks）"></a>7. PNN（Product-based Neural Networks）</h3><p>MLP中的节点add操作可能不能有效探索到不同类别数据之间的交互关系，虽然MLP理论上可以以任意精度逼近任意函数，但越泛化的表达，拟合到具体数据的特定模式越不容易。PNN主要是在深度学习网络中增加了一个inner/outer product layer，用来建模特征之间的关系。</p><p><img src="/ctr-models/PNN.png" alt="PNN"></p><p>Embedding Layer和Product Layer之间的权重为常量1，在学习过程中不更新。Product Layer的节点分为两部分，一部分是$z$向量，另一部分是$p$向量。$z$向量的维数与输入层的Field个数（$N$）相同，$z=(f_1,f_2,f_3,\cdots,f_N)$。$p$向量的每个元素的值由embedding层的feature向量两两成对并经过Product操作之后生成，$p=\{g(f_i,f_j)\},i=1 \cdots N, j=i \cdots N$，因此$p$向量的维度为$N*(N-1)$。这里的$f_i$是field $i$的embedding向量，$f_i=W_0^i x[start_i : end_i]$，其中$x$是输入向量，$x[start_i : end_i]$是field $i$的one-hot编码向量。</p><p>这里所说的Product操作有两种：内积和外积；对应的网络结构分别为IPNN和OPNN，两者的区别如下图。</p><p><img src="/ctr-models/product_operation.jpg" alt></p><p>在IPNN中，由于Product Layer的$p$向量由field两两配对产生，因此维度膨胀很大，给l1 Layer的节点计算带来了很大的压力。受FM启发，可以把这个大矩阵转换分解为小矩阵和它的转置相乘，表征到低维度连续向量空间，来减少模型复杂度：$W_p^n \odot p = \sum_{i=1}^N \sum_{j=1}^N \theta_i^n \theta_j^n \langle f_i,f_j \rangle = \langle \sum_{i=1}^N \delta_i^n, \sum_{i=1}^N \delta_i^n \rangle $。</p><p>在OPNN中，外积操作带来更多的网络参数，为减少计算量，使得模型更易于学习，采用了多个外积矩阵按元素叠加（element-wise superposition）的技巧来减少复杂度，具体如下：$p=\sum_{i=1}^N \sum_{j=1}^N f_i f_j^T=f_{\Sigma}(f_{\Sigma})^T, f_{\Sigma}=\sum_{j=1}^N f_i$。</p><h3 id="8-DeepFM"><a href="#8-DeepFM" class="headerlink" title="8. DeepFM"></a>8. DeepFM</h3><p>深度神经网络对于学习复杂的特征关系非常有潜力。目前也有很多基于CNN与RNN的用于CTR预估的模型。但是基于CNN的模型比较偏向于相邻的特征组合关系提取，基于RNN的模型更适合有序列依赖的点击数据。</p><p>FNN模型首先预训练FM，再将训练好的FM应用到DNN中。PNN网络的embedding层与全连接层之间加了一层Product Layer来完成特征组合。PNN和FNN与其他已有的深度学习模型类似，都很难有效地提取出低阶特征组合。WDL模型混合了宽度模型与深度模型，但是宽度模型的输入依旧依赖于特征工程。</p><p>上述模型要不然偏向于低阶特征或者高阶特征的提取，要不然依赖于特征工程。而DeepFM模型可以以端对端的方式来学习不同阶的组合特征关系，并且不需要其他特征工程。</p><p>DeepFM的结构中包含了因子分解机部分以及深度神经网络部分，分别负责低阶特征的提取和高阶特征的提取。其结构如下：</p><p><img src="/ctr-models/deepfm.jpg" alt="deepfm"><br>上图中红色箭头所表示的链接权重恒定为1（weight-1 connection），在训练过程中不更新，可以认为是把节点的值直接拷贝到后一层，再参与后一层节点的运算操作。</p><p>与Wide&amp;Deep Model不同，DeepFM共享相同的输入与embedding向量。在Wide&amp;Deep Model中，因为在Wide部分包含了人工设计的成对特征组，所以输入向量的长度也会显著增加，这也增加了复杂性。</p><p>DeepFM包含两部分：神经网络部分与因子分解机部分。这两部分共享同样的输入。对于给定特征$i$，向量$w_i$用于表征一阶特征的重要性，隐变量$V_i$用于表示该特征与其他特征的相互影响。在FM部分，$V_i$用于表征二阶特征，同时在神经网络部分用于构建高阶特征。所有的参数共同参与训练。DeepFM的预测结果可以写为 <script type="math/tex">\hat{y}=sigmoid(y_{FM}+y_{DNN})</script><br>其中$\hat{y}∈(0,1)$是预测的点击率，$y_{FM}$与$y_{DNN}$分是FM部分与DNN部分。</p><p>FM部分的详细结构如下：<br><img src="/ctr-models/fm.png" alt="fm"></p><p>FM的输出如下公式：</p><script type="math/tex; mode=display">y_{FM}=\langle w,x \rangle + \sum_{i=1}^d \sum_{j=i+1}^d \langle V_i,V_j \rangle x_i x_j</script><p>其中$w∈R^d,V_i∈R^k$ 。加法部分反映了一阶特征的重要性，而内积部分反应了二阶特征的影响。</p><p>深度部分详细如下：<br><img src="/ctr-models/dnn.png" alt="dnn"><br>深度部分是一个前馈神经网络。与图像或者语音这类输入不同，图像语音的输入一般是连续而且密集的，然而用于CTR的输入一般是及其稀疏的。因此需要设计特定的网络结构，具体实现为，在第一层隐含层之前，引入一个嵌入层来完成将输入向量压缩到低维稠密向量。</p><script type="math/tex; mode=display">y_{DNN}=\sigma(W^{H+1} \cdot a^H + b^{H+1})</script><p>其中H是隐层的层数。</p><h3 id="9-DIN"><a href="#9-DIN" class="headerlink" title="9. DIN"></a>9. DIN</h3><p>DIN是阿里17年的论文中提出的深度学习模型，该模型基于对用户历史行为数据的两个观察：1、多样性，一个用户可能对多种品类的东西感兴趣；2、部分对应，只有一部分的历史数据对目前的点击预测有帮助，比如系统向用户推荐泳镜时会与用户点击过的泳衣产生关联，但是跟用户买的书就关系不大。于是，DIN设计了一个attention结构，对用户的历史数据和待估算的广告之间部分匹配，从而得到一个权重值，用来进行embedding间的加权求和。<br><img src="/ctr-models/DIN.png" alt="din"></p><p>DIN模型的输入分为2个部分：用户特征和广告(商品)特征。用户特征由用户历史行为的不同实体ID序列组成。在对用户的表示计算上引入了attention network (也即图中的Activation Unit) 。DIN把用户特征、用户历史行为特征进行embedding操作，视为对用户兴趣的表示，之后通过attention network，对每个兴趣表示赋予不同的权值。这个权值是由用户的兴趣和待估算的广告进行匹配计算得到的，如此模型结构符合了之前的两个观察：用户兴趣的多峰分布以及部分对应。Attention network 的计算公式如下，</p><script type="math/tex; mode=display">V_u=f(V_a)=\sum_{i=1}^N w_i \cdot V_i =\sum_{i=1}^N g(V_i,V_a) \cdot V_i</script><p>其中，$V_u$ 代表用户表示向量， $V_i$是用户行为$i$的embedding向量， $V_a$ 代表广告的表示向量。核心在于用户的表示向量不仅仅取决于用户的历史行为，而且还与待评估的广告有直接的关联。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>主流的CTR预估模型已经从传统的宽度模型向深度模型转变，与之相应的人工特征工程的工作量也逐渐减少。上文提到的深度学习模型，除了DIN对输入数据的处理比较特殊之外，其他几个模型还是比较类似的，它们之间的区别主要在于网络结构的不同，如下图所示: <img src="/ctr-models/dnn-models.png" alt></p><p>这四种深度学习模型的比较见下表：<img src="/ctr-models/comparison.png" alt></p><p>综上，深度学习技术主要有三点优势。第一点，<strong>模型设计组件化</strong>。组件化是指在构建模型时，可以更多的关注idea和motivation本身，在真正数学化实现时可以像搭积木一样进行网络结构的设计和搭建。第二点，<strong>优化方法标准化</strong>。在2010年以前，Machine Learning还是一个要求较高的领域。它要求不仅了解问题、能定义出数学化的formulation，而且需要掌握很好的优化技巧，针对对应的问题设计具体的优化方法。但是在现在，深度学习因为模型结构上的变化，使得工业界可以用标准的SGD或SGD变种，很轻松的得到很好的优化解。第三点，<strong>深度学习可以帮助我们实现设计与优化的解耦，将设计和优化分阶段进行</strong>。对于工业界的同学来说，可以更加关注从问题本身出发，抽象和拟合领域知识。然后用一些标准的优化方法和框架来进行求解。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[He X, Pan J, Jin O, et al, 2014] Practical lessons from predicting clicks on adsat facebook. ACM SIGKDD.<br>[Rendle, 2010] Factorization machines. In ICDM.<br>[Gai et al] Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction<br>[Cheng et al., 2016] Wide &amp; deep learning for recommender systems. CoRR.<br>[W. Zhang, et al, 2016] Deep learning over multi-field categorical data: A case study on user response prediction, ECIR.<br>[Yanru Qu et al, 2016] Product-based Neural Networks for User Response Prediction.<br>[Huifeng Guo et al, 2017] DeepFM: A Factorization-Machine based Neural Network for CTR Prediction.<br>[Guorui Zhou et al, 2017] Deep Interest Network for Click-Through Rate Prediction.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;学习和预测用户的反馈对于个性化推荐、信息检索和在线广告等领域都有着极其重要的作用。在这些领域，用户的反馈行为包括点击、收藏、购买等。本文以点击率（CTR）预估为例，介绍常用的CTR预估模型，试图找出它们之间的关联和演化规律。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="CTR预估" scheme="http://xudongyang.coding.me/tags/CTR%E9%A2%84%E4%BC%B0/"/>
    
  </entry>
  
  <entry>
    <title>使用crontab调度hadoop任务和机器学习任务的正确姿势</title>
    <link href="http://xudongyang.coding.me/schedule-by-crontab/"/>
    <id>http://xudongyang.coding.me/schedule-by-crontab/</id>
    <published>2017-10-28T07:07:30.000Z</published>
    <updated>2019-04-02T02:43:36.858Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/schedule-by-crontab/flow.jpg" alt="icon"></p><p>虽然现在越来越多的开源机器学习工具支持分布式训练，但分布式机器学习平台的搭建和运维的门槛通常是比较高的。另外一方面，有一些业务场景的训练数据其实并不是很大，在一台普通的开发机上训练个把小时足矣。单机的机器学习工具使用起来通常要比分布式平台好用很多。</p><p>特征工程在机器学习任务中占了很大的一部分比重，使用hive sql这样的高级语言处理起来比较方面和快捷。因此，通常特征工程、样本构建都在离线分布式集群（hive集群）上完成，训练任务在数据量不大时可以在gateway机器上完成。这就涉及到几个问题：</p><ol><li>gateway机器上的daily训练任务什么时候开始执行？</li><li>模型训练结束，并对新数据做出预测后如何把数据上传到分布式集群？</li><li>如何通知后置任务开始执行？<a id="more"></a>对于第一个问题，理想的解决方案是公司大数据平台的调度系统能够调度某台具体gateway上部署的任务，并且可以获取任务执行的状态，在任务执行成功后可以自动调度后置任务。然而，有时候调度系统还没有这么智能的时候，就需要我们自己想办法解决了。Crontab是Unix和类Unix的操作系统中用于设置周期性被执行的指令的工具。使用Crontab可以每天定时启动任务。美中不足在于必须要自己检查前置任务是否已经结束，也就是说我们要的数据有没有产出，同时还要有一个让后置任务等待当前任务结束的机制。</li></ol><h2 id="检查前置任务是否已经结束"><a href="#检查前置任务是否已经结束" class="headerlink" title="检查前置任务是否已经结束"></a>检查前置任务是否已经结束</h2><p>如果前置任务是hive任务，那么结束标志通常是一个hive表产生了特定分区，我们只需要检查这个分区是否存在就可以了。有个问题需要注意的是，可能在hive任务执行过程中分区已经产生，但任务没有完全结束前数据还没有写完，这个时候启动后续任务是不正确。解决办法就是在任务结束时为当前表添加一个空的“标志分区”，比如原来的分区是“pt=20170921”，我们可以添加一个空的分区“pt=20170921.done”（分区字段的类型为string时可用），或者“pt=-20170921”（分区字段的类型为int时可用）。然后，crontab调度的后置任务需要检查这个“标志分区”是否存在。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> log_info()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$LOG_LEVEL</span>"</span> != <span class="string">"WARN"</span> ] &amp;&amp; [ <span class="string">"<span class="variable">$LOG_LEVEL</span>"</span> != <span class="string">"ERROR"</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"`date +"</span>%Y-%m-%d %H:%M:%S<span class="string">"` [INFO] ($$)(<span class="variable">$USER</span>): $*"</span>;</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">check_partition</span></span>() &#123;</span><br><span class="line">   log_info <span class="string">"function [<span class="variable">$FUNCNAME</span>] begin"</span></span><br><span class="line">   <span class="comment">#table,dt</span></span><br><span class="line">   temp=`hive -e <span class="string">"show partitions <span class="variable">$1</span>"</span>`</span><br><span class="line">   <span class="built_in">echo</span> <span class="variable">$temp</span>|grep -wq <span class="string">"<span class="variable">$2</span>"</span></span><br><span class="line">   <span class="keyword">if</span> [ $? -eq 0 ];<span class="keyword">then</span></span><br><span class="line">       log_info <span class="string">"<span class="variable">$1</span> parition <span class="variable">$2</span> exists, ok"</span></span><br><span class="line">       <span class="built_in">return</span> 0</span><br><span class="line">   <span class="keyword">else</span></span><br><span class="line">       log_info <span class="string">"<span class="variable">$1</span> parition <span class="variable">$2</span> doesn't exists"</span></span><br><span class="line">       <span class="built_in">return</span> 1</span><br><span class="line">   <span class="keyword">fi</span>  </span><br><span class="line">   log_info <span class="string">"function [<span class="variable">$FUNCNAME</span>] end"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果前置任务是MapReduce或者Spark任务，那么结束标志通常是在HDFS上产出了一个特定的路径，后置任务只需要检查这个特定路径是否存在就可以。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 功能: 检查给定的文件或目录在hadoop上是否存在</span></span><br><span class="line"><span class="comment">## $1 文件或者目录, 不支持*号通配符</span></span><br><span class="line"><span class="comment">## $? return 0 if file exist, none-0 otherwise</span></span><br><span class="line"><span class="keyword">function</span> hadoop_check_file_exist()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">## check params</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="variable">$#</span> -ne 1 ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        log_info <span class="string">"Unexpected params for hadoop_check_file_exist() function! Usage: hadoop_check_file_exist &lt;dir_or_file&gt;"</span>;</span><br><span class="line">        <span class="built_in">return</span> 1;</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">## do it</span></span><br><span class="line">    log_info <span class="string">"<span class="variable">$&#123;HADOOP_EXEC&#125;</span> --config <span class="variable">$&#123;HADOOP_CONF&#125;</span> fs -test -e <span class="variable">$1</span>"</span></span><br><span class="line">    <span class="variable">$&#123;HADOOP_EXEC&#125;</span> --config <span class="variable">$&#123;HADOOP_CONF&#125;</span> fs -<span class="built_in">test</span> -e <span class="string">"<span class="variable">$1</span>"</span></span><br><span class="line">    <span class="built_in">local</span> ret=$?</span><br><span class="line">    <span class="keyword">if</span> [ <span class="variable">$ret</span> -eq 0 ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        log_info <span class="string">"<span class="variable">$1</span> does exist on Hadoop"</span></span><br><span class="line">        <span class="built_in">return</span> 0;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        log_info <span class="string">"(<span class="variable">$ret</span>)<span class="variable">$1</span> does NOT exist on Hadoop"</span></span><br><span class="line">        <span class="built_in">return</span> 2;</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="built_in">return</span> 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>其实，hive任务的表的内容也是存储在HDFS上，因此也可以用检查HDFS路径的方法，来判断前置hive任务是否已经结束。可以用下面命令查看hive表对应的HDFS路径。<br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hive</span> -e <span class="string">"desc formatted <span class="variable">$tablename</span>;"</span></span><br></pre></td></tr></table></figure></p><h2 id="循环等待前置任务结束"><a href="#循环等待前置任务结束" class="headerlink" title="循环等待前置任务结束"></a>循环等待前置任务结束</h2><p>当前置任务还没有结束时，需要循环等待。有两种方法，一种是自己在Bash脚本里写代码，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hadoop_check_file_exist <span class="string">"<span class="variable">$hbase_dir</span>/<span class="variable">$table_name</span>/pt=-<span class="variable">$bizdate</span>"</span></span><br><span class="line"><span class="keyword">while</span> [ $? -ne 0 ] </span><br><span class="line"><span class="keyword">do</span>  </span><br><span class="line">  <span class="built_in">local</span> hh=`date <span class="string">'+%H'</span>`</span><br><span class="line">  <span class="keyword">if</span> [ <span class="variable">$hh</span> -gt 23 ]</span><br><span class="line">  <span class="keyword">then</span></span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"timeout, partition still not exist"</span></span><br><span class="line">      <span class="built_in">exit</span> 1</span><br><span class="line">  <span class="keyword">fi</span>  </span><br><span class="line">  log_info <span class="string">"<span class="variable">$hbase_dir</span>/<span class="variable">$table_name</span>/pt=-<span class="variable">$bizdate</span> doesn't exist, wait for a while"</span></span><br><span class="line">  sleep 5m</span><br><span class="line">  hadoop_check_file_exist <span class="string">"<span class="variable">$hbase_dir</span>/<span class="variable">$table_name</span>/pt=-<span class="variable">$bizdate</span>"</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></p><p>第二种方法，是利用crontab的周期性调度功能。比如可以让crontab每隔5分钟调度一次任务。这个时候需要注意的是，可能前一次调度的进程还没有执行结束，后一次调度就已经开始。这个时候可以使用linux flock文件锁实现任务锁定，解决冲突。<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">flock [<span class="string">-sxon</span>][<span class="symbol">-w #</span>] file [-c] command</span><br><span class="line">-s, --shared:    获得一个共享锁</span><br><span class="line">-x, --exclusive: 获得一个独占锁</span><br><span class="line">-u, --unlock:    移除一个锁，通常是不需要的，脚本执行完会自动丢弃锁</span><br><span class="line">-n, --nonblock:  如果没有立即获得锁，直接失败而不是等待</span><br><span class="line">-w, --timeout:   如果没有立即获得锁，等待指定时间</span><br><span class="line">-o, --close:     在运行命令前关闭文件的描述符号。用于如果命令产生子进程时会不受锁的管控</span><br><span class="line">-c, --command:   在shell中运行一个单独的命令</span><br><span class="line">-h, --help       显示帮助</span><br><span class="line">-V, --version:   显示版本</span><br></pre></td></tr></table></figure></p><p>其中，file是一个空文件即可。比如，crontab文件可以这样写：<br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*<span class="regexp">/5 6-23 * * * flock -xn /</span>tmp<span class="regexp">/pop_score.lock -c 'bash /</span>home<span class="regexp">/xudong.yang/</span>pop_score<span class="regexp">/train/m</span>ain.sh -T -p -c &gt;<span class="regexp">/dev/</span><span class="keyword">null</span> <span class="number">2</span>&gt;&amp;<span class="number">1</span><span class="string">'</span></span><br></pre></td></tr></table></figure></p><p>如果使用这种方法，脚本里面检查前置任务没有结束时就直接退出当前进程即可，像下面这样：<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop_check_file_exist <span class="string">"$hbase_dir/$table_name/pt=-$bizdate"</span></span><br><span class="line"><span class="keyword">if</span> [ $? -ne <span class="number">0</span> ]; then</span><br><span class="line">  log_info <span class="string">"$hbase_dir/$table_name/pt=-$bizdate doesn't exist, wait for a while"</span></span><br><span class="line">  <span class="keyword">exit</span> <span class="number">1</span></span><br><span class="line">fi</span><br></pre></td></tr></table></figure></p><p>虽然文件锁能解决crontab调度冲突的问题，但是我们只希望脚本被成功执行一次，任务结束之后，后续的调度直接退出。还有一种情况需要考虑，有可能crontab调度的任务的正在运行，这个时候我们自己也手动启动了同样的任务，如何避免这样的冲突呢？</p><p>无非就是要有个标记任务已经成功运行或者正在运行标识能够在脚本里读取，如何做到呢？对就是在指定目录下建立特定名称的空文件。在脚本开始的时候坚持标记文件是否存在，存在就直接退出。在任务正常运行结束的时候touch成功执行的标记。结构大概如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 变量定义等</span></span><br><span class="line">......</span><br><span class="line">[ -f <span class="variable">$data_home</span>/<span class="variable">$bizdate</span>/DONE ] &amp;&amp; &#123; log_info <span class="string">"DONE file exists, exit"</span> &gt;&gt; <span class="variable">$log_file_path</span>; <span class="built_in">exit</span> 0; &#125;</span><br><span class="line">[ -f <span class="variable">$data_home</span>/<span class="variable">$bizdate</span>/RUNNING ] &amp;&amp; &#123; log_info <span class="string">"RUNNING file exists, exit"</span> &gt;&gt; <span class="variable">$log_file_path</span>; <span class="built_in">exit</span> 0; &#125;</span><br><span class="line"></span><br><span class="line">touch <span class="variable">$data_home</span>/<span class="variable">$bizdate</span>/RUNNING</span><br><span class="line"><span class="built_in">trap</span> <span class="string">"rm -f <span class="variable">$data_home</span>/<span class="variable">$bizdate</span>/RUNNING; echo Bye."</span> EXIT QUIT ABRT INT HUP TERM</span><br><span class="line"><span class="comment"># do something here</span></span><br><span class="line">......</span><br><span class="line"><span class="keyword">if</span> [ -f <span class="variable">$data_home</span>/<span class="variable">$bizdate</span>/RUNNING ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    mv <span class="variable">$data_home</span>/<span class="variable">$bizdate</span>/RUNNING <span class="variable">$data_home</span>/<span class="variable">$bizdate</span>/DONE</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    touch <span class="variable">$data_home</span>/<span class="variable">$bizdate</span>/DONE</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="built_in">exit</span> 0;</span><br></pre></td></tr></table></figure></p><p>有了RUNNING标记就不怕手动执行任务时和crontab调度冲突了；有了DONE标记就不怕每天的任务被调度多次了。</p><h2 id="从分布式集群下载数据"><a href="#从分布式集群下载数据" class="headerlink" title="从分布式集群下载数据"></a>从分布式集群下载数据</h2><p>从hdfs下载数据的函数：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 功能: 将hadoop上的文件下载到本地并merge到一个文件中</span></span><br><span class="line"><span class="comment">## $1 hadoop叶子目录 或 文件名--支持通配符 (*)</span></span><br><span class="line"><span class="comment">## $2 本地文件名</span></span><br><span class="line"><span class="comment">## $? return 0 if success, none-0 otherwise</span></span><br><span class="line"><span class="keyword">function</span> hadoop_getmerge()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">## check params</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="variable">$#</span> -ne 2 ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        log_info <span class="string">"Unexpected params for hadoop_getmerge() function! Usage: hadoop_getmerge &lt;hadoop_file&gt; &lt;local_file&gt;"</span>;</span><br><span class="line">        <span class="built_in">return</span> 1;</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="variable">$2</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        log_info <span class="string">"Can not do hadoop_getmerge because local file <span class="variable">$2</span> already exists!"</span></span><br><span class="line">        <span class="built_in">return</span> 2;</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">## do it</span></span><br><span class="line">    <span class="variable">$&#123;HADOOP_EXEC&#125;</span> --config <span class="variable">$&#123;HADOOP_CONF&#125;</span> fs -getmerge <span class="variable">$1</span> <span class="variable">$2</span>;</span><br><span class="line">    <span class="keyword">if</span> [ $? -ne 0 ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        log_info <span class="string">"Do hadoop_getmerge FAILED! Source: <span class="variable">$1</span>, target: <span class="variable">$2</span>"</span>;</span><br><span class="line">        <span class="built_in">return</span> 3;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        log_info <span class="string">"Do hadoop_getmerge OK! Source: <span class="variable">$1</span>, target: <span class="variable">$2</span>"</span>;</span><br><span class="line">        <span class="built_in">return</span> 0;</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>HIVE表里的数据也可以先找到对应的HDFS目录，然后用上面的函数下载数据，唯一需要注意的是，HIVE表必须stored as textfile，否则下载下来的数据不可用。<br>万一hive表不是已文本文件的格式存储的怎么办呢？不要紧，还是有办法的，如下：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p <span class="variable">$data_home</span>/<span class="variable">$bizdate</span>/raw</span><br><span class="line">declare <span class="attribute">sql</span>=<span class="string">"</span></span><br><span class="line"><span class="string">  set hive.support.quoted.identifiers=None;</span></span><br><span class="line"><span class="string">  insert overwrite local directory '<span class="variable">$data_home</span>/<span class="variable">$bizdate</span>/raw'</span></span><br><span class="line"><span class="string">  row format delimited fields terminated by '\t'</span></span><br><span class="line"><span class="string">  select \`(pt)?+.+\` from <span class="variable">$table_name</span> where pt=<span class="variable">$bizdate</span>; </span></span><br><span class="line"><span class="string">"</span></span><br><span class="line">log_info <span class="variable">$sql</span></span><br><span class="line"><span class="variable">$hive</span> -e <span class="string">"<span class="variable">$sql</span>"</span></span><br></pre></td></tr></table></figure></p><h2 id="上传数据到分布式集群"><a href="#上传数据到分布式集群" class="headerlink" title="上传数据到分布式集群"></a>上传数据到分布式集群</h2><p>模型训练和预测之后，必须把预测数据上传到分布式集群，以便后续处理。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">local create_table_sql="</span><br><span class="line">  <span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> $target_table_name (</span><br><span class="line">      ......</span><br><span class="line">  )</span><br><span class="line">  partitioned <span class="keyword">by</span> (pt <span class="built_in">int</span>)</span><br><span class="line">  <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> </span><br><span class="line">  <span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span> </span><br><span class="line">  <span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br><span class="line">"</span><br><span class="line">log_info $create_table_sql</span><br><span class="line">$hive -e "$create_table_sql"</span><br><span class="line"></span><br><span class="line">local upload_sql="<span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'$data_home/$bizdate/$predict_file'</span> <span class="keyword">into</span> <span class="keyword">table</span> $target_table_name <span class="keyword">partition</span>(pt=$&#123;bizdate&#125;);"</span><br><span class="line">log_info $upload_sql</span><br><span class="line">$hive -e "$upload_sql"</span><br></pre></td></tr></table></figure></p><h2 id="通知后置任务开始执行"><a href="#通知后置任务开始执行" class="headerlink" title="通知后置任务开始执行"></a>通知后置任务开始执行</h2><p>其实crontab没法通知后置任务当前任务已经结束，那怎么办呢？</p><p>把真正的后置任务加一个前置依赖任务，而这个依赖任务是部署在调度系统上的一个shell任务，该任务的前置任务是crontab调度任务的前置任务，并且这个任务做的唯一一件事情就是循环检查crontab调度任务的数据有没有产出，已经产出就结束，没有产出就sleep一小段时间之后再继续检查。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">check_partition <span class="variable">$table_name</span> <span class="variable">$bizdate</span></span><br><span class="line"><span class="keyword">while</span> [ $? -ne 0 ] </span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  sleep 5m</span><br><span class="line">  hh=`date <span class="string">'+%H'</span>`</span><br><span class="line">  <span class="keyword">if</span> [ <span class="variable">$hh</span> -gt 23 ]</span><br><span class="line">  <span class="keyword">then</span></span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"timeout, partition still not exist"</span></span><br><span class="line">      <span class="built_in">exit</span> 1</span><br><span class="line">  <span class="keyword">fi</span>  </span><br><span class="line">  check_partition <span class="variable">$table_name</span> <span class="variable">$bizdate</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h2 id="那些年，我们踩过的坑"><a href="#那些年，我们踩过的坑" class="headerlink" title="那些年，我们踩过的坑"></a>那些年，我们踩过的坑</h2><p>一、crontab调度任务不会自动export环境变量</p><p>开始的时候，手动执行脚本正常运行，但是crontab调度每次都会在<code>hadoop fs -test -e $path</code>命令执行出错，表现为明明<code>$path</code>已经存在，但指令总是返回1，而不是0 。经过苦苦排查之后才发现，hadoop依赖的环境变量JAVA_HOME和HADOOP_HOME没有在脚本里导入。而用户在终端里登录到服务器上时，这2个环境变量是自动导入的。所以，务必记得在脚本开始的时候就导入环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=...</span><br></pre></td></tr></table></figure><p>二、crontab调度任务不能写太多标准输出，否则任务会在某个时刻自动中断</p><p>这个也挺坑的，务必记得在crontab的指令里重定向标准输出和标准错误到一个文件里，或者重定向到unix的黑洞<code>/dev/null</code>里。</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*<span class="regexp">/5 6-23 * * * flock -xn /</span>tmp<span class="regexp">/pop_score.lock -c 'bash /</span>home<span class="regexp">/xudong.yang/</span>pop_score<span class="regexp">/train/m</span>ain.sh -T -p -c &gt;<span class="regexp">/dev/</span><span class="keyword">null</span> <span class="number">2</span>&gt;&amp;<span class="number">1</span><span class="string">'</span></span><br></pre></td></tr></table></figure><p>这里推荐在脚本使用tee命令同时输出日志到终端和文件，这样用户手动执行的时候可以直接在终端里看到程序的执行情况，crontab调度里可以查看日志文件来排查问题。当然，输出到终端的部分，在使用crontab调度时需要重定向到黑洞里。<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">main | tee -a <span class="variable">$log_file_path</span> <span class="number">2</span>&gt;&amp;<span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$&#123;PIPESTATUS[0]&#125;</span> -ne <span class="number">0</span> ]</span><br><span class="line">then</span><br><span class="line">    log_error <span class="string">"run failed."</span></span><br><span class="line">    <span class="keyword">exit</span> <span class="number">1</span>;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/schedule-by-crontab/flow.jpg&quot; alt=&quot;icon&quot;&gt;&lt;/p&gt;
&lt;p&gt;虽然现在越来越多的开源机器学习工具支持分布式训练，但分布式机器学习平台的搭建和运维的门槛通常是比较高的。另外一方面，有一些业务场景的训练数据其实并不是很大，在一台普通的开发机上训练个把小时足矣。单机的机器学习工具使用起来通常要比分布式平台好用很多。&lt;/p&gt;
&lt;p&gt;特征工程在机器学习任务中占了很大的一部分比重，使用hive sql这样的高级语言处理起来比较方面和快捷。因此，通常特征工程、样本构建都在离线分布式集群（hive集群）上完成，训练任务在数据量不大时可以在gateway机器上完成。这就涉及到几个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;gateway机器上的daily训练任务什么时候开始执行？&lt;/li&gt;
&lt;li&gt;模型训练结束，并对新数据做出预测后如何把数据上传到分布式集群？&lt;/li&gt;
&lt;li&gt;如何通知后置任务开始执行？&lt;/li&gt;&lt;/ol&gt;
    
    </summary>
    
      <category term="程序设计" scheme="http://xudongyang.coding.me/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
      <category term="shell" scheme="http://xudongyang.coding.me/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/shell/"/>
    
    
      <category term="调度" scheme="http://xudongyang.coding.me/tags/%E8%B0%83%E5%BA%A6/"/>
    
      <category term="hadoop" scheme="http://xudongyang.coding.me/tags/hadoop/"/>
    
      <category term="hive" scheme="http://xudongyang.coding.me/tags/hive/"/>
    
      <category term="crontab" scheme="http://xudongyang.coding.me/tags/crontab/"/>
    
  </entry>
  
  <entry>
    <title>电商平台商家流量分配机制</title>
    <link href="http://xudongyang.coding.me/traffic-control/"/>
    <id>http://xudongyang.coding.me/traffic-control/</id>
    <published>2017-08-21T10:04:15.000Z</published>
    <updated>2019-04-02T02:43:36.874Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/traffic-control/traffic-control.jpg" alt="icon"></p><p>本文把电商平台搜索场景的流量分配问题定义为在用户$u$提交查询关键词$q$后，搜索结果列表的第$i$个位置展现哪个商家的哪个商品，以便达到某种商业目标。由此可见，流量跟3个因素有关：用户、查询关键词、搜索结果的展现位置。只要这3个因素其中有一个发生变化，我们就认为产生了一个新的不同的流量。按照这个定义，一般情况下，电商平台的每日流量都在亿或数十亿的数量级。</p><p>电商平台为了达到其商业目标，如最大化平台的总成交额（GMV），需要把每个流量和某个具体的商品联系起来。在搜索场景下，表现为系统决定在用户$u$提交查询关键词$q$后搜索列表的各个位置展现哪些商品的过程。这一给流量和商品建立关联的过程就称为流量分配。</p><p>在商业目标确定后，流量分配就需要满足一定的约束条件，不能随意而为之。比如，在搜索场景下，考虑到用户体验的问题，流量分配需要能够匹配用户此时此刻的查询意图，即需要满足相关性、个性化、场景化等约束；同时，平台为了更好地服务商家以及追求自身的利益，需要在流量分配时考虑最大化总成交额。</p><p>然而，平台的多个商业目标之间，并不是毫无冲突的，比如在最求自身利益最大化的同时并不能很好的服务所有的商家。表现为平台最大化GMV的贪心策略会导致流量分配的马太效应越来越明显，即流量越来越集中到少数的头部商家；中长尾的商家获得的流量少，成交量也少，从而失去参与感。导致这一现象的原因是贪心的流量分配机制严重依赖平台上的历史行为数据。系统倾向于展现过去一段时间转化率高且销量好商品；而对于新上架的商品，或者小众而精美的商品（比如手工艺品等）由于历史成交数据有限，系统未能充分探索并感知其被展现的效用值，因此这类商品得到的展现机会就较少。在机器学习领域，这一问题被称为探索/利用（Exploration and Exploitation）两难问题。</p><p>构建平台、用户和商家共赢的生态结构是电商平台的长期目标。采用贪心策略的流量分配机制长期发展下去，不利于平台生态的长远发展。为了缓解马太效应，需要在尽量不影响平台GMV的条件下，有意识的控制头部商家的流量，把更多的流量引导到中长尾商家。这就是本文主要解决的问题。<br><a id="more"></a></p><h2 id="问题建模"><a href="#问题建模" class="headerlink" title="问题建模"></a>问题建模</h2><p>为了使流量分配尽可能趋近平均，我们在目标函数里添加了最大熵正则项（maximum entropy regularizer）。目标是在使得平台整体流量利用率和转化率最大化的基础上，给予中长尾商家一定的流量激励和支持。数学表达如下：</p><script type="math/tex; mode=display">\begin{eqnarray}\max\limits_x \quad &\sum\limits_{i,j} C_{i,j} x_{i,j} - \lambda \sum\limits_{i,j} x_{i,j} \ln x_{i,j}\nonumber\\\mbox{s.t.} \quad & 0 \leq x_{i,j} \leq 1 \\ & \sum\limits_j x_{i,j} = 1 \\& \sum\limits_i I\left(argmax_j (x_{i,j}) = 1 \right) \leq B_1 \\&\vdots \nonumber\\& \sum\limits_i I\left(argmax_j (x_{i,j}) = J \right) \leq B_J \nonumber \end{eqnarray}</script><p>其中，$x_{i,j}$ 为第 $i$ 个流量分配给第 $j$ 个商家的概率；$C_{i,j}$为流量$i$分配给商家$j$时获得的收益，定义为“点击率 <em> 转化率 </em> 客单价”；信息熵（$-\sum_{i,j} x_{i,j} \ln x_{i,j}$）越大，表示流量分配的不确定性越大，即流量需要更加平均的分配；$\lambda$是超参数，用来控制正则化的强弱程度；$B_j$为第$j$个商家本时段的目标流量；$I(\cdot)$为指示函数，当参数为真时值为1，否则值为0。</p><p>由于argmax函数和指示函数都不是连续可微函数，给问题求解带来不便。这里把$K$个不等式约束简化为独立的线性表达形式，如下所示：</p><script type="math/tex; mode=display">\begin{eqnarray}\max\limits_x \quad &\sum\limits_{i,j} C_{i,j} x_{i,j} - \lambda \sum\limits_{i,j} x_{i,j} \ln x_{i,j}\nonumber\\\mbox{s.t.} \quad & 0 \leq x_{i,j} \leq 1 \\ & \sum\limits_j x_{i,j} = 1 \\& \forall j,  \sum\limits_{i} x_{i,j} \leq I_j \end{eqnarray}</script><p>其中，$I_j = f(B_j)$为商家$j$的流量总量软目标，可能的形式如：$I_j = w_j \cdot B_j$，$w_j$为一个缩放系数。在流量分配求解过程中，$I_j$作为一个已知的常数，需要预先通过其他手段预估出来。</p><p>由于流量的数量级非常大，直接求解$x_{i,j}$是非常困难的；同时流量是实时进入的，系统需要在线快速求解。因此，我们通过把原始问题转化为拉格朗日对偶问题来求解，即把庞大的参数空间{$x_{i,j}$}转化为较小的参数空间{$\alpha_j$}，从而简化问题。</p><p>该问题的拉格朗日（lagrange）形式如下：</p><script type="math/tex; mode=display">\begin{eqnarray}L(x,\alpha,\beta) = \sum\limits_{i,j} C_{i,j} x_{i,j} - \lambda \sum\limits_{i,j} x_{i,j} \ln x_{i,j} + \sum_j \alpha_j \left(I_j - \sum_{i} x_{i,j}\right) + \sum_i\beta_i\left(1-\sum_j x_{i,j}\right)\end{eqnarray}</script><p>根据 KKT 条件，最优时 $\nabla_x L(x,\alpha,\beta) = 0$ 和 $1-\sum_j x_{i,j} = 0$ 推得：</p><script type="math/tex; mode=display">\begin{eqnarray} \beta_i = \lambda \left( \ln Z_i - 1\right) \nonumber \\x_{i,j} = \frac{1}{Z_i}\exp \left( \frac{1}{\lambda} \left[C_{i,j} -  \alpha_j \right]\right) \nonumber\\Z_i=\sum_j \exp \left( \frac{1}{\lambda} \left[C_{i,j} -  \alpha_j \right]\right) \nonumber \end{eqnarray}</script><p>再将 $x$ 带回原式 $L(x,\alpha,\beta)$，原问题可转化为：</p><script type="math/tex; mode=display">\begin{eqnarray} \min\limits_{\substack{\alpha \geq 0}} \lambda \sum_{i} \ln Z_i + \sum_j \alpha_j I_j \end{eqnarray}</script><p>其中，$\alpha \geq 0$是KKT条件的要求。这是个关于 $\alpha$ 的最优化问题，参数空间相对于原始参数$x$的空间要小很多。用梯度下降（gradient descend）算法可以很好的并行化求解，迭代更新如下，其中$\eta$ 为步长：</p><script type="math/tex; mode=display">\begin{eqnarray} &\alpha_j^t = \max\{0, \alpha_j^{t-1} - \eta (I_j - \sum_{i,j} x_{i,j}^{t-1})\}\nonumber \end{eqnarray}</script><p>我们回过头看 $x$ 的求解式子，$\alpha_j$项越大，则$x_{i,j}$值越小；反之$\alpha_j$项越小，则$x_{i,j}$值越大。当所有 $\alpha$ 都为0时，也就是按 $C_{i,j}$ 目标值贪心的方法。我们观察参数求解过程，物理上的含义是：如果分配超过约束 $I_j$，则$\alpha_j$ 增大，下一轮迭代中分配概率$x_{i,j}$就会减少；反之，当分配没有超过约束 $I_j$时，$\alpha_j$减小导致下一轮获得的分配概率$x_{i,j}$增加；当$\alpha_j$ 等于0时，表示分配不再受这个约束影响。所以 $\alpha$ 的物理含义是：受约束影响下，对原始收益 $C_{i,j}$ 的折扣系数 。</p><p>综上所述，根据概率$x_{i,j}$来分配流量具有抑制已获得足够流量的商家进一步获取流量的作用，同时有助于未获得足够流量的商家在竞争中获胜，从而达到流量均衡的作用。</p><h2 id="商家目标流量预估"><a href="#商家目标流量预估" class="headerlink" title="商家目标流量预估"></a>商家目标流量预估</h2><p>那么每个商家应该获取多少流量($I_j$)才算合理呢？我们并不希望新的流量分配机制上线之后，对商家的流量分布造成非常大的变化，总体原则是在更多探索中长尾商家的优质商品的同时不能让头部商家明显感知到流量的急剧震荡。同时，我们也需要评估各个商家的流量承接能力。</p><p>商家目标流量预估也是一个复杂的问题。有几种做法可以考虑，比如：</p><ul><li>统计目标场景下过去一段时间（比如一个月）内每个商家获得的流量移动平均值；</li><li>偏好近期的统计数据，使用权重衰减的方法拟合目标值；</li><li>为了更好的稳定性，去最近一周的流量移动平均值的中位数（median of means）；</li><li>做成基于时间序列的回归模型（如ARMA)；</li><li>考虑商家/商品的历史点利率和转化率；</li></ul><h2 id="在线求解"><a href="#在线求解" class="headerlink" title="在线求解"></a>在线求解</h2><ul><li>离线为每个商家预估出每个时段的流量目标值（比如每个小时有一个独立的目标，这样线上的流量控制更加平滑），并build到商品索引；</li><li>在线记录每个商家每个时段到目前为止获得的流量，记为$b_j=\sum_{i,j} x_{i,j}^{t-1}$，更新$ \alpha_j^t = \max\{0, \alpha_j^{t-1} - \eta (I_j - b_j)\}$，其中$ \alpha_j^0 = 0 $</li><li>为每个召回的商品计算未归一化的分配概率：$x’_{i,j} = exp\left(\frac{1}{\lambda}(C_{i,j} - \alpha_j) \right)$</li><li>在每个相关性档位确定的情况下，按照$x’_{i,j}$降序排列，取Top N个商品展现</li></ul><p>虽然在线求解过程非常简单，但背后的逻辑是有理论支撑的。</p><h2 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h2><ul><li>基尼系数<ul><li>基尼系数原指经济学中用来衡量一个国家或地区贫富差距的指标，数值范围从0到1，数值越大表示收入差距越悬殊，数值越小表示收入越趋向于平均。不了解的同学可以查看这个轻松有趣的1分钟短视频：<a href="https://baike.baidu.com/wikisecond/videoflow?lemmaId=88365&amp;mediaId=mda-gk0tnrs28nnxhc60&amp;secondId=106774" target="_blank" rel="noopener">轻松秒懂基尼系数</a>。</li><li>借用基尼系数的概念，从商品曝光流量维度统计商家获取流量多少的差距水平，记为<strong>曝光基尼系数</strong>。具体计算方法：把所有商家按其获取到的曝光流量由低到高的顺序排队，分为人数相等的$n$组，从第1组到第$i$组商家累计获取曝光流量占全部商家总获取曝光流量的比重为$W_i$，则$G_{expo} = 1 - \frac{1}{n} \left(2 \sum_{i=1}^{n-1} W_i + 1 \right)$。</li><li>借用基尼系数的概念，从商品点击流量维度统计商家获取点击多少的差距水平，记为<strong>点击基尼系数</strong>。具体计算方法：把所有商家按其获取到的点击流量由低到高的顺序排队，分为人数相等的$n$组，从第1组到第$i$组商家累计获取点击流量占全部商家总获取点击流量的比重为$W_i$，则$G_{click} = 1 - \frac{1}{n} \left(2 \sum_{i=1}^{n-1} W_i + 1 \right)$。</li></ul></li><li>商家动销率：（有销量的商家数）/ (有在售商品的商家总数）</li><li>商品动销率：（有销量的商品数）/ (在线销售的商品总数）</li><li>商家曝光比率：（有商品被展现的商家数）/ (有在售商品的商家总数）</li><li>商家点击比率：（有商品被点击的商家数）/ (有在售商品的商家总数）</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/traffic-control/traffic-control.jpg&quot; alt=&quot;icon&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文把电商平台搜索场景的流量分配问题定义为在用户$u$提交查询关键词$q$后，搜索结果列表的第$i$个位置展现哪个商家的哪个商品，以便达到某种商业目标。由此可见，流量跟3个因素有关：用户、查询关键词、搜索结果的展现位置。只要这3个因素其中有一个发生变化，我们就认为产生了一个新的不同的流量。按照这个定义，一般情况下，电商平台的每日流量都在亿或数十亿的数量级。&lt;/p&gt;
&lt;p&gt;电商平台为了达到其商业目标，如最大化平台的总成交额（GMV），需要把每个流量和某个具体的商品联系起来。在搜索场景下，表现为系统决定在用户$u$提交查询关键词$q$后搜索列表的各个位置展现哪些商品的过程。这一给流量和商品建立关联的过程就称为流量分配。&lt;/p&gt;
&lt;p&gt;在商业目标确定后，流量分配就需要满足一定的约束条件，不能随意而为之。比如，在搜索场景下，考虑到用户体验的问题，流量分配需要能够匹配用户此时此刻的查询意图，即需要满足相关性、个性化、场景化等约束；同时，平台为了更好地服务商家以及追求自身的利益，需要在流量分配时考虑最大化总成交额。&lt;/p&gt;
&lt;p&gt;然而，平台的多个商业目标之间，并不是毫无冲突的，比如在最求自身利益最大化的同时并不能很好的服务所有的商家。表现为平台最大化GMV的贪心策略会导致流量分配的马太效应越来越明显，即流量越来越集中到少数的头部商家；中长尾的商家获得的流量少，成交量也少，从而失去参与感。导致这一现象的原因是贪心的流量分配机制严重依赖平台上的历史行为数据。系统倾向于展现过去一段时间转化率高且销量好商品；而对于新上架的商品，或者小众而精美的商品（比如手工艺品等）由于历史成交数据有限，系统未能充分探索并感知其被展现的效用值，因此这类商品得到的展现机会就较少。在机器学习领域，这一问题被称为探索/利用（Exploration and Exploitation）两难问题。&lt;/p&gt;
&lt;p&gt;构建平台、用户和商家共赢的生态结构是电商平台的长期目标。采用贪心策略的流量分配机制长期发展下去，不利于平台生态的长远发展。为了缓解马太效应，需要在尽量不影响平台GMV的条件下，有意识的控制头部商家的流量，把更多的流量引导到中长尾商家。这就是本文主要解决的问题。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="优化方法" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="机制设计" scheme="http://xudongyang.coding.me/tags/%E6%9C%BA%E5%88%B6%E8%AE%BE%E8%AE%A1/"/>
    
      <category term="多目标学习" scheme="http://xudongyang.coding.me/tags/%E5%A4%9A%E7%9B%AE%E6%A0%87%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Optimalization" scheme="http://xudongyang.coding.me/tags/Optimalization/"/>
    
  </entry>
  
  <entry>
    <title>JVM类加载机制剖析</title>
    <link href="http://xudongyang.coding.me/jvm-class-loader/"/>
    <id>http://xudongyang.coding.me/jvm-class-loader/</id>
    <published>2017-07-04T14:35:15.000Z</published>
    <updated>2019-04-02T02:43:36.847Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、何为类加载器"><a href="#一、何为类加载器" class="headerlink" title="一、何为类加载器"></a>一、何为类加载器</h1><p>我们编写的.java文件经过编译器编译之后，生成.class文件，即字节码文件，类加载器就是负责加载字节码文件到JVM中，并将字节码转换成为java.lang.class类的实例，这个实例便是我们编写的类，通过class实例的newInstance方法，便可以得到java类的对象。</p><p>类加载器是类加载过程中的关键角色，他存在于「类加载Class load」过程的「加载」阶段中，在这个阶段，JVM虚拟机完成了三件事情:</p><ol><li>通过一个类的全限定名(包名称+类名称)获取定义此类的二进制字节流（类的权限定名可以映射到文件系统中的文件路径）；</li><li>将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构；</li><li>在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口；<a id="more"></a><h2 id="java-lang-ClassLoader"><a href="#java-lang-ClassLoader" class="headerlink" title="java.lang.ClassLoader"></a>java.lang.ClassLoader</h2></li><li>loadClass(String className): 加载className类，返回java.lang.class类的实例，异常则抛出ClassNotFoundException</li><li>defineClass(String name, byte[] b, int off, int len): 加载字节码，返回class类的实例，异常则抛出NoClassDefFoundError</li></ol><h1 id="二、类加载器的体系结构"><a href="#二、类加载器的体系结构" class="headerlink" title="二、类加载器的体系结构"></a>二、类加载器的体系结构</h1><p><img src="http://img1.tbcdn.cn/L1/461/1/5194b58d3bfaf478db6570afc17e8f9916f702fd.png" alt></p><h2 id="1-启动类加载器「Bootstrap-ClassLoader」"><a href="#1-启动类加载器「Bootstrap-ClassLoader」" class="headerlink" title="1. 启动类加载器「Bootstrap ClassLoader」"></a>1. 启动类加载器「Bootstrap ClassLoader」</h2><p>处于最顶端的类加载器，主要负责JAVA_HOME/jre/lib目录下的核心jar或者由-Xbootclasspath选项指定的jar包的装入工作。深入分析下Launcher的源码，发现Bootstrap ClassLoader其实加载的是System.getProperty(“sun.boot.class.path”)定义下的类包路径。</p><p>查看JVM启动后Bootstrap ClassLoader具体加载了哪些jar：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">URL[] bootUrls = sun.misc.Launcher.getBootstrapClassPath().getURLs();</span><br><span class="line"><span class="keyword">for</span> (URL url : bootUrls) &#123;</span><br><span class="line">    System.out.println(url.toExternalForm());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Bootstrap ClassLoader是由C++编写的并且内嵌于JVM中，该加载器是无法被java程序直接引用的。比如，java.util.ArrayList类处于rt.jar包下，该包是由Bootstrap ClassLoader负责加载，所以下面这段代码打印出来就是null了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ArrayList list = <span class="keyword">new</span> ArrayList();</span><br><span class="line">System.out.println(<span class="string">"list的类加载器为:"</span>+list.getClass().getClassLoader());</span><br></pre></td></tr></table></figure><h2 id="2-扩展类加载器「Extension-ClassLoader」"><a href="#2-扩展类加载器「Extension-ClassLoader」" class="headerlink" title="2. 扩展类加载器「Extension ClassLoader」"></a>2. 扩展类加载器「Extension ClassLoader」</h2><p>扩展类加载器是由sun.misc.Launcher$ExtClassLoader实现，顾名思义这个类加载器主要负责加载JAVA_HOME\lib\ext目录中或者被java.ext.dirs系统变量定义的路径下的所有类库。</p><h2 id="3-应用程序类加载器「App-ClassLoader」"><a href="#3-应用程序类加载器「App-ClassLoader」" class="headerlink" title="3. 应用程序类加载器「App ClassLoader」"></a>3. 应用程序类加载器「App ClassLoader」</h2><p>应用程序类加载器是由sun.misc.Launcher$AppClassLoader实现，通过源码发现，该类加载器负责加载System.getProperty(“java.class.path”)也就是classpath下的类库。该类加载器又可以称为<strong>系统类加载器</strong>，在用户没有明确指定类加载器的情况下，系统默认使用AppClassLoader加载类。</p><h2 id="4-自定义类加载器「Custom-ClassLoader」"><a href="#4-自定义类加载器「Custom-ClassLoader」" class="headerlink" title="4. 自定义类加载器「Custom ClassLoader」"></a>4. 自定义类加载器「Custom ClassLoader」</h2><p>自定义类加载器是提供给用户自定义「加载哪里的类」而产生的，当初虚拟机在定义「通过一个类的全限定名(包名称+类名称)获取定义此类的二进制字节流」并没有把获取方式限定死，提供了灵活的方式给用户使用，被加载的类可以来自于数据库、可以来自本地文件、可以来自云存储介质等等，用户所需要的就是自定义类加载器并且继承ClassLoader,最后重写「findClass」方法，ClassLoader为我们提供了defineClass方法可以方便的加载源码的二进制字节流。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * for example, an application could create a network class loader to</span></span><br><span class="line"><span class="comment"> * download class files from a server.  Sample code might look like:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line">ClassLoader loader = <span class="keyword">new</span> NetworkClassLoader(host,port);</span><br><span class="line">Object main = loader.loadClass(<span class="string">"Main"</span>, <span class="keyword">true</span>).newInstance();</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *The network class loader subclass must define the methods &#123;@link</span></span><br><span class="line"><span class="comment"> * #findClass &lt;tt&gt;findClass&lt;/tt&gt;&#125; and &lt;tt&gt;loadClassData&lt;/tt&gt; to load a class</span></span><br><span class="line"><span class="comment"> * from the network.  Once it has downloaded the bytes that make up the class,</span></span><br><span class="line"><span class="comment"> * it should use the method &#123;@link #defineClass &lt;tt&gt;defineClass&lt;/tt&gt;&#125; to</span></span><br><span class="line"><span class="comment"> * create a class instance.  A sample implementation is:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NetworkClassLoader</span> <span class="keyword">extends</span> <span class="title">ClassLoader</span> </span>&#123;</span><br><span class="line"> String host;</span><br><span class="line"> <span class="keyword">int</span> port;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">public</span> Class <span class="title">findClass</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">byte</span>[] b = loadClassData(name);</span><br><span class="line">    <span class="keyword">return</span> defineClass(name, b, <span class="number">0</span>, b.length);</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">byte</span>[] loadClassData(String name) &#123;</span><br><span class="line">    <span class="comment">// load the class data from the connection</span></span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="http://upload-images.jianshu.io/upload_images/3901673-decfb670e9357fef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h1 id="三、双亲委派模型"><a href="#三、双亲委派模型" class="headerlink" title="三、双亲委派模型"></a>三、双亲委派模型</h1><p>在同一个JVM中，一个类加载器和一个类的全限定名共同唯一决定一个类Class的实例。也就是说判定两个类相等法则：类的全名（包名+类名）相同+类加载器相同。</p><p>类加载器在加载类的过程中，会先代理给它的父加载器，以此类推。这样的好处是能够保证Java核心库的类型安全，例如java.lang.String类，如果不存在代理模式，则不同的类加载，根据判定两个类相等的法则，会导致存在不同版本的String类，会导致不兼容问题。</p><p>核心代码：<br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">Class</span>&lt;?&gt; loadClass(String name, <span class="keyword">boolean</span> resolve)</span><br><span class="line">        <span class="keyword">throws</span> ClassNotFoundException</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">synchronized</span> (getClassLoadingLock(name)) &#123;</span><br><span class="line">            <span class="comment">//  [首先，检查该类是否被当前类加载器加载]</span></span><br><span class="line">            <span class="keyword">Class</span> c = findLoadedClass(name);</span><br><span class="line">            <span class="keyword">if</span> (c == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">long</span> t0 = System.nanoTime();</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (parent != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// [调用父类加载器的loadClass方法，实现了自底向上的检查类是否被加载的功能]</span></span><br><span class="line">                        c = parent.loadClass(name, <span class="keyword">false</span>);</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">             <span class="comment">// [父类加载器为null，也就是去调用BootClassLoader加载]</span></span><br><span class="line">                        c = findBootstrapClassOrNull(name);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">catch</span> (ClassNotFoundException e) &#123;</span><br><span class="line">                    <span class="comment">// ClassNotFoundException thrown if class not found</span></span><br><span class="line">                    <span class="comment">// from the non-null parent class loader</span></span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (c == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">long</span> t1 = System.nanoTime();</span><br><span class="line">                    <span class="comment">// [调用当前类加载器findClass方法实现了的自顶向下的类加载功能：ExtClassLoader.findClass(name) -&gt; AppClassLoader.findClass(name) -&gt; CustomClassLoader.findClass(name)]</span></span><br><span class="line">                    c = findClass(name);</span><br><span class="line">                    <span class="comment">// this is the defining class loader; record the stats</span></span><br><span class="line">                    sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0);</span><br><span class="line">                    sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1);</span><br><span class="line">                    sun.misc.PerfCounter.getFindClasses().increment();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (resolve) &#123;</span><br><span class="line">                resolveClass(c);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> c;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p><img src="http://img2.tbcdn.cn/L1/461/1/47e33802ae2f51d7452645ff811a50fce349b119.png" alt></p><h1 id="四、类加载过程"><a href="#四、类加载过程" class="headerlink" title="四、类加载过程"></a>四、类加载过程</h1><p>在前面介绍类加载器的代理模式的时候，提到过类加载器会首先代理给其它类加载器来尝试加载某个类。这就意味着真正完成类的加载工作的类加载器和启动这个加载过程的类加载器，有可能不是同一个。真正完成类的加载工作是通过调用 defineClass来实现的；而启动类的加载过程是通过调用 loadClass来实现的。前者称为一个类的定义加载器（defining loader），后者称为初始加载器（initiating loader）。在 Java 虚拟机判断两个类是否相同的时候，使用的是类的定义加载器。也就是说，哪个类加载器启动类的加载过程并不重要，重要的是最终定义这个类的加载器。两种类加载器的关联之处在于：一个类的定义加载器是它引用的其它类的初始加载器。如类 com.example.Outer引用了类com.example.Inner，则由类 com.example.Outer的定义加载器负责启动类 com.example.Inner的加载过程。</p><p>类加载器在成功加载某个类之后，会把得到的 java.lang.Class类的实例缓存起来。下次再请求加载该类的时候，类加载器会直接使用缓存的类的实例，而不会尝试再次加载。也就是说，对于一个类加载器实例来说，相同全名的类只加载一次，即 loadClass方法不会被重复调用。</p><p>参考资料：<br><a href="http://www.ibm.com/developerworks/cn/java/j-lo-classloader/" target="_blank" rel="noopener">http://www.ibm.com/developerworks/cn/java/j-lo-classloader/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一、何为类加载器&quot;&gt;&lt;a href=&quot;#一、何为类加载器&quot; class=&quot;headerlink&quot; title=&quot;一、何为类加载器&quot;&gt;&lt;/a&gt;一、何为类加载器&lt;/h1&gt;&lt;p&gt;我们编写的.java文件经过编译器编译之后，生成.class文件，即字节码文件，类加载器就是负责加载字节码文件到JVM中，并将字节码转换成为java.lang.class类的实例，这个实例便是我们编写的类，通过class实例的newInstance方法，便可以得到java类的对象。&lt;/p&gt;
&lt;p&gt;类加载器是类加载过程中的关键角色，他存在于「类加载Class load」过程的「加载」阶段中，在这个阶段，JVM虚拟机完成了三件事情:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通过一个类的全限定名(包名称+类名称)获取定义此类的二进制字节流（类的权限定名可以映射到文件系统中的文件路径）；&lt;/li&gt;
&lt;li&gt;将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构；&lt;/li&gt;
&lt;li&gt;在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口；&lt;/li&gt;&lt;/ol&gt;
    
    </summary>
    
      <category term="程序设计" scheme="http://xudongyang.coding.me/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
      <category term="java" scheme="http://xudongyang.coding.me/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/java/"/>
    
    
      <category term="JVM" scheme="http://xudongyang.coding.me/tags/JVM/"/>
    
      <category term="ClassLoader" scheme="http://xudongyang.coding.me/tags/ClassLoader/"/>
    
  </entry>
  
  <entry>
    <title>《所谓情商高就是会说话》读书笔记2</title>
    <link href="http://xudongyang.coding.me/hign-eq-is-talking2/"/>
    <id>http://xudongyang.coding.me/hign-eq-is-talking2/</id>
    <published>2017-06-27T14:27:44.000Z</published>
    <updated>2019-04-02T02:43:36.846Z</updated>
    
    <content type="html"><![CDATA[<p>作者：【日】佐佐木圭一</p><blockquote class="blockquote-center"><p>措辞就像做菜，是有谱可循的。<br>只要掌握了菜谱，任何人都可能做出美味佳肴。</p></blockquote><h1 id="完美掌握创造“警句”的技巧"><a href="#完美掌握创造“警句”的技巧" class="headerlink" title="完美掌握创造“警句”的技巧"></a>完美掌握创造“警句”的技巧</h1><ul><li>惊奇法</li><li>反差法</li><li>赤裸裸法</li><li>重复法</li><li>高潮法</li><li>数字法</li><li>合体法</li><li>顶点法<a id="more"></a></li></ul><h2 id="技巧1：惊奇法"><a href="#技巧1：惊奇法" class="headerlink" title="技巧1：惊奇法"></a>技巧1：惊奇法</h2><p>人都喜欢惊奇。同样的内容，在吃惊时能给人留下更强烈的印象。比如秘密举办的生日会。</p><p>分为两个步骤：</p><ol><li>确定想传达的内容</li><li>加入适当的惊奇词</li></ol><p>【✘】“大章鱼烧。”<br>【✓】“哇！大章鱼烧。”</p><p><img src="http://upload-images.jianshu.io/upload_images/3901673-c79501bcfeb0a74d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>【✘】“半个小时就能学会骑自行车。”<br>【✓】“啊！只要半个小时就能学会骑自行车！”</p><p>惊奇词表</p><ul><li>啊</li><li>哇</li><li>对啊</li><li>吓一跳</li><li>喔喔喔！</li><li>哎呀</li><li>竟然是这样！</li><li>啊呀</li><li>哎！？</li><li>真的！？</li><li>难以置信</li><li>（放在句尾的）！</li></ul><h2 id="技巧2：反差法"><a href="#技巧2：反差法" class="headerlink" title="技巧2：反差法"></a>技巧2：反差法</h2><p>即使忘记其他“菜谱”，也务必记住这个“菜谱”。</p><p>将想要传达的内容与“反义词”放在一起，就能形成令人印象深刻的信息。</p><p>【✓】“好像在做梦，可又不是梦。”<br>【✓】“最好是枚金牌，最坏也是金牌。”<br>【✓】“天不造人上人，亦不造人下人。”<br>【✓】“口味深浓，兼具清爽。”<br>【✓】“美女与野兽。”</p><p>分为3个步骤：</p><ol><li>确定想传达的内容</li><li>在前半句加入反义词</li><li>自由加入其他词，使前后句连接起来</li></ol><p>【✘】“大章鱼烧。”<br>【✓】“显得盘子很小的大章鱼烧”</p><p>【✘】“当海盗更有趣。”<br>【✓】“加入海军还不如当海盗更有趣。”</p><p><img src="http://upload-images.jianshu.io/upload_images/3901673-08a7a9c93c36e480.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>【✘】“加倍奉还。”<br>【✓】“人若犯我，我必犯人，还要加倍奉还。”</p><p><img src="http://upload-images.jianshu.io/upload_images/3901673-7e873f2c80afcc7c.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>【✓】“我不是在做饭，而是在创造家人的幸福” ———— 家庭主妇<br>【✓】“我不是在造螺丝，而是在创造大家的幸福” ———— 工人<br>【✓】“我不是在为公司招聘员工，而是在为公司寻找未来的合伙人” ———— 普华永道合伙人</p><h2 id="技巧3：赤裸裸法"><a href="#技巧3：赤裸裸法" class="headerlink" title="技巧3：赤裸裸法"></a>技巧3：赤裸裸法</h2><p>令人脸上发烧、难为情的、暴露自我的“措辞菜谱”。能让你创造出自己从来没有写过的、充满人情味儿的、生气勃勃的、吸引人的话语。</p><p>【✓】“激动的战栗！” “心情超爽”  ———— 奥运会冠军<br>【✓】“你忍痛努力了，我很感动！” ———— 首相对带伤获胜的运动员<br>【✓】“我喜欢你，迷恋得连自己都觉得莫名其妙”<br>【✓】“从没打过像今天这样令人陶醉的比赛。很开心，真的连眼泪都出来了。”<br>【✓】“好吃得脑中一片空白”</p><p><img src="http://upload-images.jianshu.io/upload_images/3901673-a1b33af4a80d3b76.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>找赤裸裸词的方法：想象此情此景下身体会作何反应？</p><p>嘴？—— 说不出话<br>皮肤？—— 起鸡皮疙瘩<br>脑中？—— 一片空白<br>脸上？—— 情不自禁地微笑<br>喉咙？—— 咕噜响<br>嘴唇？—— 添嘴唇<br>呼吸？—— 瞬间停顿<br>眼睛？—— 想闭上<br>汗毛？—— 全身汗毛直立<br>手心？—— 汗津津的<br>指尖？—— 颤抖<br>血流？—— 变快</p><p><img src="http://upload-images.jianshu.io/upload_images/3901673-f43dd54af204f491.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>情景：老师对学生的演讲汇报<br>【✘】“你的演讲很感人，没什么需要修改的地方。”<br>【✓】“我哭了…这样就很好”</p><p><img src="http://upload-images.jianshu.io/upload_images/3901673-5a744bf5d20e894b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h2 id="技巧4：重复法"><a href="#技巧4：重复法" class="headerlink" title="技巧4：重复法"></a>技巧4：重复法</h2><p>这是一种非常非常简单的“措辞菜谱”。一句话重复一篇，就会留着对方的脑海里。重复的话语，不仅容易留着人们的记忆里，而且会令人产生模仿的欲望。</p><p>【✓】“不要着急，不要着急，休息一会儿，休息一会儿”———— 一休<br>【✓】“我有一个梦想…我有一个梦想…”———— 马丁路德金<br>【✓】“罗密欧啊，罗密欧！为什么你偏偏是罗密欧呢？”<br>【✓】“科马内奇！科马内奇！科马内奇！”</p><p>心理学上的“纯粹接触效应”————光靠增加接触次数就能提升好感度————已被实验证实。</p><p>【✘】“大章鱼烧”<br>【✓】“很大、很大的章鱼烧”</p><p><img src="http://upload-images.jianshu.io/upload_images/3901673-e5e4d7b471486134.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h2 id="技巧5：高潮法"><a href="#技巧5：高潮法" class="headerlink" title="技巧5：高潮法"></a>技巧5：高潮法</h2><p>这个格外重要，请不要忘记。</p><p>使用高潮法，能让对方觉得“接下来的话很重要，必须仔细听”从而集中注意力。</p><p>【✓】“这里测验会考到————三角形的面积是”<br>【✓】“我必须守护的人终于出现了————那就是你”<br>【✓】“只有两种选择————要么拼命去活，要么拼命去死”<br>【✓】“我就直截了当地回答吧————抛弃金钱和名誉，剩下的就是生命”<br>【✓】“有个问题……我想问一下”</p><p><img src="http://upload-images.jianshu.io/upload_images/3901673-972aeb0db42b1b6e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>高潮词表</p><ul><li>请保密</li><li>能听见这个是你走运</li><li>只在这里说</li><li>接下来禁止拍照</li><li>只说一遍</li><li>关键有两点</li><li>在其他场合不会透露</li><li>只告诉你一个人</li><li>我要告白了</li></ul><p><img src="http://upload-images.jianshu.io/upload_images/3901673-82f2f14aaab042d4.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h2 id="技巧6：数字法"><a href="#技巧6：数字法" class="headerlink" title="技巧6：数字法"></a>技巧6：数字法</h2><p>这是超过95%的人都不知道的措辞菜谱。</p><p>光在句子中加入数字，就能在很大程度上吸引目光。</p><p>【✘】“一颗奶糖就含有大量营养”<br>【✓】“一颗300米”————奶糖广告</p><p>【✘】“别人成功卡在说话上！”<br>【✓】“措辞占9成”</p><p>【✘】“很多只斑点狗”<br>【✓】“101只斑点狗”————迪士尼动画</p><p>【✘】“短时间烹饪”<br>【✓】“3分钟烹饪”</p><p>【✘】“天才就是很少的灵感加上很大的努力”<br>【✓】“天才就是1%的灵感加上99%的努力”</p><p><img src="http://upload-images.jianshu.io/upload_images/3901673-4844fced32b3131c.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>备注：偶数比较柔和，奇数显得有棱角，比较强，一般多用奇数。</p><h2 id="技巧7：合体法"><a href="#技巧7：合体法" class="headerlink" title="技巧7：合体法"></a>技巧7：合体法</h2><p>潮流发明器。世界上大部分新事物都是由两个事物组合而成。</p><p>【✓】妖怪手表<br>【✓】懒散吉祥物<br>【✓】清凉商务<br>【✓】壁咚</p><p><img src="http://upload-images.jianshu.io/upload_images/3901673-6a853a35c90b9b6e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>【✘】“正在寻找结婚对象。”<br>【✓】“正在进行婚活！”</p><p><img src="http://upload-images.jianshu.io/upload_images/3901673-a9ed878eed8f71aa.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p><img src="http://upload-images.jianshu.io/upload_images/3901673-559875ff0bb55890.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h2 id="技巧8：顶点法"><a href="#技巧8：顶点法" class="headerlink" title="技巧8：顶点法"></a>技巧8：顶点法</h2><p>商店货架上最常用的技巧。让顾客感受到某个商品在业内位于顶点，就能说服顾客。</p><p>【✓】第一榨<br>【✓】点心的本垒打之王<br>【✓】TOP<br>【✓】全区No.1（销量No.1）<br>【✓】店长推荐</p><ol><li>真实顶点法</li></ol><p>考虑截取怎样的范围才能让商品居于顶点，在条件允许的情况下尽量选择最大的范围。比如：<br>世界第一 → 东亚第一 → 全国第一 → 全市第一 → 全区第一 → 对你个人来说是第一</p><p>【✓】那是我今年（最近）去过的最好吃的店！<br>【✓】对我来说，那家店是烤鸡肉串的王者！</p><p><img src="http://upload-images.jianshu.io/upload_images/3901673-9e12df40a1ec2cfe.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><ol><li>表现顶点法</li></ol><p>从表现手法的角度，创造某个词语，让顾客觉得这个商品是最好的。</p><p>【✘】“非常想上的课。”<br>【✓】“全世界最想上的课” ———— 保持高收视率的人气节目</p><p><img src="http://upload-images.jianshu.io/upload_images/3901673-a24a6a45508559d2.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h2 id="检验你的措辞拿手度"><a href="#检验你的措辞拿手度" class="headerlink" title="检验你的措辞拿手度"></a>检验你的措辞拿手度</h2><p>Q: 最近三个月，你有没有送过别人花？</p><h2 id="本章总结"><a href="#本章总结" class="headerlink" title="本章总结"></a>本章总结</h2><p><img src="https://img3.doubanio.com/view/note/llarge/public/p41029850.webp" alt></p><h2 id="全篇总结"><a href="#全篇总结" class="headerlink" title="全篇总结"></a>全篇总结</h2><p><img src="http://upload-images.jianshu.io/upload_images/3963387-b702a59642273227.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：【日】佐佐木圭一&lt;/p&gt;
&lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;措辞就像做菜，是有谱可循的。&lt;br&gt;只要掌握了菜谱，任何人都可能做出美味佳肴。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;完美掌握创造“警句”的技巧&quot;&gt;&lt;a href=&quot;#完美掌握创造“警句”的技巧&quot; class=&quot;headerlink&quot; title=&quot;完美掌握创造“警句”的技巧&quot;&gt;&lt;/a&gt;完美掌握创造“警句”的技巧&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;惊奇法&lt;/li&gt;
&lt;li&gt;反差法&lt;/li&gt;
&lt;li&gt;赤裸裸法&lt;/li&gt;
&lt;li&gt;重复法&lt;/li&gt;
&lt;li&gt;高潮法&lt;/li&gt;
&lt;li&gt;数字法&lt;/li&gt;
&lt;li&gt;合体法&lt;/li&gt;
&lt;li&gt;顶点法&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="essays" scheme="http://xudongyang.coding.me/categories/essays/"/>
    
    
      <category term="读书笔记" scheme="http://xudongyang.coding.me/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>《所谓情商高就是会说话》读书笔记1</title>
    <link href="http://xudongyang.coding.me/hign-eq-is-talking/"/>
    <id>http://xudongyang.coding.me/hign-eq-is-talking/</id>
    <published>2017-06-26T14:27:44.000Z</published>
    <updated>2019-04-02T02:43:36.834Z</updated>
    
    <content type="html"><![CDATA[<p>作者：【日】佐佐木圭一</p><blockquote class="blockquote-center"><p>措辞就像做菜，是有谱可循的。<br>只要掌握了菜谱，任何人都可能做出美味佳肴。</p></blockquote><h1 id="把“No”变成“Yes”的7个突破口"><a href="#把“No”变成“Yes”的7个突破口" class="headerlink" title="把“No”变成“Yes”的7个突破口"></a>把“No”变成“Yes”的7个突破口</h1><ul><li>投其所好</li><li>儆其所恶</li><li>选择的自由</li><li>被认可欲</li><li>非你不可</li><li>团队化</li><li>表示感谢</li></ul><p>【✘】“对不起，我突然有工作要做。今天的约会取消吧。”<br>【✓】“对不起，我突然有工作要做，但我更想见你了。”</p><h2 id="3个重要步骤"><a href="#3个重要步骤" class="headerlink" title="3个重要步骤"></a>3个重要步骤</h2><ol><li>不要直接说出自己的想法</li><li>揣摩对方的心理</li><li>考虑符合对方利益的措辞<a id="more"></a></li></ol><h2 id="突破口1：投其所好"><a href="#突破口1：投其所好" class="headerlink" title="突破口1：投其所好"></a>突破口1：投其所好</h2><p>这是最基本的，也是最管用的。利用“投其所好”，既能获得对方的好感，又能实现自己的期望。</p><p>情景1：店员与顾客<br>【✘】“抱歉，这种衬衫只剩这一件了。”<br>【✓】“这种衬衫卖得特别快，这是最后一件了。”</p><p><img src="/hign-eq-is-talking/cater.jpg" alt="图"></p><p>情景2：飞机供餐的鱼剩下很多时，怎样的措辞才能顺利调整配量<br>【✘】“对不起，只剩下鱼了。”<br>【✓】“机内供应以优质香草、富含矿物质的天然岩盐和粗制黑胡椒嫩煎而成的白身鱼，以及普通牛肉。”</p><p>情景3：让习惯压低价格的厂商收购高价商品的措辞<br>【✘】“让我们制作高价的高规格型号吧。”<br>【✓】“让我们制作贵公司的旗舰型号吧。”</p><h2 id="突破口2：儆其所恶"><a href="#突破口2：儆其所恶" class="headerlink" title="突破口2：儆其所恶"></a>突破口2：儆其所恶</h2><p>对难以说服的人有效。效力强劲的最终手段。但有时会显得带有强迫行，需要注意使用场合，并避免连续使用。</p><p>情景1：博物馆<br>【✘】“请勿触碰展品。”<br>【✓】“涂有药品，请勿触碰。”</p><p><img src="/hign-eq-is-talking/warning.jpg" alt="图"></p><p>情景2：让不合上马桶盖的丈夫乖乖改正的措辞<br>【✘】“合上马桶盖。”<br>【✓】“听说不合上马桶盖会失去财运哦。”</p><p>情景3：餐厅里，让一直不管淘气孩子的妈妈摇身一变的措辞<br>【✘】“为了避免打扰其他客人，可否请你们让孩子坐在座位上？”<br>【✓】“刚做好的菜很烫，如果端出来的时候被撞撒了，会给孩子造成很严重的烫伤。可否请你们让孩子坐在座位上？”</p><p><img src="/hign-eq-is-talking/warning2.jpg" alt="图"></p><p>情景4：书店里使偷书行为剧减的警示语<br>【✘】“盗窃是犯罪。”<br>【✓】“多亏大家的协助，我们捉到了盗窃犯。谢谢！”</p><h2 id="突破口3：选择的自由"><a href="#突破口3：选择的自由" class="headerlink" title="突破口3：选择的自由"></a>突破口3：选择的自由</h2><p>关键在于，要给出两个合适的选项，无论对方选择哪个，自己都能达到目的。归根结底，这种“选择的自由”是由对方决定如何选择，所以对方能产生“自主选择”的意识，被迫的感觉就会减少。</p><p>情景1：餐厅<br>【✘】“要不要来份甜点？”<br>【✓】“甜点有芒果布丁和抹茶冰激凌，您要哪种？”</p><p><img src="/hign-eq-is-talking/choice.jpg" alt="图"></p><p>情景2：让不愿穿鞋的幼儿下意识穿上鞋的措辞<br>【✘】“穿鞋。”<br>【✓】“蓝色的鞋和红色的鞋，你想穿哪双？”</p><p>情景3：让不愿出席的人参加工会会议的措辞<br>【✘】“请出席碰头会。”<br>【✓】“碰头会的便当是可选的。烤肉饭和猪排饭，要哪个？”</p><h2 id="突破口4：被认可欲"><a href="#突破口4：被认可欲" class="headerlink" title="突破口4：被认可欲"></a>突破口4：被认可欲</h2><p>对于生意和家人效果极大！人际关系也会改善。<br>“被认可欲”可以解释为心理学上的“尊重需求”，即“一个人能做出与他人的期待相对应的成果”。</p><p>情景1：让丈夫帮忙做家务的措辞<br>【✘】“你把窗户擦擦！我忙不过来。”<br>【✓】“你能够到高的地方，能把窗户擦得更亮。拜托了！”</p><p><img src="/hign-eq-is-talking/approve.jpg" alt="图"></p><p>情景2：新员工犯错误时，让打算辞职的新人恢复自信的措辞<br>【✘】“连这点小事都办不到？”<br>【✓】“没关系，铃木你一定能做到！”</p><p><img src="/hign-eq-is-talking/encourage.jpg" alt="图"></p><p>情景3：过马路时，让不愿牵手的孩子主动牵手的措辞<br>【✘】“这里很危险，拉住我的手。”<br>【✓】“我一个人很害怕，你能不能拉着我的手一起过马路？”</p><h2 id="突破口5：非你不可"><a href="#突破口5：非你不可" class="headerlink" title="突破口5：非你不可"></a>突破口5：非你不可</h2><p>听到“只有你是特别的”，人就容易被说到。利用“非你不可”能让对方感到只有自己被选中的优越感，从而乐于做出回应。</p><p>情景1：邀请同事参加公司酒会的措辞<br>【✘】“去喝酒吧？”<br>【✓】“市川，你不来不热闹，所以只有市川你务必得出席啊。”</p><p><img src="/hign-eq-is-talking/only-you1.jpg" alt="图"></p><p>情景2：把投诉变成喜爱的客服措辞<br>【✘】“我们会为您更换。”<br>【✓】“我们只为一直支持本公司的佐佐木先生您免费更换。”</p><p>情景3：终极问题：“工作和我哪个重要？”的正确答案<br>【✘】“对不起。但我也不是因为喜欢才工作的。”<br>【✓】“对不起。但是只有优子你，我不愿让你这样想。对不起，是我没用。”</p><p><img src="/hign-eq-is-talking/only-you.jpg" alt="图"></p><p>备注：语句中的名字不能省略。</p><h2 id="突破口6：团队化"><a href="#突破口6：团队化" class="headerlink" title="突破口6：团队化"></a>突破口6：团队化</h2><p>“一起”这种说法，本身就令人愉快。利用“团队化”，能使对方产生伙伴意识，即使是麻烦的请求，也会乐于接受。</p><p>情景1：邀请同事帮忙组织酒会的措辞<br>【✘】“你也来组织酒会吧？”<br>【✓】“咋们一起组织酒会吧。”</p><p><img src="/hign-eq-is-talking/team.jpg" alt="图"></p><p>情景2：使陷入恐慌状态的涩谷十字路口得到巧妙疏导的DJ警察的措辞</p><p>补充说明：13年6月4日，世界杯预选赛一直以0：1落后的日本队在全场比赛即将结束的补时阶段，由本田圭佑射入点球，帮助日本队拿到了参加巴西世界杯正赛的入场券。在涩谷附近喝酒的年轻球迷太过兴奋，大量聚集在涩谷十字路口，造成拥堵，还开始陷入恐慌。</p><p>【✘】“请不要踏入车道！请遵守交通规则！”<br>【✓】“别看我这个巡警在你们面前板着脸，其实我也因为日本队的世界杯出线而感到开心。”<br>【✓】“巡警也是你们的队友。请听听队友的话。”</p><p><img src="/hign-eq-is-talking/team2.jpg" alt="图"></p><p>情景3：女儿说服完全不运动的父亲开始锻炼的措辞<br>【✘】“多运动运动吧。”<br>【✓】“我想夜跑，但自己一个人害怕，你能不能陪我一起跑？”</p><h2 id="突破口7：感谢"><a href="#突破口7：感谢" class="headerlink" title="突破口7：感谢"></a>突破口7：感谢</h2><p>仅凭一句“谢谢”，就能拉近自己与对方的距离，使对方很难说出“No”。这种情况可以用心理学上的“互惠式好感”来解释，即“一个人接收到好意，就会产生向对方回报以好意的心理”。</p><p>利用“感谢”能让对方隐约产生信赖意识，从而难以轻易拒绝。</p><p>情景1：邀请别人帮忙<br>【✘】“把这桌子搬走。”<br>【✓】“把这桌子搬走。谢谢啊！”</p><p><img src="/hign-eq-is-talking/thanks1.jpg" alt="图"></p><p>情景2：让砍价高手按定价购买的措辞<br>【✘】“抱歉，不能打折。”<br>【✓】“请允许我送上我的真心作为赠品，还请高抬贵手。谢谢。”</p><p><img src="/hign-eq-is-talking/thanks.jpg" alt="图"></p><p>要点：在对方还没有做出回应之前，就说谢谢。</p><h2 id="检验你的措辞拿手度"><a href="#检验你的措辞拿手度" class="headerlink" title="检验你的措辞拿手度"></a>检验你的措辞拿手度</h2><p>Q: 前辈（上司）的孩子叫什么名字？</p><p>答不上来的人需要注意了。所有人都会对自己的孩子爱若珍宝。知道孩子的名字说明你考虑过“上司或前辈的爱好”。相反，不知道孩子的名字，就说明你几乎没考虑过“上司或前辈的爱好”，没能实践“揣摩对方的心理”这条社交基本原则。</p><p>Q：你还记得今天说过“谢谢”的人吗？</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="http://upload-images.jianshu.io/upload_images/3963387-9dd97a7ea8483aea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：【日】佐佐木圭一&lt;/p&gt;
&lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;措辞就像做菜，是有谱可循的。&lt;br&gt;只要掌握了菜谱，任何人都可能做出美味佳肴。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;把“No”变成“Yes”的7个突破口&quot;&gt;&lt;a href=&quot;#把“No”变成“Yes”的7个突破口&quot; class=&quot;headerlink&quot; title=&quot;把“No”变成“Yes”的7个突破口&quot;&gt;&lt;/a&gt;把“No”变成“Yes”的7个突破口&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;投其所好&lt;/li&gt;
&lt;li&gt;儆其所恶&lt;/li&gt;
&lt;li&gt;选择的自由&lt;/li&gt;
&lt;li&gt;被认可欲&lt;/li&gt;
&lt;li&gt;非你不可&lt;/li&gt;
&lt;li&gt;团队化&lt;/li&gt;
&lt;li&gt;表示感谢&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;【✘】“对不起，我突然有工作要做。今天的约会取消吧。”&lt;br&gt;【✓】“对不起，我突然有工作要做，但我更想见你了。”&lt;/p&gt;
&lt;h2 id=&quot;3个重要步骤&quot;&gt;&lt;a href=&quot;#3个重要步骤&quot; class=&quot;headerlink&quot; title=&quot;3个重要步骤&quot;&gt;&lt;/a&gt;3个重要步骤&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;不要直接说出自己的想法&lt;/li&gt;
&lt;li&gt;揣摩对方的心理&lt;/li&gt;
&lt;li&gt;考虑符合对方利益的措辞&lt;/li&gt;&lt;/ol&gt;
    
    </summary>
    
      <category term="essays" scheme="http://xudongyang.coding.me/categories/essays/"/>
    
    
      <category term="读书笔记" scheme="http://xudongyang.coding.me/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>搜索广告(sponsored-search)概述</title>
    <link href="http://xudongyang.coding.me/sponsored-search/"/>
    <id>http://xudongyang.coding.me/sponsored-search/</id>
    <published>2017-05-10T16:49:53.000Z</published>
    <updated>2019-04-02T02:43:36.869Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要内容来自于斯坦福大学的Introduction to Computational Advertising课程，重点介绍了搜索广告相关的一部分内容。</p><h1 id="计算广告-Computational-Advertising"><a href="#计算广告-Computational-Advertising" class="headerlink" title="计算广告(Computational Advertising)"></a>计算广告(Computational Advertising)</h1><p>计算广告是计算机科学中出现的一个相对较新的子科学领域，利用算法来给用户展示出最佳的广告。它集合了下面的技术于一身：</p><ul><li>信息检索 (Information retrieval)</li><li>大规模搜索与文本分析（Large scale search and text analysis）</li><li>统计建模（Statistical modeling）</li><li>机器学习 （Machine learning）</li><li>微观经济学 (Microeconomics)</li><li>博弈论、拍卖理论与机制设计 （Game theory, auction theory, mechanism design）</li><li>分类(Classification)</li><li>优化(Optimization)</li><li>推荐系统 (Recommender systems)<a id="more"></a></li></ul><h2 id="计算广告的核心挑战"><a href="#计算广告的核心挑战" class="headerlink" title="计算广告的核心挑战"></a>计算广告的核心挑战</h2><p>在一个给定场景下的给定用户和合适的广告之间找到一个最佳的匹配(Find the “best match” between a given user in a given context and a suitable advertisement)。</p><p>如果把广告看作一种信息，那么找到一个“最佳广告”就是一个信息检索问题，这个问题附带有多个可能互相矛盾的效用函数。</p><h2 id="为什么需要计算广告"><a href="#为什么需要计算广告" class="headerlink" title="为什么需要计算广告"></a>为什么需要计算广告</h2><ol><li>把传统广告学和计算机的计算能力相结合</li><li>从算法的角度来思考旧的挑战</li></ol><p>传统广告：</p><ul><li>相对而言平台较小－－杂志、广告牌、报纸、传单、电视等</li><li>每个平台花费巨资（几百万的电视广告费用）</li><li>不可能个性化</li><li>只能由聪明的广告人来决定在哪里投放</li><li>很难度量投资回报率(ROI)</li></ul><p>计算广告：</p><ul><li>亿级别的投放机会</li><li>亿级别的创意形式</li><li>完全个性化</li><li>每次投放而言花费很小</li><li>更容易度量</li></ul><p>著名的百货商店之父约翰*沃纳梅克曾经说过：<br><blockquote class="blockquote-center"><p>我在广告上的投资有一半是无用的，但是问题是我不知道是哪一半。</p></blockquote></p><h2 id="计算广告的分类"><a href="#计算广告的分类" class="headerlink" title="计算广告的分类"></a>计算广告的分类</h2><p>根据广告主的计费方式，可以分为</p><ul><li>千次展现付费 CPM(cost per thousand impressions) 主要用于品牌曝光,例如钻展业务</li><li>每次点击扣费 CPC(cost per click) 通常用于文本广告,例如直通车搜索／定向广告</li><li>成交/行为付费 CPT/CPA(cost per transaction/action) 例如淘宝客业务</li></ul><p>根据展现形式分为:图片广告[Graphical Ads]、文本广告[Textual Ads]、视频等。</p><p>根据不同的产品形式分为</p><ul><li>搜索广告(Sponsored Search)，例如搜索直通车KGB业务线</li><li>上下文广告[Contexual Ads]，例如Google Adsense</li><li>展示广告[Display ads]，例如钻展业务线</li><li>定向广告[Targeting Ads]，例如定向直通车K2业务线</li></ul><p>在互联网中，搜索广告是最主要的文本广告的形式。</p><h2 id="互联网广告的意义"><a href="#互联网广告的意义" class="headerlink" title="互联网广告的意义"></a>互联网广告的意义</h2><p>广告支撑起了互联网上一个巨大的生态系统。如果没有广告，互联网就不可能像现在这么发展迅速、规模宏大。广告给消费者提供了直接和间接的巨大价值。</p><ol><li>内容提供商通过广告赚钱，养活了 宏观／微观的内容提供商 ［就是各种大小网站］</li><li>精准触达／定向使得长尾生意成为可能</li><li>广告主的收入使得大批“免费”的服务成为可能：Facebook, Google, Twitter,Yahoo</li></ol><p>计算广告的参与方：</p><ol><li>流量提供方(Publishers)</li><li>广告主(Advertisers)</li><li>浏览者／用户(Users)</li><li>广告平台／广告网络(Match maker/Ad network)</li></ol><p>这些参与者有各自不同的诉求：流量提供者渴望每次展现/搜索的高收益，广告主渴望高投资回报率(ROI)和流量，用户希望高相关性，广告网络渴望收益与市场份额。而广告的选择，就是要兼顾四个参与者的收益，达到最优状态,需要权衡长期和短期的商业目标。</p><h2 id="计算广告对性能的要求很高"><a href="#计算广告对性能的要求很高" class="headerlink" title="计算广告对性能的要求很高"></a>计算广告对性能的要求很高</h2><p>亿级别：</p><ul><li>搜索广告中有数亿级别的广告</li><li>每个小时有亿级别的搜索</li><li>万亿级别页面展现次数</li><li>亿级的用户</li></ul><p>毫秒级别：</p><ul><li>请求是在用户“等待”过程中完成的,必须在100ms内返回</li></ul><p>钱：</p><ul><li>每个请求都需要消耗CPU资源</li><li>数据通常放在内存中 ［需要大量内存，而内存比硬盘贵］</li><li>每次请求的耗费必须比收益要低</li><li>过低的点击率(ctr)使得上面的挑战更加困难</li></ul><h1 id="搜索广告-sponsored-search"><a href="#搜索广告-sponsored-search" class="headerlink" title="搜索广告(sponsored search)"></a>搜索广告(sponsored search)</h1><p>搜索广告是由搜索关键词驱动的广告。广告主选择一个“竞价词”，当用户触发某个搜索请求时，广告主的广告得以展现。业界的搜索广告系统：Google AdWords、 百度凤巢、淘宝搜索直通车等。</p><p>在上文中我们提到的计算广告中有4个参与方，在搜索广告中，流量提供方是搜索结果页 SERP(search results page)，通常流量提供方和广告平台是同一个（Google，Bing），当然也可以不一样（微软给雅虎提供广告搜索）。</p><h2 id="搜索广告参与者之间的交互行为"><a href="#搜索广告参与者之间的交互行为" class="headerlink" title="搜索广告参与者之间的交互行为"></a>搜索广告参与者之间的交互行为</h2><p>广告主：</p><ol><li>提交广告，购买相关的竞价词</li><li>为了获得好的展示位置而竞价</li><li>为获得的点击付费</li></ol><p>浏览者：给搜索引擎提交查询串，表达一定的意图</p><p>搜索引擎：</p><ol><li>根据用户的查询串在web页面语料库和广告语料库中分别进行检索</li><li>把自然搜索结果和广告搜索结果结合到一起，展示在搜索结果页 SERP上</li></ol><h2 id="搜索广告中存在的三个子问题"><a href="#搜索广告中存在的三个子问题" class="headerlink" title="搜索广告中存在的三个子问题"></a>搜索广告中存在的三个子问题</h2><p>从搜索引擎的角度来看，搜索广告中存在三个子问题：</p><ol><li>广告检索</li><li>给拿到的广告排序</li><li>根据点击收费</li></ol><p>以上三个顺序是搜索广告事件发生的顺序,这里面1和2属于信息检索问题，而2和3又属于微观经济学问题。</p><p>文章一开头提到了计算广告中涉及到了博弈论，拍卖理论，机制设计，到底在哪里用到了呢？想了解这些疑问就得接着往下看。</p><p>对于目前的搜索广告来说，都被设计成了拍卖的机制。搜索引擎拍卖的是每个流量中可能的广告位，广告主提交对购买的关键词的每次点击的最高出价，广告主是不知道其他人的出价信息的。虽然每个流量中一般会有多个广告位置，但是广告主只出一个价格。最终搜索引擎根据广告主竞价和广告的点击率CTR来对广告进行排序，决定最终的展示位置。</p><p>点击扣费时，目前普遍采用的是Google发明的广义第二价格扣费GSP(General Second Price)，有两种策略：</p><ul><li>竞价排序：根据广告的出价倒序排列，位于第i个的广告支付第i+1个广告的竞价</li><li>根据广告平台的收益排序：根据期望最大收益ecpm来排序，ecpm=bidprice*ctr</li></ul><p>被点击的广告主i付的费用为<br><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">price=bidprice(<span class="name">i+1</span>)*(<span class="name">ctr</span>(<span class="name">i+1</span>)/ctr(<span class="name">i</span>))</span><br></pre></td></tr></table></figure></p><p>通常根据期望最大收益ecpm来排序的，由于<code>bidprice(i)*ctr(i)&gt;bidprice(i+1)*ctr(i+1)</code>,可以从上述公式看到广告主实际扣费肯定小于最高出价。</p><p>在广告搜索引擎中，不能直接拿着用户的查询串在倒排索引中进行广告检索的，因为这样可能导致搜出来的广告深度不够，而且查询多种多样，在搜索引擎有限的资源下，不可能对所有查询建立倒排索引，所以需要经过查询改写来改写出归一化后的多个搜索词，用这些搜索词去检索广告。</p><h2 id="查询改写-Query-Rewrite"><a href="#查询改写-Query-Rewrite" class="headerlink" title="查询改写(Query Rewrite)"></a>查询改写(Query Rewrite)</h2><p>把用户查询(Query)改写成竞价词(Bidword)的过程。总的来说有离线(offline)改写和在线改写(online)两类。</p><ul><li>离线改写</li></ul><p>在离线的时候利用相对在线而言更多的数据来处理用户的查询，生成一个query-&gt;bidword的映射关系表，缺点是只能给那些高频词进行离线处理。这里有两个问题：我们应该改写哪些查询－－我们需要市场深度的查询上。我们应该改写成什么样的查询－－那些市场深度足够的查询上。</p><ul><li>在线改写</li></ul><p>对于离线不能处理的少数请求需要我们进行在线改写，在线改写相对离线而言，资源受限（内存限制，时间限制），需要在很短的时间（数ms）内做分析。</p><h2 id="广告选择-Ad-Selection"><a href="#广告选择-Ad-Selection" class="headerlink" title="广告选择(Ad Selection)"></a>广告选择(Ad Selection)</h2><p>给定一个查询，搜索引擎可以展示两类广告：</p><ul><li>精确匹配EM(Exact match): 广告主对特定的查询竞价</li><li>高级匹配AM(Advanced match)或广泛匹配(Broad match): 广告主并不对特定的词进行竞价，但是这个查询被认为是广告主感兴趣的。</li></ul><p>搜索广告中的流量具有长尾效应，非常多的长尾流量查询在搜索广告中没有对应的广告，高级匹配主要是为了解决这相当一部分流量没有被竞价的问题，广告主不关心竞价词，他们只关心转化－－卖掉商品。如何覆盖到相关的流量呢，从引擎的角度出发，高级匹配比精确匹配更有挑战性。</p><p>在广告搜索引擎中，广告语料库中的广告数量巨大，高达百万之多，而实际展示的过程中，只有极少数量的广告能够展示出来，目前业界普遍采用两阶段广告选择来解决性能问题。即在广告检索过程中，整个广告的选择分为两部分：</p><ul><li>广告检索（Ad Retrieval）:从整个广告语料库里面选出一个最可能的候选集合。此过程是粗选阶段，我们用来排序的目标函数（例如评估相关性）可能和最终排序的函数（例如ecpm）不同。</li><li>广告重排序(Ad Reordering)：利用更多的数据来对第一阶段返回的有限的候选广告集合进行更为精细，更为复杂的计算。这个阶段必须综合考虑竞价和相关性分数（例如ecpm）。</li></ul><p>对于广告重排序阶段，目前有两种主流的方法，以赛马为例：</p><ul><li>反应式(Reactive)：选定一匹马，根据它的历史成绩来预测未来的表现</li><li>预估式(Predictive)：根据体重，腿长等特征为赛马建模，找到这些特征在预测比赛名次终的重要程度，然后基于这些特征来给见过、未见过的赛马预测成绩。</li></ul><p>当我们拥有对某赛马的足够信息的时候，就使用这些信息（反应性），否则使用模型（预测性）。</p><p>目前主流的搜索广告系统都是预估式的，通过抽取广告中影响CTR的特征值来给广告建模，基于这些特征值来给广告进行CTR预估。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要内容来自于斯坦福大学的Introduction to Computational Advertising课程，重点介绍了搜索广告相关的一部分内容。&lt;/p&gt;
&lt;h1 id=&quot;计算广告-Computational-Advertising&quot;&gt;&lt;a href=&quot;#计算广告-Computational-Advertising&quot; class=&quot;headerlink&quot; title=&quot;计算广告(Computational Advertising)&quot;&gt;&lt;/a&gt;计算广告(Computational Advertising)&lt;/h1&gt;&lt;p&gt;计算广告是计算机科学中出现的一个相对较新的子科学领域，利用算法来给用户展示出最佳的广告。它集合了下面的技术于一身：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;信息检索 (Information retrieval)&lt;/li&gt;
&lt;li&gt;大规模搜索与文本分析（Large scale search and text analysis）&lt;/li&gt;
&lt;li&gt;统计建模（Statistical modeling）&lt;/li&gt;
&lt;li&gt;机器学习 （Machine learning）&lt;/li&gt;
&lt;li&gt;微观经济学 (Microeconomics)&lt;/li&gt;
&lt;li&gt;博弈论、拍卖理论与机制设计 （Game theory, auction theory, mechanism design）&lt;/li&gt;
&lt;li&gt;分类(Classification)&lt;/li&gt;
&lt;li&gt;优化(Optimization)&lt;/li&gt;
&lt;li&gt;推荐系统 (Recommender systems)&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="计算广告" scheme="http://xudongyang.coding.me/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"/>
    
    
      <category term="计算广告" scheme="http://xudongyang.coding.me/tags/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"/>
    
      <category term="搜索广告" scheme="http://xudongyang.coding.me/tags/%E6%90%9C%E7%B4%A2%E5%B9%BF%E5%91%8A/"/>
    
  </entry>
  
  <entry>
    <title>拨开深度学习的迷雾：训练一个性能优秀的深度模型</title>
    <link href="http://xudongyang.coding.me/deep-learning/"/>
    <id>http://xudongyang.coding.me/deep-learning/</id>
    <published>2017-03-30T03:03:40.000Z</published>
    <updated>2019-04-02T02:43:36.814Z</updated>
    
    <content type="html"><![CDATA[<p>深度学习技术已经在很多领域取得了非常大的成功，然而其调参的复杂性也导致很多机器学习从业者迷失在丛林里。本文旨在总结一些训练出一个性能优秀的深度模型的相关经验，帮助自己以及这个领域的初学者少走弯路。</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>sigmoid/tanh. 存在饱和的问题，现在不建议使用。</p><p>ReLU. 最近几年常用的激活函数，形式为$f(x) = \max(0, x)$，目前建议首先尝试用这个激活函数。</p><ul><li>相比于sigmoid/tanh函数而言，ReLU能极大地加快SGD算法的收敛速度，因为其分段线性的形式不会导致饱和；</li><li>相比于sigmoid/tanh函数而言，ReLU实现简单不需要昂贵的指数运算</li><li>然而，训练过程中ReLU单元可能会失效。例如，当一个非常大的梯度流经ReLU单元时，导致该单元对应的权重更新后（变得非常小）再也无法在任何数据点上激活。因此，需要小心设置学习率，较小的学习率可以缓解该问题。（太大的学习率可能会导致40%以上的ReLU单元变成dead的不可逆状态）<a id="more"></a>Leaky ReLU. 函数形式为：$f(x) = \mathbb{1}(x &lt; 0) (\alpha x) + \mathbb{1}(x&gt;=0) (x)$，其中$\alpha$是一个小的常数。Leaky ReLU是为了修复ReLU的dying问题，然后实际使用的结果并不一致。</li></ul><p>Maxout. 可以看作是ReLU的泛化。形式如： $\max(w_1^Tx+b_1, w_2^Tx + b_2)$</p><h3 id="如何决定网络结构（层数和每层的节点数）"><a href="#如何决定网络结构（层数和每层的节点数）" class="headerlink" title="如何决定网络结构（层数和每层的节点数）"></a>如何决定网络结构（层数和每层的节点数）</h3><p>增加神经网络的层数或者节点数，模型的容量（能够表示的函数空间）会增大。下图是在一个二分类问题上不同隐层节点数的3个单隐层神经网络模型的训练结果。<br><img src="http://cs231n.github.io/assets/nn1/layer_sizes.jpeg" alt></p><p>可以看出，节点数越多越能够表示复杂的函数，然而也越容易过拟合，因为高容量的模型容易捕获到训练数据噪音。如上图所示，只有隐层只有3个节点的模型的分类决策面是比较光滑的，它把那些错误分类的点认为是数据中的异常点/噪音（outlier）。实际中，这样的模型泛化性能可能更好。</p><p>那么是否意味着我们应该偏好小的模型呢？答案是否定的，因为我们有其他更好的方法来防止模型过拟合，比如说正则化、dropout、噪音注入等。实际中，更常用这些方法来对抗过拟合问题，而不是简单粗暴地减少节点数量。</p><p>这其中微妙的原因在于，小的神经网络用梯度下降算法更难训练。小的神经网络有更少的局部极小值，然而其中许多极小值点对应的泛化性能较差且易于被训练算法到达。相反，大的神经网络包含更多的局部极小值点，但这些点的实际损失是比较小的。更多内容请参考这篇论文《<a href="http://arxiv.org/abs/1412.0233" target="_blank" rel="noopener">The Loss Surfaces of Multilayer Networks</a>》。实际中，小的网络模型的预测结果方差比较大；大的网络模型方差较小。</p><p>重申一下，正则化是更加好的防止过拟合的方法，下图是有20个节点的单隐层网络在不同正则化强度下的结果。<br><img src="http://cs231n.github.io/assets/nn1/reg_strengths.jpeg" alt><br>可见，合适的正则化强度可以使得一个较大的模型的决策分类面比较光滑。然而，千万要注意的是不能让正则项的值淹没了原始数据损失，那样的话梯度就主要有正则项来决定了。</p><p>因此，在计算能力预算充足的情况下，应该偏好使用大的网络模型，同时使用一些防止过拟合的技术。</p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>假设有数据集为一个N * D的矩阵X，N是数据记录数，D是每条数据的维数。</p><ul><li><p>减去均值。在每个特征上都把原始数据减去该维特征的均值是一种常用的预处理手段，处理之后的数据是以原点为中心的。<code>X -= np.mean(X, axis = 0)</code></p></li><li><p>归一化。使得不同维度的数据有相同的scale。主要有两种归一化方法，一种是各个维度上首先减去均值后再除以标准差：<code>X /= np.std(X, axis = 0</code>；另一种是最小最大标准化，这种方法归一化之后的范围在[-1,1]，只有在不同维度特征数据有不同的单位或者scale时，采用这种方法才是有意义的。</p></li></ul><p><img src="http://cs231n.github.io/assets/nn2/prepro1.jpeg" alt></p><ul><li>降维。如PCA方法、Whitening方法。这是一个可选的步骤。</li></ul><p>注意数据预处理的陷阱。所有预处理阶段需要用到的统计数据，比如均值、标准差等，只能在训练集上计算，然后应用到包括测试集和验证集的数据上。例如，在整个数据集上计算均值，然后每条数据减去均值做数据原点中心化，最后再把数据集分裂为训练集、验证集和测试集的流程是错误的。这种类型的错误有时候被叫做<strong>数据穿透</strong>，即训练集中包含了本不该包含的数据或数据的统计特征，是机器学习从业者常犯的一种数据。比如，在做商品点击率预估时，假设我们用不包括昨天在内的前7天的日志数据作为特征提取的数据集，昨天的日志数据作为数据样本的label生成数据集，那么需要格外小心计算特征（比如，用户的偏好类目）时，千万不要把昨天的数据也统计进去。</p><h3 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h3><p>神经网络权重初始化的基本原则是要打破网络结构的对称性（symmetry breaking）。比如，权重全部初始化为0是错误的，这样的话所有节点计算到的梯度值都是一样的，权重更新也是一致的，最终的结果就是所有权重拥有相同的值。</p><ul><li>随机初始化为小的数值。当然也不能太小，否则计算出的梯度就会很小。<code>W = 0.01* np.random.randn(D,H)</code></li><li>用n的平方根校正方差。<code>w = np.random.randn(n) / sqrt(n)</code>，其中<code>n</code>是输入的数量。这也意味着，权重需要逐层初始化，因为每层的节点对应的输入通常是不同的。如果节点的激活函数为ReLU，那么用<code>sqrt(2.0/n)</code>来校正方差更好：<code>w = np.random.randn(n) * sqrt(2.0/n)</code>。</li><li>稀疏初始化。首先把所有的权重初始化为0，然后为每个节点随机选择固定数量（比如10）的链接赋予小的高斯权重。该方法也可以打破网络结构的对称性。</li><li><a href="http://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization</a>。</li></ul><p>偏置（biases)通常初始化为0。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>多分类问题的常见损失函数为：</p><ul><li>SVM loss：$L_i = \sum_{j\neq y_i} \max(0, f_j - f_{y_i} + 1)$</li><li>Cross-entropy loss: $L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)$。对应非常多类别的分类任务，交叉熵损失的计算代价是非常大的，缓解这一问题的常用方法包括Hierarchical Softmax和negative sampling。</li></ul><p>属性分类问题(Attribute classification，即每个样本的label不止一个)的常用损失函数为：</p><script type="math/tex; mode=display">L_i = \sum_j \max(0, 1 - y_{ij} f_j)</script><p>上式中的加和是在所有类别$j$上进行的，当第i个样本有第j个类别标签时$y_{ij}$的值为1，否则为-1；当第j个类别被预测时，$f_j$的值为正，否则为负。<br>另一种常见的方法，是为每一个类别训练一个二分类模型，这时采用的损失为逻辑回归损失函数：<script type="math/tex">L_i = \sum_j y_{ij} \log(\sigma(f_j)) + (1 - y_{ij}) \log(1 - \sigma(f_j))</script>，其中$y_{ij}$在模型预测为正例时值为1，预测为负例时值为0。</p><p>回归问题的损失函数：</p><ul><li>L2 loss: $L_i = \Vert f - y_i \Vert_2^2$</li><li>L1 loss: $L_i = \Vert f - y_i \Vert_1 = \sum_j \mid f_j - (y_i)_j \mid$</li></ul><p>L2损失通常比较难优化，相对于比较稳定的Softmax损失而言，因为它需要网络输出尽可能精确逼近一个值；而对于Softmax而言，每个节点输出的具体值并不是那么重要，softmax只关心它们的（相对）大小是否恰当。并且，L2损失易受异常点的影响，鲁棒性不够。</p><p>因此，当遇到一个回归问题时，首先考虑能否转化为分类问题，即可否把要回归的值划分到固定大小的桶。比如，一个评分预测任务，与其训练一个回归模型，不如训练5个独立的分类模型，用来预测用户是否给评分1~5。</p><blockquote><p>When faced with a regression task, first consider if it is absolutely necessary. Instead, have a strong preference to discretizing your outputs to bins and perform classification over them whenever possible.</p></blockquote><h3 id="检查梯度"><a href="#检查梯度" class="headerlink" title="检查梯度"></a>检查梯度</h3><p>如果自己实现模型，需要做梯度的解析解和数值解的对比验证。数值解用下面的公式计算：<script type="math/tex">\frac{df(x)}{dx} = \frac{f(x + h) - f(x - h)}{2h} \hspace{0.1in}</script><br>其中，$h$的推荐值为1e-4 ~ 1e-6。<br>在比较两者的差异时，使用相对误差，而不是绝对误差：<script type="math/tex">\frac{\mid f'_a - f'_n \mid}{\max(\mid f'_a \mid, \mid f'_n \mid)}</script></p><ul><li>relative error &gt; 1e-2 usually means the gradient is probably wrong</li><li>1e-2 &gt; relative error &gt; 1e-4 should make you feel uncomfortable</li><li>1e-4 &gt; relative error is usually okay for objectives with kinks. But if there are no kinks (e.g. use of tanh nonlinearities and softmax), then 1e-4 is too high.</li><li>1e-7 and less you should be happy.</li></ul><p>网络越深，两者的相对误差越大。另外，为了防止数值问题，在计算过程中使用double类型而不是float类型。还需要额外注意不可导的点，比如ReLU在原点出不可导。需要在尽可能多的节点比较两者，而不只是一小部分。可以只在一部分维度上做检查。做梯度检查时，需要关闭正则项、dropout等。</p><h3 id="训练前的检查"><a href="#训练前的检查" class="headerlink" title="训练前的检查"></a>训练前的检查</h3><ul><li>初始化权重之后，检查损失是否符合预期。比如，10个类别的分类问题，采用交叉熵损失函数，那么期望的初始数据损失（不包括正则项）为2.302左右，因为我们预计初始时每个类别被预测的概率都是0.1，因此交叉熵损失就是正确类别的负对数概率：-ln(0.1)=2.302。对于The Weston Watkins SVM损失，初始时假设有9个类别都违反了最小间隔是合理的，因此期望损失为9（因为每一个错误的列表的最小间隔为1）。</li><li>增加正则项强度，应该要能对应地增加损失。</li><li>用一小部分数据训练模型，直到模型过拟合，最终的损失为0（正则项强度设为0）。如果这项检查没有通过，就不该开始训练模型。</li></ul><h3 id="监控训练过程"><a href="#监控训练过程" class="headerlink" title="监控训练过程"></a>监控训练过程</h3><ul><li><p>跟踪损失的变化情况（evaluated on the individual batches during the forward pass），验证学习率是否设置合理。<br><img src="http://cs231n.github.io/assets/nn3/learningrates.jpeg" alt="loss"></p></li><li><p>跟踪正确率的变化（在训练集和验证集上分别跟踪），判断模型是否过拟合，以及模型该在什么时候停止训练。<br><img src="http://cs231n.github.io/assets/nn3/accuracies.jpeg" alt="accuracy"><br>如果发生过拟合，则应加强正则化的强度，比如增加L2正则项的系数$\lambda$，或者增加dropout的概率等。当然，如果验证集的正确率和训练集的正确率一直吻合得很好也是有问题的，这意味着模型的容量可能不够，应该尝试增加更多的节点（参数）。</p></li><li><p>跟踪权重更新情况，计算并记录每组参数更新的比率：$\frac{\Delta w}{w}$，这个比率应该在1e-3左右，如果比这个值小意味着学习率可能过小，反之，则应怀疑学习率是否过大。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># assume parameter vector W and its gradient vector dW</span></span><br><span class="line">param_scale = np.linalg.norm(W.ravel())</span><br><span class="line">update = -learning_rate<span class="number">*d</span>W #<span class="built_in"> simple </span>SGD update</span><br><span class="line">update_scale = np.linalg.norm(update.ravel())</span><br><span class="line">W += update # the actual update</span><br><span class="line"><span class="builtin-name">print</span> update_scale / param_scale # want ~1e-3</span><br></pre></td></tr></table></figure></li><li><p>跟踪每层的激活函数值分布或梯度分布，验证初始化是否正确。比如使用tanh激活函数的层，如果看到激活函数的值大量集中在0、1或者-1，则表示不正常。</p></li><li><p>如果是在处理图像任务，则可以尝试可视化第一层的权重，查看模拟的图片是否光滑、干净。</p></li></ul><h3 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h3><p>神经网络模型的参数更新有多种方式，具体可以查看这篇文章：<a href="/gradient-descent-variants/" title="深度学习中的常用训练算法">深度学习中的常用训练算法</a>。</p><p>SGD+Nesterov Momentum 或者 Adam 是两种推荐的参数更新方法。</p><h3 id="超参数优化"><a href="#超参数优化" class="headerlink" title="超参数优化"></a>超参数优化</h3><p>神经网络模型的主要超参数包括：</p><ul><li>初始学习率</li><li>学习率衰减调度方法</li><li>正则化的强度</li></ul><p>由于神经网络模型通常比较大，因此交叉验证的代价很高，通常用一折验证来代替交叉验证。</p><p>在log scale上搜索超参数是推荐的做法，比如学习率的搜索范围可以设为<code>learning_rate = 10 ** uniform(-6, 1)</code>，正则化强度也可以采用类似的方法。这是因为学习率和正则化强度都是以乘积的形式影响代价函数的。如果学习率为0.001，那么加上0.01就会产生巨大的影响；但如果学习率为10，那么加上0.01则几乎观察不到任何影响，因此考虑学习率的范围时乘以或除以一个数，要不加上或者减去一个数更好。在另外一些参数上，则还是保留原来的量级较好，比如dropout概率：<code>dropout = uniform(0,1)</code>。</p><p>需要注意搜索范围的边界，如果效果在边界处最好，则可能需要修改搜索范围并重新搜索。</p><p>与Grid search相比，random search更好，据说random search效率更高。深度学习中经常发生的情况是，其中一些超参数要比另一些更加重要，与网格搜索相比随机搜索能够精确地在重要的超参数上发现更好的值。具体查看这偏论文：《<a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" target="_blank" rel="noopener">Random Search for Hyper-Parameter Optimization</a>》<br><img src="http://cs231n.github.io/assets/nn3/gridsearchbad.jpeg" alt></p><p>超参搜索需要分阶段，从粗粒度到细粒度分层进行。比如首先先搜索较粗的范围(e.g. 10 ** [-6, 1])，然后根据搜索到的最好的值，使用更窄的搜索范围。粗粒度搜索时，可以不用等待训练完全收敛，只需要观察前面几个epoch即可。</p><p>贝叶斯超参数优化是一个研究如何高效地探索超参数空间的研究领域，其核心思想是在不同的超参数上验证性能时做好探索和利用的平衡（exploration - exploitation trade-off）。 Spearmint，SMAC，Hyperopt是几个有名的基于贝叶斯超参数优化方法的库。</p><h3 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h3><p>训练几个独立的神经网络模型，用他们预测结果的平均值（或者多数表决）来确定最终的结果，是一种常用的改进性能的方法。通常集成的模型数量越多，改进的空间也越大。当然，集成彼此之间有差异化的模型更好。几种构建集成模型的常用方法如下：</p><ul><li>相同的模型，不同的初始化。用交叉验证的方法确定最佳超参数，然后训练多个使用最佳超参数但不同初始化的模型。这样方法，集成的模型多样性可能不够。</li><li>交叉验证中发现的性能最好的多个模型。有足够的多样性，但也增加了集成进一些次优的模型的风险。</li><li>保留同一个模型在训练过程中的不同副本（checkpoint）。因为深度学习的训练通常都是昂贵的，这种方法不需要训练多个模型，是非常经济的。但也有缺乏多样性的风险。</li><li>在训练过程中平均模型的参数得到一个新的模型。在训练过程中维护一个额外的网络，它的参数取值与正式模型权重的指数衰减和（an exponentially decaying sum of previous weights during training）。相对于是维护了最近几次迭代生成的模型的移动平均，这种平滑的方法相对于是前一种方法的一种特殊实现，在实际中可以获得一到两个百分点的性能提升。对这种方法一个比较粗略的认识是，代价函数的误差超平面是一个碗状的结构，模型尝试到达碗底的位置，因此平均之后的模型更容易到家接近碗底的位置。</li></ul><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://www.youtube.com/watch?v=EK61htlw8hY" target="_blank" rel="noopener">Dark Knowledge</a> from Geoff Hinton<br><a href="http://arxiv.org/pdf/1206.5533v2.pdf" target="_blank" rel="noopener">Practical Recommendations for Gradient-Based Training of Deep Architectures</a> from Yoshua Bengio<br><a href="http://cs231n.github.io/" target="_blank" rel="noopener">CS231n: Convolutional Neural Networks for Visual Recognition </a> from Andrew Ng</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;深度学习技术已经在很多领域取得了非常大的成功，然而其调参的复杂性也导致很多机器学习从业者迷失在丛林里。本文旨在总结一些训练出一个性能优秀的深度模型的相关经验，帮助自己以及这个领域的初学者少走弯路。&lt;/p&gt;
&lt;h3 id=&quot;激活函数&quot;&gt;&lt;a href=&quot;#激活函数&quot; class=&quot;headerlink&quot; title=&quot;激活函数&quot;&gt;&lt;/a&gt;激活函数&lt;/h3&gt;&lt;p&gt;sigmoid/tanh. 存在饱和的问题，现在不建议使用。&lt;/p&gt;
&lt;p&gt;ReLU. 最近几年常用的激活函数，形式为$f(x) = \max(0, x)$，目前建议首先尝试用这个激活函数。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;相比于sigmoid/tanh函数而言，ReLU能极大地加快SGD算法的收敛速度，因为其分段线性的形式不会导致饱和；&lt;/li&gt;
&lt;li&gt;相比于sigmoid/tanh函数而言，ReLU实现简单不需要昂贵的指数运算&lt;/li&gt;
&lt;li&gt;然而，训练过程中ReLU单元可能会失效。例如，当一个非常大的梯度流经ReLU单元时，导致该单元对应的权重更新后（变得非常小）再也无法在任何数据点上激活。因此，需要小心设置学习率，较小的学习率可以缓解该问题。（太大的学习率可能会导致40%以上的ReLU单元变成dead的不可逆状态）&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中的常用训练算法</title>
    <link href="http://xudongyang.coding.me/gradient-descent-variants/"/>
    <id>http://xudongyang.coding.me/gradient-descent-variants/</id>
    <published>2017-03-24T06:42:57.000Z</published>
    <updated>2019-04-02T02:43:36.822Z</updated>
    
    <content type="html"><![CDATA[<p>梯度下降算法是深度学习中使用得最多的优化算法。它是一种用来最小化目标函数$J(\theta)$（$\theta \in \mathbb{R}^d$是模型的参数）的方法，其沿着与目标函数相对于参数的梯度$\nabla_\theta J(\theta)$相反的方向更新参数的值。形象地说，就是沿着目标函数所在的超平面上的斜坡下降，直到到达山谷(目标函数极小值）。</p><p>关于梯度下降算法的具体介绍请参考我的另一篇博文：<a href="/gradient-descent/" title="《梯度下降算法分类及调参优化技巧》">《梯度下降算法分类及调参优化技巧》</a></p><p>梯度更新通常有两大类方法，一类是基于随机梯度下降及其变种的一阶方法；另一类是以牛顿方法、共轭梯度为代表的二阶近似优化方法。</p><p>本文主要对第一类方法做一个简要的综述。<br><a id="more"></a></p><h1 id="基本算法"><a href="#基本算法" class="headerlink" title="基本算法"></a>基本算法</h1><h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><p>在训练数据非常多时，一般使用小批量随机梯度下降算法，也常常被简称为SGD，其更新公式如下：</p><script type="math/tex; mode=display">\theta_{t+1}=\theta_{t}+\Delta \theta_{t} \quad where \quad  \Delta \theta_{t}=-\epsilon \cdot g_{t}</script><p>其中，$\epsilon$为学习率。</p><h2 id="动量（momentum）"><a href="#动量（momentum）" class="headerlink" title="动量（momentum）"></a>动量（momentum）</h2><p>现实任务的误差超平面往往是非常复杂的，存在许多局部极小点或者鞍点，使得原生的SGD算法收敛速度较慢，且容易陷入泛化性能不太好的区域，如下图所示。<br><img src="http://www.willamette.edu/~gorr/classes/cs449/figs/descent2.gif" alt></p><p>动量方法旨在加速学习，特别是处理高曲率、小但一致的梯度，或是带噪声的梯度。动量算法积累了之前梯度指数级衰减的移动平均，并且继续沿该方向移动。从形式上看，动量算法引入了变量$v$充当速度角色——它代表参数在参数空间移动的方向和速率。速度被设为负梯度的指数衰减平均。名称动量来自物理类比，根据牛顿运动定律，负梯度是移动参数空间中质点的力。动量在物理学上定义为质量乘以速度。在动量学习算法中，我们假设是质点是单位质量，因此速度向量$v$也可以看作是质点的动量。其梯度更新公式如下：</p><script type="math/tex; mode=display">v \leftarrow \alpha v - \epsilon \nabla_{\theta} \left( \frac{1}{m} \sum_{i=1}^m  L(f(x^{(i)}; \theta), y^{(i)}   )  \right), \theta \leftarrow \theta  + v .</script><p>超参数$α∈[0,1)$决定了之前梯度的贡献衰减得有多快。实践中，$α$的一般取值为0.5，0.90和0.99。和学习率一样，α也可以随着时间不断调整。一般初始值是一个较小的值，随后会慢慢变大。随着时间推移调整$α$没有收缩$ϵ$重要。</p><p>形象地，当我们沿着误差超平面滚动一个小球下山时，小球的速度回越来越快，使得小球可以越过山腰上的沟壑，有机会最终到达谷底。动量项使得参数在梯度方向一致的维度上获得较大的更新，在梯度方向改变的维度上获得较小的更新。相对于SGD，可以减少参数更新过程中代价函数的抖动，获得更快的收敛速度。</p><h2 id="Nesterov动量"><a href="#Nesterov动量" class="headerlink" title="Nesterov动量"></a>Nesterov动量</h2><p>沿着误差超平面滚动一个小球下山时，如果盲目地跟随斜坡下降是不能令人满意的，我们需要一个更加智能的小球，它能够知道自己如果不做任何改变时下一步将会到达什么位置，并根据此位置调整当前的动作。</p><p>基于Nesterov动量的算法，也叫做NAG（Nesterov accelerated gradient）算法，能够提供这样的预知。在动量算法中，$\theta + \alpha v_{t-1}$近似表示了参数下一步即将到达的位置，因此，我们可以基于即将到达的位置来计算梯度，即计算代价函数相对于未来一步的参数的梯度，并沿着计算出的梯度的反方向更新参数。</p><script type="math/tex; mode=display">v \leftarrow \alpha v - \epsilon \nabla_{\theta} \left[ \frac{1}{m} \sum_{i=1}^m L\big(f(x^{(i)}; \theta + \alpha v), y^{(i)} \big) \right],  \theta \leftarrow \theta + v</script><p>Nesterov动量和标准动量之间的区别体现在梯度计算上。Nesterov动量中，梯度计算在施加当前速度之后。因此，Nesterov动量可以解释为往标准动量方法中添加了一个校正因子。</p><p><img src="http://sebastianruder.com/content/images/2016/09/nesterov_update_vector.png" alt></p><p>如上图中的蓝线所示，在动量算法中，我们首先计算当前的梯度，然后沿着累积的梯度方向前进一大步。在Nesterov动量算法中，我们首先沿着先前累积的梯度方向前进一大步（棕色向量），再评估梯度，然后做出校正（红色向量），最终的实际效果是沿着绿色向量方向前进一步。</p><h1 id="自适应学习率的算法"><a href="#自适应学习率的算法" class="headerlink" title="自适应学习率的算法"></a>自适应学习率的算法</h1><p>在多层网络中，代价函数的梯度大小在不同的层可能会相差很大，尤其是在网络权重初始化为很小的值的情况下。同时，节点的输出误差对节点的输入非常敏感。因此，对网络中的所有权重更新采用相同的全局学习率，在某种程度上来说，不是最好的选择。损失通常高度敏感于参数空间中的某些方向，而不敏感于其他。如果我们相信方向敏感有轴对齐的偏好，那么为每个参数设置不同的学习率，在整个学习过程中自适应地改变这些学习率是有道理的。</p><p>有些模型的部分参数可能在整个训练过程中被有效使用的频率要低于其他部分，比如在其对应的输入（<a href="/good-feature/" title="特征">特征</a>）在大部分训练样本中都为0的情况下，代价函数对这些参数的梯度大部分时候也为0，因而这次参数的更新频率要低于其他参数。一个例子就是在训练词向量的Word2vec模型中，低频词对应的权重向量更新的频率就要低于高频词。如果我们希望对应频繁更新的参数采用较小的学习率，对不频繁更新的参数采用较大的学习率，那么也必须为网络中的每个参数设置不同的学习率。</p><p>下面介绍几种自适应学习率的算法。</p><h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p>AdaGrad算法按照参数的历史梯度平方和的平方根的倒数来收缩学习率。<br>具体地，首先计算mini-batch的梯度：<script type="math/tex">g \leftarrow \frac{1}{m} \nabla_{\theta} \sum_i L(f(x^{(i)};\theta),y^{(i)})</script> 然后，累积平方梯度：<script type="math/tex">r \leftarrow r + g \odot g</script><br>接着计算更新：<script type="math/tex">\Delta \theta \leftarrow - \frac{\epsilon}{\delta+ \sqrt{r}} \odot g</script><br>其中$\epsilon$是初始学习率，$\delta$是一个很小的常数，通常设为$10^{-7}$（用于被小数除时的数值稳定）。最后，执行更新：<script type="math/tex">\theta \leftarrow \theta + \Delta \theta</script></p><p>具有较大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。</p><p>AdaGrad算法倾向于给很少出现的特征更多的权重，因为这些权重更新的频率较低，从而累积的平方梯度较小。</p><p>在凸优化背景中，AdaGrad算法具有一些令人满意的理论性质。然而，经验上已经发现，对于训练深度神经网络模型而言，从<strong>训练开始</strong>时积累梯度平方会导致有效学习率过早和过量的减小。AdaGrad,在某些深度学习模型上效果不错，但不是全部。</p><h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p>RMSProp算法修改AdaGrad以在非凸设定下效果更好，改变梯度积累为指数加权的移动平均。 AdaGrad旨在应用于凸问题时快速收敛。当应用于非凸函数训练神经网络时，学习轨迹可能穿过了很多不同的结构，最终到达一个局部是凸碗的区域。AdaGrad根据平方梯度的整个历史收缩学习率，可能使得学习率在达到这样的凸结构前就变得太小了。</p><p>RMSProp使用指数衰减以丢弃遥远过去的历史，使其能够在找到凸碗状结构后快速收敛，它就像一个初始化于该碗状结构的AdaGrad算法实例。</p><p>RMSProp使用指数衰减累积梯度：<script type="math/tex">r \leftarrow \rho r + (1-\rho) g \odot g</script><br>相比于AdaGrad，使用移动平均引入了一个新的超参数$\rho$，用来控制移动平均的长度范围。<br>计算参数更新大小：<script type="math/tex">\Delta \theta =    -\frac{\epsilon}{\sqrt{\delta + r}} \odot g</script><br>执行参数更新：<script type="math/tex">\theta \leftarrow \theta + \Delta \theta</script></p><p>RMSProp算法还可以和动量结合在一起使用，过程如下：</p><ul><li>计算临时更新：$\tilde{\theta} \leftarrow \theta + \alpha v$</li><li>计算梯度：$g \leftarrow \frac{1}{m} \nabla_{\tilde{\theta}} \sum_i L(f(x^{(i)};\tilde{\theta}),y^{(i)})$</li><li>累积梯度：$r \leftarrow \rho r + (1-\rho) g \odot g$</li><li>计算速度更新：$v \leftarrow \alpha v -\frac{\epsilon}{\sqrt{r}} \odot g$</li><li>执行更新：$\theta \leftarrow \theta + v$</li></ul><h2 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a>AdaDelta</h2><p>与RMSProp算法类似，AdaDelta算法也是为了克服AdaGrad算法过度积极地单调衰减学习率问题而做的另一种扩展。</p><p>AdaDelta算法和RMSProp算法非常类似，其提出者Matthew D. Zeiler是Hinton的亲传弟子之一，而Hinton就是RMSProp算法的提出者，因此这两个算法比较相似也就不足为怪了。</p><p>AdaDelta算法的作者注意到SGD、Momentum、AdaGrad和RMSProp等算法在参数更新时，单位并不匹配，他认为更新应该和参数应有相同的假想单位，为了实现这一想法，AdaDelta在RMSProp的基础上，还考虑了参数更新的指数衰减累积平方和:<script type="math/tex">E[\Delta \theta^2]_t = \rho E[\Delta \theta^2]_{t-1} + (1 - \rho) \Delta \theta^2_t</script></p><p>和RMSProp算法一样，AdaDelta也需要计算指数衰减的累积梯度平方和：<script type="math/tex">E[g^2]_t = \rho E[g^2]_{t-1} + (1 - \rho) g^2_t</script></p><p>AdaDelta算法每一步的参数更新为：<script type="math/tex">\Delta \theta_t = - \dfrac{\sqrt{E[\Delta \theta^2]_{t-1} + \delta}}{\sqrt{E[g^2]_t + \delta}} g_{t}</script></p><p>其中，$\delta$是一个很小的常数，用于被小数除时的数值稳定。最终的参数更新公式为：<script type="math/tex">\theta_{t+1} = \theta_t + \Delta \theta_t</script></p><p>由此可见，AdaDelta算法不需要设置初始学习率。</p><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>Adam是另一种学习率自适应的优化算法。“Adam”这个名字派生自短语”adaptive moments”。</p><p>除了维护指数衰减的累积梯度平方和，Adam还维护一份指数衰减的累积梯度和，就像momentum算法一样。其算法如下：</p><hr><p>Require: 步长 $\epsilon$ （建议默认为： $0.001$）<br>Require: 矩估计的指数衰减速率， $\rho_1$ 和 $\rho_2$ 在区间 $[0, 1)$内。（建议默认为：分别为$0.9$ 和 $0.999$）<br>Require: 用于数值稳定的小常数 $\delta$  （建议默认为： $10^{-8}$）<br>Require: 初始参数 $\theta$<br>　初始化一阶和二阶矩变量 $s = 0 $, $r = 0$<br>　初始化时间步 $t=0$<br>　while{没有达到停止准则}<br>　　从训练集中采包含$m$个样本$\{ x^{(1)},\dots, x^{(m)}\}$ 的小批量，对应目标为$y^{(i)}$。<br>　　计算梯度：$g \leftarrow \frac{1}{m} \nabla_{\theta} \sum_i L(f(x^{(i)};\theta),y^{(i)})$<br>　　$t \leftarrow t + 1$<br>　　更新有偏一阶矩估计： $s \leftarrow \rho_1 s + (1-\rho_1) g$<br>　　更新有偏二阶矩估计：$r \leftarrow \rho_2 r + (1-\rho_2)g \odot g$<br>　　修正一阶矩的偏差：$\hat{s} \leftarrow \frac{s}{1-\rho_1^t}$<br>　　修正二阶矩的偏差：$\hat{r} \leftarrow \frac{r}{1-\rho_2^t}$<br>　　计算更新：$\Delta \theta = - \epsilon \frac{\hat{s}}{\sqrt{\hat{r}} + \delta}$ （逐元素应用操作）<br>　　执行更新：$\theta \leftarrow \theta + \Delta \theta$<br>　end while</p><hr><p>Adam通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要从建议的默认修改。</p><h1 id="如何选择合适的优化算法"><a href="#如何选择合适的优化算法" class="headerlink" title="如何选择合适的优化算法"></a>如何选择合适的优化算法</h1><p>在这一点上并没有达成共识，还需要具体问题具体分析。下面两幅图可以提供一些关于收敛速度的参考。第一幅图展示了在代价函数误差等高线上几种不同的优化算法收敛速度情况；第二幅图展示了在遇到鞍点时，算法的鲁棒性。</p><p><img src="/gradient-descent-variants/contours_evaluation_optimizers.gif" alt="SGD optimization on loss surface contours"></p><p><img src="/gradient-descent-variants/saddle_point_evaluation_optimizers.gif" alt="SGD optimization on saddle point"></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="noopener">G. Hinton’s lecture</a></p><p><a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a></p><p><a href="http://www.deeplearningbook.org/contents/optimization.html" target="_blank" rel="noopener">Deep Learning book</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;梯度下降算法是深度学习中使用得最多的优化算法。它是一种用来最小化目标函数$J(\theta)$（$\theta \in \mathbb{R}^d$是模型的参数）的方法，其沿着与目标函数相对于参数的梯度$\nabla_\theta J(\theta)$相反的方向更新参数的值。形象地说，就是沿着目标函数所在的超平面上的斜坡下降，直到到达山谷(目标函数极小值）。&lt;/p&gt;
&lt;p&gt;关于梯度下降算法的具体介绍请参考我的另一篇博文：&lt;a href=&quot;/gradient-descent/&quot; title=&quot;《梯度下降算法分类及调参优化技巧》&quot;&gt;《梯度下降算法分类及调参优化技巧》&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;梯度更新通常有两大类方法，一类是基于随机梯度下降及其变种的一阶方法；另一类是以牛顿方法、共轭梯度为代表的二阶近似优化方法。&lt;/p&gt;
&lt;p&gt;本文主要对第一类方法做一个简要的综述。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="梯度下降" scheme="http://xudongyang.coding.me/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>梯度下降算法分类及调参优化技巧</title>
    <link href="http://xudongyang.coding.me/gradient-descent/"/>
    <id>http://xudongyang.coding.me/gradient-descent/</id>
    <published>2017-03-22T13:34:52.000Z</published>
    <updated>2019-04-02T02:43:36.834Z</updated>
    
    <content type="html"><![CDATA[<p>梯度下降算法是<a href="/machine-learning/" title="机器学习">机器学习</a>中使用非常广泛的优化算法，尤其常用来解非凸优化问题。因此，特别适合在深度学习中应用，几乎每一个先进的(state-of-the-art)机器学习库或者深度学习库都会包括梯度下降算法不同变种的实现。</p><p>梯度下降是一种用来最小化目标函数$J(\theta)$（$\theta \in \mathbb{R}^d$是模型的参数）的方法，其沿着与目标函数相对于参数的梯度$\nabla_\theta J(\theta)$相反的方向更新参数的值。形象地说，就是沿着目标函数所在的超平面上的斜坡下降，直到到达山谷(目标函数极小值）。</p><p>根据计算目标函数的梯度时所用的数据量的多少，梯度下降算法共有三个不同的变种，它们之间的主要区别在于每次参数更新的精度和所花的时间。</p><a id="more"></a><h1 id="批量梯度下降（Batch-gradient-descent）"><a href="#批量梯度下降（Batch-gradient-descent）" class="headerlink" title="批量梯度下降（Batch gradient descent）"></a>批量梯度下降（Batch gradient descent）</h1><p>最普通的梯度下降，也叫做批量梯度下降，有时候会把“批量”省略，直接叫做梯度下降。它在计算目标函数的梯度时需要使用所有的训练数据。</p><script type="math/tex; mode=display">\theta = \theta - \eta \cdot \nabla_\theta J( \theta)</script><p>也就是说，批量梯度下降在执行每一次参数更新时都需要在整个数据集上计算梯度。当数据集大到不能一次性装入内存时，这种方法就不可行。同时，批量梯度下降也不支持在线学习。</p><p>批量梯度下降的代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_epochs):</span><br><span class="line">  params_grad = evaluate_gradient(loss_function, data, params)</span><br><span class="line">  params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure></p><p>其中，epochs 是用户输入的最大迭代次数。</p><p>批量梯度下降每次学习都使用整个训练集，因此其优点在于每次更新都会朝着正确的方向进行，最后能够保证收敛于极值点(凸函数收敛于全局极值点，非凸函数可能会收敛于局部极值点)，但是其缺点在于每次学习时间过长，并且如果训练集很大则需要消耗大量的内存，并且全量梯度下降不能进行在线模型参数更新。</p><h1 id="随机梯度下降-Stochastic-gradient-descent"><a href="#随机梯度下降-Stochastic-gradient-descent" class="headerlink" title="随机梯度下降(Stochastic gradient descent)"></a>随机梯度下降(Stochastic gradient descent)</h1><p>随机梯度下降算法每次从训练集中随机选择一个样本来进行学习，计算目标函数的梯度并更新参数。</p><script type="math/tex; mode=display">\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_epochs):</span><br><span class="line">  np.random.shuffle(data)</span><br><span class="line">  <span class="keyword">for</span> example <span class="keyword">in</span> data:</span><br><span class="line">    params_grad = evaluate_gradient(loss_function, example, params)</span><br><span class="line">    params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure><p>随机梯度下降算法每次只随机选择一个样本来更新模型参数，因此每次的学习是非常快速的，并且可以进行在线更新。其最大的缺点在于每次更新可能并不会按照正确的方向进行，导致目标函数剧烈的波动(扰动)，如下图：<br><img src="http://sebastianruder.com/content/images/2016/09/sgd_fluctuation.png" alt="from  Wikipedia"></p><p>不过从另一个方面来看，随机梯度下降所带来的波动有个好处就是，对于类似盆地区域（即很多局部极小值点）那么波动可能会导致从当前的局部极小值点跳到另一个更好的局部极小值点，最终收敛于一个较好的局部极值点，甚至全局极值点。</p><p>由于波动，学习的迭代次数会增多，即收敛速度变慢。不过如果我们缓慢地降低学习率，随机梯度下降最终会和批量梯度下降算法一样，具有相同的收敛性，即凸函数收敛于全局极值点，非凸损失函数收敛于局部极值点。</p><h1 id="小批量梯度下降-Mini-batch-gradient-descent"><a href="#小批量梯度下降-Mini-batch-gradient-descent" class="headerlink" title="小批量梯度下降(Mini-batch gradient descent)"></a>小批量梯度下降(Mini-batch gradient descent)</h1><p>小批量梯度下降综合了批量梯度下降与随机梯度下降，在每次更新速度与更新次数中间取得一个平衡，其每次更新从训练集中随机选择$n$个样本进行学习，即：<script type="math/tex">\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})</script></p><p>其代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_epochs):</span><br><span class="line">  np.random.shuffle(data)</span><br><span class="line">  <span class="keyword">for</span> batch <span class="keyword">in</span> get_batches(data, batch_size=<span class="number">50</span>):</span><br><span class="line">    params_grad = evaluate_gradient(loss_function, batch, params)</span><br><span class="line">    params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure></p><p>相对于随机梯度下降，Mini-batch梯度下降降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定。相对于批量梯度下降，其提高了每次学习的速度。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。一般而言每次更新随机选择[50,256]个样本进行学习，但是也要根据具体问题选择，实践中可以进行多次试验，选择一个更新速度与更次次数都较适合的样本数。</p><p>mini-batch梯度下降算法常用于神经网络的训练中，这个时候其也被称为随机梯度下降(SGD)。也就是说深度学习文献资料中说的SGD，通常都是指mini-batch梯度下降。</p><p>mini-batch梯度下降和随机梯度下降的一个重要的性质是每一步更新的计算时间不依赖训练样本数目的多寡。即使训练样本数目非常大时，它们也能收敛。 对于足够大的数据集，它们可能会在处理完整个训练集之前就收敛到最终测试集误差的某个固定容差范围内。</p><h1 id="设置合适的学习率"><a href="#设置合适的学习率" class="headerlink" title="设置合适的学习率"></a>设置合适的学习率</h1><p>学习率可通过试错来选取，通常最好的选择方法是监测目标函数值随时间变化的学习曲线。下面对最常用的Mini-batch梯度下降算法，介绍如何选择合适的学习率。</p><p>由于Mini-batch梯度下降算法中梯度估计引入的噪声源（$n$个训练样本的随机采样），并且不会在极小点处消失。相比之下，当我们使用批量梯度下降算法到达极小点时，整个代价函数的真实梯度会变得很小，最后为0，因此批量梯度下降可以使用固定的学习率。</p><p>保证Mini-batch梯度下降算法（以及随机梯度下降算法）收敛需要在算法迭代过程中不断降低学习率。保证收敛的一个充分条件是<script type="math/tex">\sum_{k=1}^\infty \eta_k = \infty,</script> 并且 <script type="math/tex">\sum_{k=1}^\infty \eta_k^2 < \infty</script></p><p>实践中，一般会线性衰减学习率直到第$\tau$次迭代：<script type="math/tex">\eta_k = (1-\alpha) \eta_0 + \alpha \eta_\tau</script>  其中$\alpha = \frac{k}{\tau}$。 在$\tau$步迭代之后，一般使$\eta$保持常数。</p><p>使用线性策略时，需要选择的参数为$\eta_0$，$\eta_\tau$，$\tau$。通常$\tau$被设为需要反复遍历训练集几百次的迭代次数。 通常$\eta_\tau$应设为大约$\eta_0$的1%。 主要问题是如何设置$\eta_0$。 若$\eta_0$太大，学习曲线将会剧烈振荡，代价函数值通常会明显增加。 温和的振荡是良好的，容易在训练随机代价函数（例如使用Dropout）时出现。 如果学习率太小，那么学习过程会很缓慢。如果初始学习率太低，那么学习可能会卡在一个相当高的代价值。通常，就总训练时间和最终代价值而言，最优初始学习率要高于前100次左右迭代产生最佳效果的学习率。因此，通常会检测最早的几轮迭代中效果最佳的学习率，然后把初始学习率设为比检测到的最佳学习率稍大的值，但又不能太大导致严重的震荡。</p><p>关于梯度下降算法的优化请参考我的另一篇博文：<a href="/gradient-descent-variants/" title="《深度学习中的常用优化技巧》">《深度学习中的常用优化技巧》</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;梯度下降算法是&lt;a href=&quot;/machine-learning/&quot; title=&quot;机器学习&quot;&gt;机器学习&lt;/a&gt;中使用非常广泛的优化算法，尤其常用来解非凸优化问题。因此，特别适合在深度学习中应用，几乎每一个先进的(state-of-the-art)机器学习库或者深度学习库都会包括梯度下降算法不同变种的实现。&lt;/p&gt;
&lt;p&gt;梯度下降是一种用来最小化目标函数$J(\theta)$（$\theta \in \mathbb{R}^d$是模型的参数）的方法，其沿着与目标函数相对于参数的梯度$\nabla_\theta J(\theta)$相反的方向更新参数的值。形象地说，就是沿着目标函数所在的超平面上的斜坡下降，直到到达山谷(目标函数极小值）。&lt;/p&gt;
&lt;p&gt;根据计算目标函数的梯度时所用的数据量的多少，梯度下降算法共有三个不同的变种，它们之间的主要区别在于每次参数更新的精度和所花的时间。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://xudongyang.coding.me/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="梯度下降" scheme="http://xudongyang.coding.me/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
      <category term="深度学习" scheme="http://xudongyang.coding.me/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://xudongyang.coding.me/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
</feed>
